{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-01T11:12:23.630211Z",
     "start_time": "2018-09-01T11:12:23.626417Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, History \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T19:35:07.402119Z",
     "start_time": "2018-10-28T19:35:06.274115Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "import psutil\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trainig with hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params): \n",
    "    \n",
    "    x_train_temp = train_sample[df_features].copy() \n",
    "    y_train_temp = train_sample[df_target].copy()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    if(params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(params['units2']))\n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=params['optimizer'])\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['early_stop_rounds'])\n",
    "    history = History()\n",
    "    \n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=1,\n",
    "              callbacks=[early_stop, history],\n",
    "              verbose=2,\n",
    "              validation_data=(valid_sample[df_features],valid_sample[df_target])) \n",
    "    \n",
    "    [loss, acc] = model.evaluate(valid_sample[df_features],valid_sample[df_target], verbose=0)\n",
    "    \n",
    "    global num\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    if(np.isnan(acc)):\n",
    "        \n",
    "        print(\"{}) Validation set Accuracy: NaN\".format(num),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "        \n",
    "        \n",
    "        f1 = open(\"status.txt\", \"a+\")\n",
    "        f1.write(\"num:{} , loss:{}, Valid acc: {}, params: {} \\n \\n\".format(num,loss, acc,params))\n",
    "        f1.close() \n",
    "      \n",
    "        \n",
    "        num = num + 1\n",
    "        return {'loss': np.inf, 'status': STATUS_OK, 'paras': params}\n",
    "    else:\n",
    "        \n",
    "        model.fit(x_train_temp, y_train_temp,\n",
    "                  batch_size=params['batch_size'],\n",
    "                  epochs=5,\n",
    "                  callbacks=[early_stop, history],\n",
    "                  verbose=2,\n",
    "                  validation_data=(valid_sample[df_features],valid_sample[df_target]))\n",
    "        \n",
    "        [loss, acc] = model.evaluate(valid_sample[df_features],valid_sample[df_target], verbose=0)\n",
    "        \n",
    "        mem = psutil.virtual_memory()\n",
    "        if(np.isnan(acc)):\n",
    "            print(\"{}) Validation set Accuracy: NaN\".format(num),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "            \n",
    "            \n",
    "            f1 = open(\"status.txt\", \"a+\")\n",
    "            f1.write(\"num:{} , loss:{}, Valid acc: {}, params: {} \\n \\n\".format(num,loss, acc,params))\n",
    "            f1.close() \n",
    "            \n",
    "            num = num + 1\n",
    "            return {'loss': np.inf, 'status': STATUS_OK, 'params': params}\n",
    "        print(\"{}) Validation set Accuracy: {:7.2f}%\".format(num,acc*100),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "        \n",
    "        f1 = open(\"status.txt\", \"a+\")\n",
    "        f1.write(\"num:{} , loss:{}, Valid acc: {}, params: {} \\n \\n\".format(num,loss, acc,params))\n",
    "        f1.close() \n",
    "        \n",
    "        num = num + 1\n",
    "        return {'loss': -acc, 'status': STATUS_OK, 'params': params}\n",
    "    \n",
    "    \n",
    "def get_best_model_nn(space):\n",
    "\n",
    "    trials=Trials()\n",
    "    global num\n",
    "    num = 1\n",
    "    best = fmin(create_model, space, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "\n",
    "    return trials.best_trial['result']\n",
    "\n",
    "def train_best_model(best_params):\n",
    "\n",
    "    x_train_temp = train_sample[df_features].copy() \n",
    "    y_train_temp = train_sample[df_target].copy()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(best_params['activation']))\n",
    "    model.add(Dropout(best_params['dropout1']))\n",
    "    if(best_params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(best_params['units2']))\n",
    "        model.add(Activation(best_params['activation']))\n",
    "        model.add(Dropout(best_params['dropout2']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=best_params['optimizer'])\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=best_params['early_stop_rounds'])\n",
    "    history = History()\n",
    "\n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=best_params['batch_size'],\n",
    "              epochs=500,\n",
    "              callbacks=[early_stop, history],\n",
    "              verbose=2,\n",
    "              validation_data=(valid_sample[df_features],valid_sample[df_target]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o = pd.read_pickle('../../data/train_insurance_2017.pkl')\n",
    "valid_o = pd.read_pickle('../../data/valid_insurance_2017.pkl')\n",
    "# test_o = pd.read_pickle('../../data/test_insurance_2017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_o.drop(['StartedFlag'],axis=1,inplace=True)\n",
    "# valid_o.drop(['StartedFlag'],axis=1,inplace=True)\n",
    "# # test_o.drop(['StartedFlag'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid, test, df_features, df_target = data(all_data,target='NewEstimateTotal')\n",
    "# del all_data\n",
    "df_target = 'NewEstimateTotal'\n",
    "# df_features = list(set(train_o.columns) - set([df_target]))\n",
    "df_features = list(train_o.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.remove('NewEstimateTotal')\n",
    "df_features.remove('StartedFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('df_features.pkl', 'wb') as f:\n",
    "     pickle.dump(df_features, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_o.head(100)\n",
    "valid_sample = valid_o.head(20)\n",
    "# del train\n",
    "# del valid\n",
    "del train_o\n",
    "del valid_o\n",
    "# del test_o\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.7124 - acc: 0.5800 - val_loss: 0.6054 - val_acc: 0.6500\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      " - 0s - loss: 0.6563 - acc: 0.6400 - val_loss: 0.5741 - val_acc: 0.7500\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.5697 - acc: 0.7400 - val_loss: 0.5738 - val_acc: 0.7000\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.5398 - acc: 0.7300 - val_loss: 0.5678 - val_acc: 0.7000\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.5520 - acc: 0.6400 - val_loss: 0.5771 - val_acc: 0.6500\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.5070 - acc: 0.7900 - val_loss: 0.5844 - val_acc: 0.6500\n",
      "1) Validation set Accuracy:   65.00% \tAvailable Mem: 29918.7578125 mb\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8577 - acc: 0.4200 - val_loss: 0.6393 - val_acc: 0.6500\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      " - 0s - loss: 0.7800 - acc: 0.5200 - val_loss: 0.6363 - val_acc: 0.6500\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.7469 - acc: 0.5300 - val_loss: 0.6299 - val_acc: 0.6500\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.7731 - acc: 0.5000 - val_loss: 0.6202 - val_acc: 0.6500\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.6896 - acc: 0.6100 - val_loss: 0.6163 - val_acc: 0.6500\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.7005 - acc: 0.6200 - val_loss: 0.6112 - val_acc: 0.6500\n",
      "2) Validation set Accuracy:   65.00% \tAvailable Mem: 29902.48828125 mb\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9936 - acc: 0.4700 - val_loss: 0.6828 - val_acc: 0.7500\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      " - 0s - loss: 0.8876 - acc: 0.7000 - val_loss: 0.6861 - val_acc: 0.7500\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.6172 - acc: 0.7800 - val_loss: 0.6508 - val_acc: 0.6500\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.6153 - acc: 0.7400 - val_loss: 0.6941 - val_acc: 0.5500\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.6066 - acc: 0.8100 - val_loss: 0.6874 - val_acc: 0.5500\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.6018 - acc: 0.8000 - val_loss: 0.6956 - val_acc: 0.7000\n",
      "3) Validation set Accuracy:   70.00% \tAvailable Mem: 29883.35546875 mb\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7350 - acc: 0.5400 - val_loss: 0.6225 - val_acc: 0.5500\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      " - 0s - loss: 0.5990 - acc: 0.6700 - val_loss: 0.5520 - val_acc: 0.7000\n",
      "Epoch 2/5\n",
      " - 0s - loss: 0.5365 - acc: 0.7200 - val_loss: 0.5439 - val_acc: 0.7000\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.4568 - acc: 0.8000 - val_loss: 0.6558 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      " - 0s - loss: 0.4118 - acc: 0.8300 - val_loss: 0.7948 - val_acc: 0.5500\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.4873 - acc: 0.8100 - val_loss: 0.7202 - val_acc: 0.5000\n",
      "4) Validation set Accuracy:   50.00% \tAvailable Mem: 29852.50390625 mb\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3966 - acc: 0.5800 - val_loss: 0.5892 - val_acc: 0.7000\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      " - 0s - loss: 0.9425 - acc: 0.6100 - val_loss: 0.5300 - val_acc: 0.6500\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.0488 - acc: 0.6500 - val_loss: 0.6512 - val_acc: 0.7000\n",
      "Epoch 3/5\n",
      " - 0s - loss: 0.8876 - acc: 0.6900 - val_loss: 0.5547 - val_acc: 0.7500\n",
      "Epoch 4/5\n",
      " - 0s - loss: 1.1182 - acc: 0.6100 - val_loss: 0.6065 - val_acc: 0.7000\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.9427 - acc: 0.6200 - val_loss: 0.5529 - val_acc: 0.7000\n",
      "5) Validation set Accuracy:   70.00% \tAvailable Mem: 29848.7578125 mb\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/500\n",
      " - 0s - loss: 1.4669 - acc: 0.6000 - val_loss: 0.6122 - val_acc: 0.7500\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.8128 - acc: 0.6300 - val_loss: 0.6862 - val_acc: 0.7000\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.8104 - acc: 0.7600 - val_loss: 0.6139 - val_acc: 0.8000\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.6423 - acc: 0.7500 - val_loss: 0.6442 - val_acc: 0.7500\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.6534 - acc: 0.8400 - val_loss: 0.6695 - val_acc: 0.6500\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.6926 - acc: 0.7900 - val_loss: 0.6865 - val_acc: 0.6000\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.5742 - acc: 0.8500 - val_loss: 0.6847 - val_acc: 0.7500\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.6432 - acc: 0.8000 - val_loss: 0.6992 - val_acc: 0.7500\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.7841 - acc: 0.8200 - val_loss: 0.7238 - val_acc: 0.7500\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.7138 - acc: 0.8400 - val_loss: 0.7116 - val_acc: 0.7500\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.6575 - acc: 0.8300 - val_loss: 0.6980 - val_acc: 0.7000\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.6826 - acc: 0.7900 - val_loss: 0.6758 - val_acc: 0.7500\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.6718 - acc: 0.8800 - val_loss: 0.6764 - val_acc: 0.7500\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.6779 - acc: 0.8500 - val_loss: 0.6683 - val_acc: 0.7000\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.5682 - acc: 0.8500 - val_loss: 0.6772 - val_acc: 0.6500\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.5258 - acc: 0.8000 - val_loss: 0.6698 - val_acc: 0.7500\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.7058 - acc: 0.8600 - val_loss: 0.6852 - val_acc: 0.8000\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.6384 - acc: 0.8500 - val_loss: 0.6795 - val_acc: 0.8000\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.7567 - acc: 0.7900 - val_loss: 0.7041 - val_acc: 0.8000\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.5962 - acc: 0.8400 - val_loss: 0.7425 - val_acc: 0.7500\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.6755 - acc: 0.8500 - val_loss: 0.7702 - val_acc: 0.7500\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.5466 - acc: 0.8800 - val_loss: 0.8213 - val_acc: 0.6500\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.4704 - acc: 0.8800 - val_loss: 0.8701 - val_acc: 0.6500\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.6522 - acc: 0.7800 - val_loss: 0.8292 - val_acc: 0.6000\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.5309 - acc: 0.8500 - val_loss: 0.8177 - val_acc: 0.5000\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.4476 - acc: 0.8400 - val_loss: 0.7728 - val_acc: 0.7500\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.4571 - acc: 0.8800 - val_loss: 0.7679 - val_acc: 0.7000\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.3815 - acc: 0.8800 - val_loss: 0.8003 - val_acc: 0.6500\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.3619 - acc: 0.8500 - val_loss: 0.7765 - val_acc: 0.7000\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.4497 - acc: 0.9100 - val_loss: 0.8156 - val_acc: 0.7500\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.4139 - acc: 0.8900 - val_loss: 0.8588 - val_acc: 0.7500\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.5792 - acc: 0.8800 - val_loss: 0.8923 - val_acc: 0.8000\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.5044 - acc: 0.9400 - val_loss: 0.9235 - val_acc: 0.7000\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.3893 - acc: 0.9100 - val_loss: 0.9091 - val_acc: 0.7000\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.4214 - acc: 0.9100 - val_loss: 0.8402 - val_acc: 0.7000\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.5139 - acc: 0.8800 - val_loss: 0.7876 - val_acc: 0.7000\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.4519 - acc: 0.8700 - val_loss: 0.7894 - val_acc: 0.7000\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.4242 - acc: 0.9300 - val_loss: 0.8156 - val_acc: 0.7000\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.3651 - acc: 0.9200 - val_loss: 0.8573 - val_acc: 0.7000\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.3525 - acc: 0.9000 - val_loss: 0.9619 - val_acc: 0.8000\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.4863 - acc: 0.9200 - val_loss: 1.0677 - val_acc: 0.7000\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.2459 - acc: 0.9300 - val_loss: 1.1301 - val_acc: 0.6500\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.5481 - acc: 0.9100 - val_loss: 0.9191 - val_acc: 0.7000\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.5127 - acc: 0.9200 - val_loss: 0.8554 - val_acc: 0.7000\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.7137 - acc: 0.8900 - val_loss: 0.8187 - val_acc: 0.7000\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.3286 - acc: 0.9300 - val_loss: 0.8811 - val_acc: 0.7000\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.4588 - acc: 0.9400 - val_loss: 0.9520 - val_acc: 0.7000\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.4063 - acc: 0.9100 - val_loss: 1.0901 - val_acc: 0.6500\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.3417 - acc: 0.8800 - val_loss: 0.9785 - val_acc: 0.6500\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.5161 - acc: 0.9400 - val_loss: 0.9433 - val_acc: 0.8000\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.4861 - acc: 0.9200 - val_loss: 1.0075 - val_acc: 0.7000\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "space = {'num_layers': hp.choice('num_layers',['one_hidden', 'two_hidden']),\n",
    "                'units1': hp.choice('units1', [32, 64, 128, 256,512]),\n",
    "                'units2': hp.choice('units2', [32, 64, 128, 256,512]),\n",
    "                'dropout1': hp.uniform('dropout1', .25,.75),\n",
    "                'dropout2': hp.uniform('dropout2',  .25,.75),\n",
    "                'batch_size' : hp.choice('batch_size', [16,32,64,128]),\n",
    "                'nb_epochs' :  500,\n",
    "                'optimizer': hp.choice('optimizer',['rmsprop', 'adam', 'nadam','sgd']),\n",
    "                'activation': hp.choice('activation',['relu','sigmoid']),\n",
    "                'early_stop_rounds': hp.choice('early_stop_rounds',[10,20,30,40,50]),\n",
    "            }\n",
    "best_trails1 = get_best_model_nn(space)\n",
    "\n",
    "\n",
    "f = open(\"estimate_total_100000_hyperopt_keras_2017.txt\", \"w\")\n",
    "f.write(\"Took {} seconds with best trails as: {}\".format(time.time()-start_time,best_trails1))\n",
    "f.close() \n",
    "\n",
    "# Training with the best params\n",
    "\n",
    "model = train_best_model(best_trails1['params'])\n",
    "\n",
    "# To save model\n",
    "# best_model.save('insurance_classify_100000_estimate_total_hyperopt_keras_2017.h5')\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model1.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# with open('insurance_classify_100000_estimate_total_hyperopt_2017.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the pickle model\\n\"\n",
    "# from keras.models import load_model\n",
    "# load_model('insurance_classify_100000_estimate_total_hyperopt_keras_2017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid.loc[:,'prob_up'] = model.predict_proba(valid[df_features])[:,1]\n",
    "\n",
    "# valid['prediction'] = np.where(valid.prob_up > 0.5,1,0)\n",
    "\n",
    "# accuracy_score(valid.NewEstimateTotal, valid.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[:,'prob_up'] = model.predict_proba(test[df_features])[:,1]\n",
    "\n",
    "# test['prediction'] = np.where(test.prob_up > 0.5,1,0)\n",
    "\n",
    "# accuracy_score(test.NewEstimateTotal, test.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "best_model = load_model('insurance_classify_100000_estimate_total_hyperopt_keras_2017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = pd.read_pickle('../../data/test_insurance_2017.pkl')\n",
    "# test_o.drop(['StartedFlag'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "test_new = pd.DataFrame()\n",
    "for i,chunk in enumerate(np.array_split(test_o, 5)):\n",
    "    print(i)\n",
    "    pred = model.predict(chunk[df_features]).flatten()\n",
    "    chunk['preds'] = np.where(pred>=0.5,1,0)\n",
    "#     print(type(chunk))\n",
    "#     print(chunk.head(5))\n",
    "#     break\n",
    "    test_new = test_new.append(chunk)\n",
    "#     print(type(test_new))\n",
    "# test_new.to_pickle('test_insurance_2017_preds_estimate_total_hyperopt.pkl')\n",
    "acc = accuracy_score(test_new.NewEstimateTotal, test_new.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7360813639864876"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix on Test Data\n",
    "# metrics.confusion_matrix(test_new.NewEstimateTotal, test_new.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test csv result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = pd.read_pickle('test_insurance_2017_preds_estimate_total_hyperopt_keras.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dummies(dummies_df):\n",
    "    sep = '_'\n",
    "    dummy_cols = list(set(col.split(sep)[0] for col in dummies_df.columns if sep in col))\n",
    "    print(dummy_cols)\n",
    "    other_cols = [col for col in dummies_df.columns if sep not in col]\n",
    "    print(other_cols)\n",
    "    dfs = []\n",
    "    for i, col in enumerate(dummy_cols):\n",
    "        print(i, col)\n",
    "        dfs.append(dummies_df.filter(regex=col).rename(columns=lambda name: name.split(sep)[1]).idxmax(axis=1))\n",
    "\n",
    "    df = pd.concat(dfs + [dummies_df[other_cols]], axis=1)\n",
    "    df.columns = dummy_cols + other_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DisplayName', 'State', 'DivisionName', 'CommOrRes']\n",
      "['NoteCount', 'PhotoCount', 'JobCount', 'ClaimCount', 'LossYearMo', 'NewEstimateTotal', 'preds']\n",
      "0 DisplayName\n",
      "1 State\n",
      "2 DivisionName\n",
      "3 CommOrRes\n"
     ]
    }
   ],
   "source": [
    "test_result = reverse_dummies(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('test_insurance_2017_preds_estimate_total_hyperopt_keras.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
