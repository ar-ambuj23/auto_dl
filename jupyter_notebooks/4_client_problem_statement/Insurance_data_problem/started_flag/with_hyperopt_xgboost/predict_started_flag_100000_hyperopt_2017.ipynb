{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# from hyperopt.mongoexp import MongoTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trainig with hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model developed to be used in for loop without print statement\n",
    "\n",
    "def model_fit_hyperopt(params):\n",
    "    \n",
    "#     attempt_count += 1\n",
    "\n",
    "    ### Asssigning default parameters\n",
    "#     useTrainCV = False\n",
    "    cv_folds = 5\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    ### Forming copy of input datasets\n",
    "    train_c, valid_c = train_sample.copy(), valid_sample.copy()\n",
    "#     out_path = params['out_path']\n",
    "    predictors, target = df_features, df_target\n",
    "    \n",
    "    alg = XGBClassifier(\n",
    "                learning_rate =params['learning_rate'],\n",
    "                n_estimators=5000,\n",
    "                max_depth=params['max_depth'],\n",
    "                min_child_weight=params['min_child_weight'],\n",
    "                gamma=params['gamma'],\n",
    "                subsample=params['subsample'],\n",
    "                colsample_bytree=params['colsample_bytree'],\n",
    "                objective= 'binary:logistic',\n",
    "                scale_pos_weight=params['scale_pos_weight'],\n",
    "                seed=27, n_jobs=7)\n",
    "\n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(train_c[predictors].values, label=train_c[target].values)\n",
    "#         xgtest = xgb.DMatrix(valid_c[predictors].values)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics = 'auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=False, seed=27)\n",
    "#         ne_new = n_estimators=cvresult.shape[0]\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    #Fit the algorithm on the data\n",
    "    eval_set = [(valid_c[predictors],valid_c[target])]\n",
    "    alg.fit(train_c[predictors], train_c[target],eval_metric='auc', early_stopping_rounds=early_stopping_rounds, eval_set = eval_set, verbose = False)     \n",
    "    \n",
    "    valid_c.loc[:,'prob'] = alg.predict_proba(valid_c[predictors])[:,1]\n",
    "#     test_c.loc[:,'prob'] = alg.predict_proba(test_c[predictors])[:,1]\n",
    "    \n",
    "    valid_auc = roc_auc_score(np.array(valid_c.loc[:,target]), np.array(valid_c.loc[:,'prob']))\n",
    "#     test_auc = roc_auc_score(np.array(test_c.loc[:,target]), np.array(test_c.loc[:,'prob']))\n",
    "    loss = 1 - valid_auc\n",
    "    del train_c\n",
    "    del valid_c\n",
    "#     if out_path != None:\n",
    "#         f = open(os.path.join(out_path, \"hyperopt_logs.txt\"), \"a+\")\n",
    "#         f.write(\"Valid AUC: {} and Test AUC: {}, loss: {}\".format(valid_auc,test_auc,loss))\n",
    "#         f.close()\n",
    "\n",
    "    global num\n",
    "    f1 = open(\"status.txt\", \"a+\")\n",
    "    f1.write(\"num:{} , loss:{}, params: {} \\n \\n\".format(num,loss, params))\n",
    "    f1.close() \n",
    "    num = num +1\n",
    "    \n",
    "    print (\"Valid AUC: {} and Loss: {}\".format(valid_auc, loss))\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK, 'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_xgb(space):\n",
    "    \n",
    "#     f = open(os.path.join(out_path, \"hyperopt_logs.txt\"), \"w+\")\n",
    "#     f.close()\n",
    "\n",
    "    \n",
    "    trials=Trials()\n",
    "    global num\n",
    "    num = 1\n",
    "    best = fmin(model_fit_hyperopt, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    \n",
    "    return trials.best_trial['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(best_params):\n",
    "    cv_folds = 5\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_c, valid_c = train_sample.copy(), valid_sample.copy()\n",
    "\n",
    "    predictors, target = df_features, df_target\n",
    "    \n",
    "    alg = XGBClassifier(\n",
    "                learning_rate =best_params['learning_rate'],\n",
    "                n_estimators=5000,\n",
    "                max_depth=best_params['max_depth'],\n",
    "                min_child_weight=best_params['min_child_weight'],\n",
    "                gamma=best_params['gamma'],\n",
    "                subsample=best_params['subsample'],\n",
    "                colsample_bytree=best_params['colsample_bytree'],\n",
    "                objective= 'binary:logistic',\n",
    "                scale_pos_weight=best_params['scale_pos_weight'],\n",
    "                seed=27, n_jobs=7)\n",
    "    \n",
    "    eval_set = [(valid_c[predictors],valid_c[target])]\n",
    "    alg.fit(train_c[predictors], train_c[target],eval_metric='auc', early_stopping_rounds=early_stopping_rounds, eval_set = eval_set, verbose = False)\n",
    "    \n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o = pd.read_pickle('../../data/train_insurance_2017.pkl')\n",
    "valid_o = pd.read_pickle('../../data/valid_insurance_2017.pkl')\n",
    "# test_o = pd.read_pickle('../../data/test_insurance_2017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o.drop(['NewEstimateTotal'],axis=1,inplace=True)\n",
    "valid_o.drop(['NewEstimateTotal'],axis=1,inplace=True)\n",
    "# test_o.drop(['StartedFlag'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid, test, df_features, df_target = data(all_data,target='StartedFlag')\n",
    "# del all_data\n",
    "df_target = 'StartedFlag'\n",
    "df_features = list(set(train_o.columns) - set([df_target]))\n",
    "train_sample = train_o.head(100000)\n",
    "valid_sample = valid_o.head(20000)\n",
    "# del train\n",
    "# del valid\n",
    "del train_o\n",
    "del valid_o\n",
    "# del test_o\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "space = {'learning_rate': hp.choice('learning_rate',[0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1]),\n",
    "                     'min_child_weight': hp.choice('min_child_weight', [0,1,2,3,4,5,6,7]),\n",
    "                     'max_depth': hp.choice('max_depth', [3,4,5,6,7,8,9,10]),\n",
    "                     'gamma': hp.choice('gamma', [0,1,5,10]),\n",
    "                     'subsample': hp.choice('subsample',np.arange(0.4,1.05,0.1)),\n",
    "                     'colsample_bytree': hp.choice('colsample_bytree',np.arange(0.5,1.05,0.1)),\n",
    "                     'scale_pos_weight': (len(train_sample) -train_sample[df_target].sum())/train_sample[df_target].sum(),\n",
    "#                      'predictors': df_features,\n",
    "#                      'target': df_target\n",
    "        }\n",
    "\n",
    "best_trails1 = get_best_model_xgb(space)\n",
    "\n",
    "f = open(\"started_flag_100000_hyperopt_2017.txt\", \"w\")\n",
    "f.write(\"Took {} seconds with best trails as: {}\".format(time.time()-start_time,best_trails1))\n",
    "f.close() \n",
    "\n",
    "best_model = train_best_model(best_trails1['params'])\n",
    "\n",
    "# To save model\n",
    "with open('insurance_classify_100000_started_flag_hyperopt_2017.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the pickle model\\n\",\n",
    "with open('insurance_classify_100000_started_flag_hyperopt_2017.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid.loc[:,'prob_up'] = model.predict_proba(valid[df_features])[:,1]\n",
    "\n",
    "# valid['prediction'] = np.where(valid.prob_up > 0.5,1,0)\n",
    "\n",
    "# accuracy_score(valid.StartedFlag, valid.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[:,'prob_up'] = model.predict_proba(test[df_features])[:,1]\n",
    "\n",
    "# test['prediction'] = np.where(test.prob_up > 0.5,1,0)\n",
    "\n",
    "# accuracy_score(test.StartedFlag, test.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = pd.read_pickle('../../data/test_insurance_2017.pkl')\n",
    "test_o.drop(['NewEstimateTotal'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414139, 2789)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "test_new = pd.DataFrame()\n",
    "for i,chunk in enumerate(np.array_split(test_o, 5)):\n",
    "    print(i)\n",
    "    pred = best_model.predict(chunk[df_features])\n",
    "    chunk['preds'] = pred\n",
    "    test_new = test_new.append(chunk)\n",
    "test_new.to_pickle('test_insurance_2017_preds_started_flag_hyperopt.pkl')\n",
    "acc = accuracy_score(test_new.StartedFlag, test_new.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.805309328510476"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156283,  45098],\n",
       "       [ 35531, 177227]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix on test_new Data\n",
    "metrics.confusion_matrix(test_new.StartedFlag, test_new.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test csv result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dummies(dummies_df):\n",
    "    sep = '_'\n",
    "    dummy_cols = list(set(col.split(sep)[0] for col in dummies_df.columns if sep in col))\n",
    "    print(dummy_cols)\n",
    "    other_cols = [col for col in dummies_df.columns if sep not in col]\n",
    "    print(other_cols)\n",
    "    dfs = []\n",
    "    for i, col in enumerate(dummy_cols):\n",
    "        print(i, col)\n",
    "        dfs.append(dummies_df.filter(regex=col).rename(columns=lambda name: name.split(sep)[1]).idxmax(axis=1))\n",
    "\n",
    "    df = pd.concat(dfs + [dummies_df[other_cols]], axis=1)\n",
    "    df.columns = dummy_cols + other_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CommOrRes', 'DisplayName', 'State', 'DivisionName']\n",
      "['StartedFlag', 'NoteCount', 'PhotoCount', 'JobCount', 'ClaimCount', 'LossYearMo', 'preds']\n",
      "0 CommOrRes\n",
      "1 DisplayName\n",
      "2 State\n",
      "3 DivisionName\n"
     ]
    }
   ],
   "source": [
    "test_result = reverse_dummies(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('test_insurance_2017_preds_started_flag_hyperopt.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
