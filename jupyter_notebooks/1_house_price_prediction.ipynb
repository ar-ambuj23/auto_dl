{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the basic implementation. Using tensorflow's keras class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has shuffling of the data, normalaisation of the features (but on whole df not column wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has custom callback. The printing of dot with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding data directly from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing = keras.datasets.boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 13 different features:\n",
    "\n",
    "Per capita crime rate. <br>\n",
    "The proportion of residential land zoned for lots over 25,000 square feet.<br>\n",
    "The proportion of non-retail business acres per town.<br>\n",
    "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).<br>\n",
    "Nitric oxides concentration (parts per 10 million).<br>\n",
    "The average number of rooms per dwelling.<br>\n",
    "The proportion of owner-occupied units built before 1940.<br>\n",
    "Weighted distances to five Boston employment centers.<br>\n",
    "Index of accessibility to radial highways.<br>\n",
    "Full-value property-tax rate per $10,000. <br>\n",
    "Pupil-teacher ratio by town.<br>\n",
    "1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.<br>\n",
    "Percentage lower status of the population. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02177</td>\n",
       "      <td>82.5</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>7.610</td>\n",
       "      <td>15.7</td>\n",
       "      <td>6.2700</td>\n",
       "      <td>2.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>395.38</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.89822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>4.970</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>375.52</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>6.037</td>\n",
       "      <td>34.5</td>\n",
       "      <td>5.9853</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.69311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.376</td>\n",
       "      <td>88.4</td>\n",
       "      <td>2.5671</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>391.43</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.21977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>5.602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>6.0877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.16211</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.240</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.4290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.03466</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>6.031</td>\n",
       "      <td>23.3</td>\n",
       "      <td>6.6407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>362.25</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>2.14918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.709</td>\n",
       "      <td>98.5</td>\n",
       "      <td>1.6232</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>261.95</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.01439</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>6.604</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.2196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>376.70</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2    3       4      5      6       7     8      9   \\\n",
       "0    1.23247   0.0   8.14  0.0  0.5380  6.142   91.7  3.9769   4.0  307.0   \n",
       "1    0.02177  82.5   2.03  0.0  0.4150  7.610   15.7  6.2700   2.0  348.0   \n",
       "2    4.89822   0.0  18.10  0.0  0.6310  4.970  100.0  1.3325  24.0  666.0   \n",
       "3    0.03961   0.0   5.19  0.0  0.5150  6.037   34.5  5.9853   5.0  224.0   \n",
       "4    3.69311   0.0  18.10  0.0  0.7130  6.376   88.4  2.5671  24.0  666.0   \n",
       "..       ...   ...    ...  ...     ...    ...    ...     ...   ...    ...   \n",
       "399  0.21977   0.0   6.91  0.0  0.4480  5.602   62.0  6.0877   3.0  233.0   \n",
       "400  0.16211  20.0   6.96  0.0  0.4640  6.240   16.3  4.4290   3.0  223.0   \n",
       "401  0.03466  35.0   6.06  0.0  0.4379  6.031   23.3  6.6407   1.0  304.0   \n",
       "402  2.14918   0.0  19.58  0.0  0.8710  5.709   98.5  1.6232   5.0  403.0   \n",
       "403  0.01439  60.0   2.93  0.0  0.4010  6.604   18.8  6.2196   1.0  265.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    21.0  396.90  18.72  \n",
       "1    14.7  395.38   3.11  \n",
       "2    20.2  375.52   3.26  \n",
       "3    20.2  396.90   8.01  \n",
       "4    20.2  391.43  14.65  \n",
       "..    ...     ...    ...  \n",
       "399  17.9  396.90  16.20  \n",
       "400  18.6  396.90   6.59  \n",
       "401  16.9  362.25   7.83  \n",
       "402  14.7  261.95  15.79  \n",
       "403  15.6  376.70   4.38  \n",
       "\n",
       "[404 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 12 features in the data\n",
    "# 404 training rows\n",
    "pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.08460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>6.434</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.8347</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>27.25</td>\n",
       "      <td>29.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>5.913</td>\n",
       "      <td>92.9</td>\n",
       "      <td>2.3534</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>394.95</td>\n",
       "      <td>16.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>5.985</td>\n",
       "      <td>45.4</td>\n",
       "      <td>4.8122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.27346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>6.250</td>\n",
       "      <td>92.6</td>\n",
       "      <td>1.7984</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>338.92</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>6.121</td>\n",
       "      <td>56.8</td>\n",
       "      <td>3.7476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>395.15</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.03049</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>6.874</td>\n",
       "      <td>28.1</td>\n",
       "      <td>6.4654</td>\n",
       "      <td>5.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>387.97</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.03551</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>6.167</td>\n",
       "      <td>46.7</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>390.64</td>\n",
       "      <td>7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.09299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.961</td>\n",
       "      <td>92.9</td>\n",
       "      <td>2.0869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>378.09</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.56868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>6.437</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.8965</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.37</td>\n",
       "      <td>14.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.405</td>\n",
       "      <td>85.4</td>\n",
       "      <td>2.7147</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>70.80</td>\n",
       "      <td>10.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.35472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.072</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.1750</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.73</td>\n",
       "      <td>13.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.54452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6240</td>\n",
       "      <td>6.151</td>\n",
       "      <td>97.9</td>\n",
       "      <td>1.6687</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>7.831</td>\n",
       "      <td>53.6</td>\n",
       "      <td>3.1992</td>\n",
       "      <td>3.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.63</td>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.04820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>5.648</td>\n",
       "      <td>87.6</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>291.55</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.33983</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>6.108</td>\n",
       "      <td>34.9</td>\n",
       "      <td>8.0555</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>390.18</td>\n",
       "      <td>9.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.08826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>6.417</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>383.73</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.04544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>6.144</td>\n",
       "      <td>32.2</td>\n",
       "      <td>5.8736</td>\n",
       "      <td>4.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>368.57</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.20177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>6.127</td>\n",
       "      <td>83.4</td>\n",
       "      <td>2.7227</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.43</td>\n",
       "      <td>11.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.24720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>5.837</td>\n",
       "      <td>59.7</td>\n",
       "      <td>1.9976</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>24.65</td>\n",
       "      <td>15.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>5.841</td>\n",
       "      <td>61.4</td>\n",
       "      <td>3.3779</td>\n",
       "      <td>5.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>377.56</td>\n",
       "      <td>11.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.19133</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>5.605</td>\n",
       "      <td>70.2</td>\n",
       "      <td>7.9549</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>389.13</td>\n",
       "      <td>18.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.03548</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>5.876</td>\n",
       "      <td>19.1</td>\n",
       "      <td>9.2203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>395.18</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.62356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>6.879</td>\n",
       "      <td>77.7</td>\n",
       "      <td>3.2721</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>390.39</td>\n",
       "      <td>9.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.01709</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>6.728</td>\n",
       "      <td>36.1</td>\n",
       "      <td>12.1265</td>\n",
       "      <td>5.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>384.46</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.33889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>6.380</td>\n",
       "      <td>95.6</td>\n",
       "      <td>1.9682</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>60.72</td>\n",
       "      <td>24.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.05789</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4090</td>\n",
       "      <td>5.878</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6.4980</td>\n",
       "      <td>4.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>396.21</td>\n",
       "      <td>8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4.03841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>6.229</td>\n",
       "      <td>90.7</td>\n",
       "      <td>3.0993</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.33</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.09266</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>6.495</td>\n",
       "      <td>18.4</td>\n",
       "      <td>5.4917</td>\n",
       "      <td>7.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>383.61</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5240</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.12816</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4090</td>\n",
       "      <td>5.885</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.4980</td>\n",
       "      <td>4.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.14052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>6.375</td>\n",
       "      <td>32.3</td>\n",
       "      <td>3.9454</td>\n",
       "      <td>4.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>385.81</td>\n",
       "      <td>9.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.03705</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>6.968</td>\n",
       "      <td>37.2</td>\n",
       "      <td>5.2447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>392.23</td>\n",
       "      <td>4.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>4.880</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5895</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>372.92</td>\n",
       "      <td>30.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.53700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>5.981</td>\n",
       "      <td>68.1</td>\n",
       "      <td>3.6715</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>378.35</td>\n",
       "      <td>11.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.53412</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>7.520</td>\n",
       "      <td>89.4</td>\n",
       "      <td>2.1398</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>388.37</td>\n",
       "      <td>7.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.20608</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>5.593</td>\n",
       "      <td>76.5</td>\n",
       "      <td>7.9549</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>372.49</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>9.96654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>6.485</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.9784</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>386.73</td>\n",
       "      <td>18.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.25356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>5.705</td>\n",
       "      <td>77.7</td>\n",
       "      <td>3.9450</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>396.42</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5240</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.11069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>5.951</td>\n",
       "      <td>93.8</td>\n",
       "      <td>2.8893</td>\n",
       "      <td>5.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>396.90</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.22927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>6.030</td>\n",
       "      <td>85.5</td>\n",
       "      <td>5.6894</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>392.74</td>\n",
       "      <td>18.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.07244</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>5.884</td>\n",
       "      <td>18.5</td>\n",
       "      <td>10.7103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>392.33</td>\n",
       "      <td>7.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.36894</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>8.259</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.9067</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.21038</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>6.812</td>\n",
       "      <td>32.2</td>\n",
       "      <td>4.1007</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.65660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>6.122</td>\n",
       "      <td>97.3</td>\n",
       "      <td>1.6180</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>372.80</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.66351</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>7.333</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.8946</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>383.29</td>\n",
       "      <td>7.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.47428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>8.780</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.9047</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.55</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4370</td>\n",
       "      <td>6.273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2515</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.92</td>\n",
       "      <td>6.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.83377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>7.802</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>389.61</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.92400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>6.101</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.2834</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>240.16</td>\n",
       "      <td>9.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3       4      5      6        7     8      9   \\\n",
       "0    18.08460   0.0  18.10  0.0  0.6790  6.434  100.0   1.8347  24.0  666.0   \n",
       "1     0.12329   0.0  10.01  0.0  0.5470  5.913   92.9   2.3534   6.0  432.0   \n",
       "2     0.05497   0.0   5.19  0.0  0.5150  5.985   45.4   4.8122   5.0  224.0   \n",
       "3     1.27346   0.0  19.58  1.0  0.6050  6.250   92.6   1.7984   5.0  403.0   \n",
       "4     0.07151   0.0   4.49  0.0  0.4490  6.121   56.8   3.7476   3.0  247.0   \n",
       "5     0.27957   0.0   9.69  0.0  0.5850  5.926   42.6   2.3817   6.0  391.0   \n",
       "6     0.03049  55.0   3.78  0.0  0.4840  6.874   28.1   6.4654   5.0  370.0   \n",
       "7     0.03551  25.0   4.86  0.0  0.4260  6.167   46.7   5.4007   4.0  281.0   \n",
       "8     0.09299   0.0  25.65  0.0  0.5810  5.961   92.9   2.0869   2.0  188.0   \n",
       "9     3.56868   0.0  18.10  0.0  0.5800  6.437   75.0   2.8965  24.0  666.0   \n",
       "10    0.22876   0.0   8.56  0.0  0.5200  6.405   85.4   2.7147   5.0  384.0   \n",
       "11    1.35472   0.0   8.14  0.0  0.5380  6.072  100.0   4.1750   4.0  307.0   \n",
       "12    0.54452   0.0  21.89  0.0  0.6240  6.151   97.9   1.6687   4.0  437.0   \n",
       "13    0.05602   0.0   2.46  0.0  0.4880  7.831   53.6   3.1992   3.0  193.0   \n",
       "14   12.04820   0.0  18.10  0.0  0.6140  5.648   87.6   1.9512  24.0  666.0   \n",
       "15    0.33983  22.0   5.86  0.0  0.4310  6.108   34.9   8.0555   7.0  330.0   \n",
       "16    0.08826   0.0  10.81  0.0  0.4130  6.417    6.6   5.2873   4.0  305.0   \n",
       "17    0.04544   0.0   3.24  0.0  0.4600  6.144   32.2   5.8736   4.0  430.0   \n",
       "18    5.69175   0.0  18.10  0.0  0.5830  6.114   79.8   3.5459  24.0  666.0   \n",
       "19    5.20177   0.0  18.10  1.0  0.7700  6.127   83.4   2.7227  24.0  666.0   \n",
       "20   15.02340   0.0  18.10  0.0  0.6140  5.304   97.3   2.1007  24.0  666.0   \n",
       "21   12.24720   0.0  18.10  0.0  0.5840  5.837   59.7   1.9976  24.0  666.0   \n",
       "22    0.09744   0.0   5.96  0.0  0.4990  5.841   61.4   3.3779   5.0  279.0   \n",
       "23    0.19133  22.0   5.86  0.0  0.4310  5.605   70.2   7.9549   7.0  330.0   \n",
       "24    0.03548  80.0   3.64  0.0  0.3920  5.876   19.1   9.2203   1.0  315.0   \n",
       "25    5.82401   0.0  18.10  0.0  0.5320  6.242   64.7   3.4242  24.0  666.0   \n",
       "26    0.62356   0.0   6.20  1.0  0.5070  6.879   77.7   3.2721   8.0  307.0   \n",
       "27    0.01709  90.0   2.02  0.0  0.4100  6.728   36.1  12.1265   5.0  187.0   \n",
       "28    9.33889   0.0  18.10  0.0  0.6790  6.380   95.6   1.9682  24.0  666.0   \n",
       "29    0.05789  12.5   6.07  0.0  0.4090  5.878   21.4   6.4980   4.0  345.0   \n",
       "..        ...   ...    ...  ...     ...    ...    ...      ...   ...    ...   \n",
       "72    4.03841   0.0  18.10  0.0  0.5320  6.229   90.7   3.0993  24.0  666.0   \n",
       "73    0.18337   0.0  27.74  0.0  0.6090  5.414   98.3   1.7554   4.0  711.0   \n",
       "74    0.09266  34.0   6.09  0.0  0.4330  6.495   18.4   5.4917   7.0  329.0   \n",
       "75    0.11747  12.5   7.87  0.0  0.5240  6.009   82.9   6.2267   5.0  311.0   \n",
       "76    0.12816  12.5   6.07  0.0  0.4090  5.885   33.0   6.4980   4.0  345.0   \n",
       "77    0.14052   0.0  10.59  0.0  0.4890  6.375   32.3   3.9454   4.0  277.0   \n",
       "78    0.03705  20.0   3.33  0.0  0.4429  6.968   37.2   5.2447   5.0  216.0   \n",
       "79   14.33370   0.0  18.10  0.0  0.7000  4.880  100.0   1.5895  24.0  666.0   \n",
       "80    0.53700   0.0   6.20  0.0  0.5040  5.981   68.1   3.6715   8.0  307.0   \n",
       "81    0.53412  20.0   3.97  0.0  0.6470  7.520   89.4   2.1398   5.0  264.0   \n",
       "82    0.20608  22.0   5.86  0.0  0.4310  5.593   76.5   7.9549   7.0  330.0   \n",
       "83    9.96654   0.0  18.10  0.0  0.7400  6.485  100.0   1.9784  24.0  666.0   \n",
       "84    0.25356   0.0   9.90  0.0  0.5440  5.705   77.7   3.9450   4.0  304.0   \n",
       "85    0.14455  12.5   7.87  0.0  0.5240  6.172   96.1   5.9505   5.0  311.0   \n",
       "86   14.33370   0.0  18.10  0.0  0.6140  6.229   88.0   1.9512  24.0  666.0   \n",
       "87    0.11069   0.0  13.89  1.0  0.5500  5.951   93.8   2.8893   5.0  276.0   \n",
       "88    0.06263   0.0  11.93  0.0  0.5730  6.593   69.1   2.4786   1.0  273.0   \n",
       "89    5.73116   0.0  18.10  0.0  0.5320  7.061   77.0   3.4106  24.0  666.0   \n",
       "90    0.22927   0.0   6.91  0.0  0.4480  6.030   85.5   5.6894   3.0  233.0   \n",
       "91    0.07244  60.0   1.69  0.0  0.4110  5.884   18.5  10.7103   4.0  411.0   \n",
       "92    0.11329  30.0   4.93  0.0  0.4280  6.897   54.3   6.3361   6.0  300.0   \n",
       "93    0.36894  22.0   5.86  0.0  0.4310  8.259    8.4   8.9067   7.0  330.0   \n",
       "94    0.21038  20.0   3.33  0.0  0.4429  6.812   32.2   4.1007   5.0  216.0   \n",
       "95    1.65660   0.0  19.58  0.0  0.8710  6.122   97.3   1.6180   5.0  403.0   \n",
       "96    0.66351  20.0   3.97  0.0  0.6470  7.333  100.0   1.8946   5.0  264.0   \n",
       "97    3.47428   0.0  18.10  1.0  0.7180  8.780   82.9   1.9047  24.0  666.0   \n",
       "98    0.07896   0.0  12.83  0.0  0.4370  6.273    6.0   4.2515   5.0  398.0   \n",
       "99    1.83377   0.0  19.58  1.0  0.6050  7.802   98.2   2.0407   5.0  403.0   \n",
       "100   0.35809   0.0   6.20  1.0  0.5070  6.951   88.5   2.8617   8.0  307.0   \n",
       "101   2.92400   0.0  19.58  0.0  0.6050  6.101   93.0   2.2834   5.0  403.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    20.2   27.25  29.05  \n",
       "1    17.8  394.95  16.21  \n",
       "2    20.2  396.90   9.74  \n",
       "3    14.7  338.92   5.50  \n",
       "4    18.5  395.15   8.44  \n",
       "5    19.2  396.90  13.59  \n",
       "6    17.6  387.97   4.61  \n",
       "7    19.0  390.64   7.51  \n",
       "8    19.1  378.09  17.93  \n",
       "9    20.2  393.37  14.36  \n",
       "10   20.9   70.80  10.63  \n",
       "11   21.0  376.73  13.04  \n",
       "12   21.2  396.90  18.46  \n",
       "13   17.8  392.63   4.45  \n",
       "14   20.2  291.55  14.10  \n",
       "15   19.1  390.18   9.16  \n",
       "16   19.2  383.73   6.72  \n",
       "17   16.9  368.57   9.09  \n",
       "18   20.2  392.68  14.98  \n",
       "19   20.2  395.43  11.48  \n",
       "20   20.2  349.48  24.91  \n",
       "21   20.2   24.65  15.69  \n",
       "22   19.2  377.56  11.41  \n",
       "23   19.1  389.13  18.46  \n",
       "24   16.4  395.18   9.25  \n",
       "25   20.2  396.90  10.74  \n",
       "26   17.4  390.39   9.93  \n",
       "27   17.0  384.46   4.50  \n",
       "28   20.2   60.72  24.08  \n",
       "29   18.9  396.21   8.10  \n",
       "..    ...     ...    ...  \n",
       "72   20.2  395.33  12.87  \n",
       "73   20.1  344.05  23.97  \n",
       "74   16.1  383.61   8.67  \n",
       "75   15.2  396.90  13.27  \n",
       "76   18.9  396.90   8.79  \n",
       "77   18.6  385.81   9.38  \n",
       "78   14.9  392.23   4.59  \n",
       "79   20.2  372.92  30.62  \n",
       "80   17.4  378.35  11.65  \n",
       "81   13.0  388.37   7.26  \n",
       "82   19.1  372.49  12.50  \n",
       "83   20.2  386.73  18.85  \n",
       "84   18.4  396.42  11.50  \n",
       "85   15.2  396.90  19.15  \n",
       "86   20.2  383.32  13.11  \n",
       "87   16.4  396.90  17.92  \n",
       "88   21.0  391.99   9.67  \n",
       "89   20.2  395.28   7.01  \n",
       "90   17.9  392.74  18.80  \n",
       "91   18.3  392.33   7.79  \n",
       "92   16.6  391.25  11.38  \n",
       "93   19.1  396.90   3.54  \n",
       "94   14.9  396.90   4.85  \n",
       "95   14.7  372.80  14.10  \n",
       "96   13.0  383.29   7.79  \n",
       "97   20.2  354.55   5.29  \n",
       "98   18.7  394.92   6.78  \n",
       "99   14.7  389.61   1.92  \n",
       "100  17.4  391.70   9.71  \n",
       "101  14.7  240.16   9.81  \n",
       "\n",
       "[102 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 102 test rows\n",
    "pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (404, 13)\n",
      "Testing set:  (102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: {}\".format(train_data.shape))  # 404 examples, 13 features\n",
    "print(\"Testing set:  {}\".format(test_data.shape))   # 102 examples, 13 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort - Returns the indices that would sort an array.\n",
    "# np.random.random - generate samples from the uniform distribution on [0, 1).\n",
    "np.random.seed(0)\n",
    "order = np.argsort(np.random.random(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[order]\n",
    "train_labels = train_labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03578</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>7.820</td>\n",
       "      <td>64.5</td>\n",
       "      <td>4.6947</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>387.31</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.52058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>6.631</td>\n",
       "      <td>76.5</td>\n",
       "      <td>4.1480</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>388.45</td>\n",
       "      <td>9.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.28807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>6.341</td>\n",
       "      <td>96.4</td>\n",
       "      <td>2.0720</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>318.01</td>\n",
       "      <td>17.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10659</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>5.936</td>\n",
       "      <td>19.5</td>\n",
       "      <td>10.5857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>376.04</td>\n",
       "      <td>5.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.53501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>6.152</td>\n",
       "      <td>82.6</td>\n",
       "      <td>1.7455</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>88.01</td>\n",
       "      <td>15.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS     NOX     RM   AGE      DIS   RAD    TAX  \\\n",
       "0  0.03578  20.0   3.33   0.0  0.4429  7.820  64.5   4.6947   5.0  216.0   \n",
       "1  0.52058   0.0   6.20   1.0  0.5070  6.631  76.5   4.1480   8.0  307.0   \n",
       "2  6.28807   0.0  18.10   0.0  0.7400  6.341  96.4   2.0720  24.0  666.0   \n",
       "3  0.10659  80.0   1.91   0.0  0.4130  5.936  19.5  10.5857   4.0  334.0   \n",
       "4  3.53501   0.0  19.58   1.0  0.8710  6.152  82.6   1.7455   5.0  403.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     14.9  387.31   3.76  \n",
       "1     17.4  388.45   9.54  \n",
       "2     20.2  318.01  17.79  \n",
       "3     22.0  376.04   5.57  \n",
       "4     14.7   88.01  15.02  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "                'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "df = pd.DataFrame(train_data, columns=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.4 25.1 14.9 20.6 15.6 22.7 22.8 17.  19.7 30.5]\n"
     ]
    }
   ],
   "source": [
    "# The labels are the house prices in thousands of dollars\n",
    "\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to normalize features that use different scales and ranges. For each feature, subtract the mean of the feature and divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40190852  0.35890566 -1.14281587 -0.25683275 -0.97702129  2.19057613\n",
      " -0.16163668  0.47069536 -0.51114231 -1.1428069  -1.62718308  0.34605\n",
      " -1.23949229]\n"
     ]
    }
   ],
   "source": [
    "# Test data is *not* used when calculating the mean and std.\n",
    "\n",
    "# Calculating column wise mean and std\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "print(train_data[0])  # First training sample, normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, build_model, since we'll create a second model, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, \n",
    "                       input_shape=(train_data.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "    optimizer = tf.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 500 epochs, and record the training and validation accuracy in the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 527.0632 - mae: 21.5992\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 544.3390 - mae: 21.3409 - val_loss: 471.9128 - val_mae: 20.1240\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 462.1577 - mae: 19.3168 - val_loss: 396.4579 - val_mae: 18.1948\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 380.9586 - mae: 17.1189 - val_loss: 318.9368 - val_mae: 16.0550\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 299.1760 - mae: 14.8143 - val_loss: 238.6523 - val_mae: 13.4843\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 219.4169 - mae: 12.3566 - val_loss: 165.8060 - val_mae: 10.8727\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 153.7905 - mae: 9.8949 - val_loss: 111.2761 - val_mae: 8.5142\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 107.2845 - mae: 8.0736 - val_loss: 77.6724 - val_mae: 6.8201\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 79.1504 - mae: 6.8239 - val_loss: 56.7689 - val_mae: 5.6709\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 59.8347 - mae: 5.9263 - val_loss: 43.5525 - val_mae: 4.9212\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 47.5459 - mae: 5.2058 - val_loss: 37.8962 - val_mae: 4.5196\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 38.9095 - mae: 4.6815 - val_loss: 33.7894 - val_mae: 4.1747\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 32.7791 - mae: 4.2662 - val_loss: 30.2755 - val_mae: 3.8356\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 28.2060 - mae: 3.9524 - val_loss: 28.5326 - val_mae: 3.5848\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 24.5581 - mae: 3.6247 - val_loss: 25.7701 - val_mae: 3.3372\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 21.7301 - mae: 3.3458 - val_loss: 25.4914 - val_mae: 3.2757\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 19.9226 - mae: 3.1632 - val_loss: 23.6139 - val_mae: 3.2127\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.4541 - mae: 3.0517 - val_loss: 22.6321 - val_mae: 3.0268\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 17.1343 - mae: 2.8864 - val_loss: 21.9842 - val_mae: 3.0704\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.9698 - mae: 2.7844 - val_loss: 20.5573 - val_mae: 2.8964\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.0751 - mae: 2.7216 - val_loss: 22.7119 - val_mae: 3.0493\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.5188 - mae: 2.6559 - val_loss: 21.4283 - val_mae: 3.0054\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.8721 - mae: 2.5661 - val_loss: 20.5877 - val_mae: 2.8440\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.4058 - mae: 2.5690 - val_loss: 21.1001 - val_mae: 2.8807\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.2499 - mae: 2.5364 - val_loss: 20.5219 - val_mae: 2.8536\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.9047 - mae: 2.5192 - val_loss: 18.3511 - val_mae: 2.6666\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.9885 - mae: 2.4481 - val_loss: 17.4614 - val_mae: 2.7495\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.7459 - mae: 2.4568 - val_loss: 17.8579 - val_mae: 2.6700\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.3984 - mae: 2.3914 - val_loss: 17.1056 - val_mae: 2.5810\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.0418 - mae: 2.3774 - val_loss: 18.1447 - val_mae: 2.5907\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.9776 - mae: 2.3556 - val_loss: 16.1361 - val_mae: 2.5450\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.5192 - mae: 2.3273 - val_loss: 15.4641 - val_mae: 2.4958\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.4794 - mae: 2.3521 - val_loss: 17.3053 - val_mae: 2.5653\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.1829 - mae: 2.2729 - val_loss: 17.7616 - val_mae: 2.5686\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8556 - mae: 2.2289 - val_loss: 16.4843 - val_mae: 2.5386\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8247 - mae: 2.2490 - val_loss: 15.8968 - val_mae: 2.4634\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.7229 - mae: 2.2296 - val_loss: 14.7473 - val_mae: 2.3573\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.5145 - mae: 2.2132 - val_loss: 15.3880 - val_mae: 2.4471\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.4861 - mae: 2.2071 - val_loss: 15.0673 - val_mae: 2.3276\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0678 - mae: 2.1534 - val_loss: 14.9196 - val_mae: 2.3580\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.9990 - mae: 2.1502 - val_loss: 14.4627 - val_mae: 2.3045\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0519 - mae: 2.1770 - val_loss: 14.7153 - val_mae: 2.2945\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.9556 - mae: 2.1562 - val_loss: 14.2701 - val_mae: 2.3163\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5579 - mae: 2.1335 - val_loss: 14.8137 - val_mae: 2.3205\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4652 - mae: 2.1008 - val_loss: 16.9291 - val_mae: 2.6249\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.1346 - mae: 2.1825 - val_loss: 13.4026 - val_mae: 2.2617\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4027 - mae: 2.0936 - val_loss: 15.7708 - val_mae: 2.3690\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.4548 - mae: 2.0975 - val_loss: 14.5902 - val_mae: 2.4326\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.2979 - mae: 2.0851 - val_loss: 13.5449 - val_mae: 2.2858\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1643 - mae: 2.0792 - val_loss: 13.5290 - val_mae: 2.2070\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1474 - mae: 2.0651 - val_loss: 14.6379 - val_mae: 2.5190\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0407 - mae: 2.0634 - val_loss: 14.2140 - val_mae: 2.4488\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9607 - mae: 2.0475 - val_loss: 14.9434 - val_mae: 2.3268\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0116 - mae: 2.0361 - val_loss: 14.0060 - val_mae: 2.2535\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7876 - mae: 1.9988 - val_loss: 12.7741 - val_mae: 2.2605\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8883 - mae: 2.0364 - val_loss: 12.7559 - val_mae: 2.2299\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6658 - mae: 2.0099 - val_loss: 13.5005 - val_mae: 2.2664\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7193 - mae: 2.0135 - val_loss: 12.9712 - val_mae: 2.2185\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5360 - mae: 1.9804 - val_loss: 12.7218 - val_mae: 2.1709\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6425 - mae: 1.9758 - val_loss: 13.5093 - val_mae: 2.2908\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2436 - mae: 1.9526 - val_loss: 13.1334 - val_mae: 2.2487\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5173 - mae: 1.9520 - val_loss: 12.6028 - val_mae: 2.3628\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4921 - mae: 1.9986 - val_loss: 12.5857 - val_mae: 2.2847\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2124 - mae: 1.9586 - val_loss: 14.7633 - val_mae: 2.3650\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3850 - mae: 1.9545 - val_loss: 13.2804 - val_mae: 2.3475\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3774 - mae: 1.9348 - val_loss: 12.6727 - val_mae: 2.1829\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0558 - mae: 1.9301 - val_loss: 12.7512 - val_mae: 2.3266\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0223 - mae: 1.9240 - val_loss: 13.6454 - val_mae: 2.2201\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9521 - mae: 1.9092 - val_loss: 13.3600 - val_mae: 2.2109\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9010 - mae: 1.8956 - val_loss: 12.4618 - val_mae: 2.1453\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9374 - mae: 1.9203 - val_loss: 14.0494 - val_mae: 2.3759\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8303 - mae: 1.9096 - val_loss: 13.3504 - val_mae: 2.2083\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8350 - mae: 1.9043 - val_loss: 12.3423 - val_mae: 2.1849\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9047 - mae: 1.8971 - val_loss: 13.2938 - val_mae: 2.3512\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9261 - mae: 1.9029 - val_loss: 12.4593 - val_mae: 2.1858\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6997 - mae: 1.8751 - val_loss: 12.1049 - val_mae: 2.1074\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5786 - mae: 1.8598 - val_loss: 11.9212 - val_mae: 2.1300\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6255 - mae: 1.8682 - val_loss: 12.3108 - val_mae: 2.1298\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6415 - mae: 1.8989 - val_loss: 13.5516 - val_mae: 2.2865\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7056 - mae: 1.8782 - val_loss: 12.8352 - val_mae: 2.1027\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5050 - mae: 1.8314 - val_loss: 12.2219 - val_mae: 2.0722\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4492 - mae: 1.8361 - val_loss: 12.3097 - val_mae: 2.1029\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3330 - mae: 1.8044 - val_loss: 11.8449 - val_mae: 2.1240\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3659 - mae: 1.8176 - val_loss: 12.2908 - val_mae: 2.3348\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4617 - mae: 1.8989 - val_loss: 13.7017 - val_mae: 2.5020\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4853 - mae: 1.8444 - val_loss: 11.6022 - val_mae: 2.1725\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2555 - mae: 1.8403 - val_loss: 11.7276 - val_mae: 2.0533\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0914 - mae: 1.8074 - val_loss: 11.6229 - val_mae: 2.0553\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0459 - mae: 1.7929 - val_loss: 12.1707 - val_mae: 2.0949\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1358 - mae: 1.8261 - val_loss: 11.9341 - val_mae: 2.1240\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1284 - mae: 1.8239 - val_loss: 12.1470 - val_mae: 2.1282\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0169 - mae: 1.7930 - val_loss: 11.9640 - val_mae: 2.1780\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0405 - mae: 1.8231 - val_loss: 11.7617 - val_mae: 2.0830\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9956 - mae: 1.7936 - val_loss: 11.4541 - val_mae: 2.1226\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9557 - mae: 1.7815 - val_loss: 12.4569 - val_mae: 2.3789\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9052 - mae: 1.7729 - val_loss: 11.2086 - val_mae: 2.1608\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.0347 - mae: 1.8083 - val_loss: 12.0517 - val_mae: 2.1216\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8436 - mae: 1.7769 - val_loss: 13.3634 - val_mae: 2.2750\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0060 - mae: 1.8076 - val_loss: 11.4280 - val_mae: 2.0811\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8136 - mae: 1.7824 - val_loss: 12.3215 - val_mae: 2.3947\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8558 - mae: 1.7989 - val_loss: 11.9821 - val_mae: 2.1313\n",
      "Epoch 101/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 3.3365 - mae: 1.4355\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7459 - mae: 1.7426 - val_loss: 12.5264 - val_mae: 2.1403\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6887 - mae: 1.7462 - val_loss: 12.5480 - val_mae: 2.3192\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6665 - mae: 1.6900 - val_loss: 11.7791 - val_mae: 2.0842\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7222 - mae: 1.7553 - val_loss: 11.6616 - val_mae: 2.0440\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6223 - mae: 1.7295 - val_loss: 12.0841 - val_mae: 2.1849\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4922 - mae: 1.7270 - val_loss: 11.3574 - val_mae: 2.0905\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5001 - mae: 1.6997 - val_loss: 11.7407 - val_mae: 2.0920\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4252 - mae: 1.7075 - val_loss: 13.4086 - val_mae: 2.1858\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7738 - mae: 1.7509 - val_loss: 12.5461 - val_mae: 2.0963\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2930 - mae: 1.6666 - val_loss: 13.4561 - val_mae: 2.1863\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4923 - mae: 1.7167 - val_loss: 11.7256 - val_mae: 2.0958\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4102 - mae: 1.7010 - val_loss: 12.3422 - val_mae: 2.1367\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4700 - mae: 1.7371 - val_loss: 11.6077 - val_mae: 2.1987\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3981 - mae: 1.7082 - val_loss: 11.6257 - val_mae: 2.2928\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3343 - mae: 1.7084 - val_loss: 11.4800 - val_mae: 2.1190\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2887 - mae: 1.6791 - val_loss: 12.3498 - val_mae: 2.3265\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2184 - mae: 1.6853 - val_loss: 11.4806 - val_mae: 2.1444\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2088 - mae: 1.6988 - val_loss: 12.3240 - val_mae: 2.0860\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1425 - mae: 1.6722 - val_loss: 13.4916 - val_mae: 2.1438\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2825 - mae: 1.6919 - val_loss: 12.3519 - val_mae: 2.3210\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4339 - mae: 1.7038 - val_loss: 12.1171 - val_mae: 2.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9457 - mae: 1.6380 - val_loss: 12.4775 - val_mae: 2.1718\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2089 - mae: 1.6860 - val_loss: 11.4665 - val_mae: 2.1639\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0563 - mae: 1.6756 - val_loss: 13.6836 - val_mae: 2.1701\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2657 - mae: 1.6317 - val_loss: 12.7898 - val_mae: 2.1682\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1808 - mae: 1.6965 - val_loss: 12.5846 - val_mae: 2.0889\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0118 - mae: 1.6372 - val_loss: 11.6413 - val_mae: 2.1000\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8420 - mae: 1.6238 - val_loss: 11.2980 - val_mae: 2.0998\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0080 - mae: 1.6594 - val_loss: 11.8682 - val_mae: 2.1670\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2319 - mae: 1.6564 - val_loss: 11.6748 - val_mae: 2.0503\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8819 - mae: 1.6276 - val_loss: 13.1105 - val_mae: 2.5105\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9597 - mae: 1.6470 - val_loss: 11.9442 - val_mae: 2.1804\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0588 - mae: 1.6577 - val_loss: 12.1152 - val_mae: 2.3545\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0095 - mae: 1.6614 - val_loss: 11.7740 - val_mae: 2.1196\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8148 - mae: 1.6330 - val_loss: 12.1950 - val_mae: 2.1679\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0141 - mae: 1.6848 - val_loss: 12.2237 - val_mae: 2.1027\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6600 - mae: 1.6029 - val_loss: 11.6571 - val_mae: 2.1672\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0416 - mae: 1.6388 - val_loss: 11.7604 - val_mae: 2.1745\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7818 - mae: 1.6385 - val_loss: 12.3649 - val_mae: 2.2601\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7542 - mae: 1.5960 - val_loss: 12.3132 - val_mae: 2.3037\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5760 - mae: 1.5998 - val_loss: 11.2911 - val_mae: 2.1154\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8748 - mae: 1.6254 - val_loss: 11.4157 - val_mae: 2.1112\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7365 - mae: 1.6115 - val_loss: 11.5763 - val_mae: 2.0852\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6620 - mae: 1.6008 - val_loss: 11.0603 - val_mae: 2.0429\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 4.5535 - mae: 1.5892 - val_loss: 11.1779 - val_mae: 2.0439\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7367 - mae: 1.6317 - val_loss: 11.3226 - val_mae: 2.2081\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7335 - mae: 1.6089 - val_loss: 11.5624 - val_mae: 2.2602\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6035 - mae: 1.6113 - val_loss: 11.7306 - val_mae: 2.1799\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5398 - mae: 1.5649 - val_loss: 11.4951 - val_mae: 2.0479\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6191 - mae: 1.5876 - val_loss: 11.2328 - val_mae: 2.0652\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4926 - mae: 1.5787 - val_loss: 11.6503 - val_mae: 2.1486\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5109 - mae: 1.5831 - val_loss: 11.9212 - val_mae: 2.1282\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3778 - mae: 1.5636 - val_loss: 11.6664 - val_mae: 2.2026\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3975 - mae: 1.5670 - val_loss: 11.8937 - val_mae: 2.0462\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5353 - mae: 1.5613 - val_loss: 12.1951 - val_mae: 2.0685\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3422 - mae: 1.5330 - val_loss: 12.3052 - val_mae: 2.0810\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3992 - mae: 1.5553 - val_loss: 13.4271 - val_mae: 2.4149\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5557 - mae: 1.5788 - val_loss: 11.7423 - val_mae: 2.0597\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2403 - mae: 1.5235 - val_loss: 12.1022 - val_mae: 2.0831\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3071 - mae: 1.5369 - val_loss: 11.6060 - val_mae: 2.2174\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3316 - mae: 1.5665 - val_loss: 13.1394 - val_mae: 2.3721\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2015 - mae: 1.4930 - val_loss: 11.8474 - val_mae: 2.0961\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3628 - mae: 1.5609 - val_loss: 11.8174 - val_mae: 2.0988\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1538 - mae: 1.5352 - val_loss: 12.6174 - val_mae: 2.3968\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3569 - mae: 1.5624 - val_loss: 12.3579 - val_mae: 2.1276\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3664 - mae: 1.5562 - val_loss: 12.3987 - val_mae: 2.2500\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9870 - mae: 1.4811 - val_loss: 11.9843 - val_mae: 2.1709\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4392 - mae: 1.6041 - val_loss: 11.9955 - val_mae: 2.0948\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1263 - mae: 1.4769 - val_loss: 11.0868 - val_mae: 2.1811\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1393 - mae: 1.5225 - val_loss: 12.0374 - val_mae: 2.0959\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1824 - mae: 1.5065 - val_loss: 12.0030 - val_mae: 2.2766\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1566 - mae: 1.5209 - val_loss: 12.5580 - val_mae: 2.1486\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2482 - mae: 1.5036 - val_loss: 12.2059 - val_mae: 2.1220\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9964 - mae: 1.4722 - val_loss: 12.2985 - val_mae: 2.1562\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1754 - mae: 1.5224 - val_loss: 13.4617 - val_mae: 2.4496\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1765 - mae: 1.5200 - val_loss: 11.7468 - val_mae: 2.0278\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9498 - mae: 1.4777 - val_loss: 11.5323 - val_mae: 2.2221\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9662 - mae: 1.4864 - val_loss: 12.9884 - val_mae: 2.1842\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9951 - mae: 1.4940 - val_loss: 11.5173 - val_mae: 2.1378\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9273 - mae: 1.4775 - val_loss: 11.8454 - val_mae: 2.0528\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9070 - mae: 1.4765 - val_loss: 11.9138 - val_mae: 2.2193\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8740 - mae: 1.4460 - val_loss: 11.3754 - val_mae: 2.0977\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8871 - mae: 1.4684 - val_loss: 11.4304 - val_mae: 2.0113\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8147 - mae: 1.4416 - val_loss: 11.5405 - val_mae: 2.1764\n",
      "Epoch 185/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9242 - mae: 1.4642 - val_loss: 12.0037 - val_mae: 2.1116\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7702 - mae: 1.4428 - val_loss: 11.9667 - val_mae: 2.1727\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8952 - mae: 1.4575 - val_loss: 12.9387 - val_mae: 2.1411\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8967 - mae: 1.4780 - val_loss: 12.3367 - val_mae: 2.1072\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9840 - mae: 1.4775 - val_loss: 13.9693 - val_mae: 2.6082\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1976 - mae: 1.5072 - val_loss: 11.7056 - val_mae: 2.3151\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8097 - mae: 1.4359 - val_loss: 11.5308 - val_mae: 2.1901\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6227 - mae: 1.4205 - val_loss: 12.9014 - val_mae: 2.4793\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7621 - mae: 1.4583 - val_loss: 13.9940 - val_mae: 2.2781\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.9844 - mae: 1.4552 - val_loss: 11.8074 - val_mae: 2.0840\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5574 - mae: 1.4197 - val_loss: 12.2196 - val_mae: 2.2615\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6358 - mae: 1.4199 - val_loss: 12.6597 - val_mae: 2.1224\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7262 - mae: 1.4486 - val_loss: 12.4728 - val_mae: 2.3118\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6814 - mae: 1.4050 - val_loss: 12.0922 - val_mae: 2.1382\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5780 - mae: 1.4134 - val_loss: 11.9979 - val_mae: 2.1211\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7101 - mae: 1.4460 - val_loss: 11.9863 - val_mae: 2.0520\n",
      "Epoch 201/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 2.9015 - mae: 1.4413\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6388 - mae: 1.4288 - val_loss: 15.3974 - val_mae: 2.8486\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9402 - mae: 1.5315 - val_loss: 12.5825 - val_mae: 2.1011\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5185 - mae: 1.3910 - val_loss: 14.2984 - val_mae: 2.5730\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8451 - mae: 1.4439 - val_loss: 13.2624 - val_mae: 2.2416\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5173 - mae: 1.4066 - val_loss: 12.6868 - val_mae: 2.2821\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7726 - mae: 1.4484 - val_loss: 11.4873 - val_mae: 2.1154\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5253 - mae: 1.3925 - val_loss: 13.3228 - val_mae: 2.1747\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5473 - mae: 1.3820 - val_loss: 11.7258 - val_mae: 2.0721\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4935 - mae: 1.4148 - val_loss: 13.8663 - val_mae: 2.2621\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8367 - mae: 1.4590 - val_loss: 12.0669 - val_mae: 2.3233\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5156 - mae: 1.3818 - val_loss: 11.6919 - val_mae: 2.1180\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4946 - mae: 1.3838 - val_loss: 13.7333 - val_mae: 2.4264\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5580 - mae: 1.4013 - val_loss: 11.5852 - val_mae: 2.2843\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3849 - mae: 1.3709 - val_loss: 12.6711 - val_mae: 2.4209\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6809 - mae: 1.4541 - val_loss: 12.5342 - val_mae: 2.1479\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4243 - mae: 1.3623 - val_loss: 12.0511 - val_mae: 2.2592\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3624 - mae: 1.3757 - val_loss: 11.5965 - val_mae: 2.1302\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3208 - mae: 1.3497 - val_loss: 12.2241 - val_mae: 2.2969\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5128 - mae: 1.4047 - val_loss: 12.1193 - val_mae: 2.1880\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3339 - mae: 1.3540 - val_loss: 11.6944 - val_mae: 2.1465\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3737 - mae: 1.3895 - val_loss: 11.5845 - val_mae: 2.1233\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4045 - mae: 1.3775 - val_loss: 11.6457 - val_mae: 2.0874\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1649 - mae: 1.3362 - val_loss: 12.5903 - val_mae: 2.5640\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3551 - mae: 1.3809 - val_loss: 12.3611 - val_mae: 2.3208\n",
      "Epoch 225/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3403 - mae: 1.3754 - val_loss: 12.1226 - val_mae: 2.1094\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1508 - mae: 1.3185 - val_loss: 11.9806 - val_mae: 2.1309\n",
      "Epoch 227/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2931 - mae: 1.3488 - val_loss: 12.8858 - val_mae: 2.3876\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1588 - mae: 1.3064 - val_loss: 11.4219 - val_mae: 2.0732\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2479 - mae: 1.3554 - val_loss: 12.8084 - val_mae: 2.0981\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1810 - mae: 1.3249 - val_loss: 11.9740 - val_mae: 2.1239\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1399 - mae: 1.3116 - val_loss: 13.0847 - val_mae: 2.2661\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2515 - mae: 1.3352 - val_loss: 12.7483 - val_mae: 2.1481\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3218 - mae: 1.3235 - val_loss: 11.9978 - val_mae: 2.0866\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1201 - mae: 1.3167 - val_loss: 12.4718 - val_mae: 2.1339\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3428 - mae: 1.3432 - val_loss: 13.0176 - val_mae: 2.4632\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1834 - mae: 1.3145 - val_loss: 12.5447 - val_mae: 2.2037\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1214 - mae: 1.2914 - val_loss: 12.7379 - val_mae: 2.4621\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0920 - mae: 1.3168 - val_loss: 12.3370 - val_mae: 2.3935\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0611 - mae: 1.3359 - val_loss: 11.6742 - val_mae: 2.2181\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2138 - mae: 1.3400 - val_loss: 12.5201 - val_mae: 2.3453\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0236 - mae: 1.3049 - val_loss: 12.1630 - val_mae: 2.1063\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9084 - mae: 1.2801 - val_loss: 11.6618 - val_mae: 2.1312\n",
      "Epoch 243/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step - loss: 3.1165 - mae: 1.3068 - val_loss: 12.5964 - val_mae: 2.2242\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0820 - mae: 1.2880 - val_loss: 12.6537 - val_mae: 2.5428\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.1157 - mae: 1.3354 - val_loss: 12.3894 - val_mae: 2.1514\n",
      "Epoch 246/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9036 - mae: 1.2774 - val_loss: 12.0427 - val_mae: 2.1351\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2381 - mae: 1.3631 - val_loss: 12.4117 - val_mae: 2.1271\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9474 - mae: 1.2788 - val_loss: 12.4040 - val_mae: 2.2838\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8910 - mae: 1.2560 - val_loss: 14.2017 - val_mae: 2.3525\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.0747 - mae: 1.2842 - val_loss: 12.1107 - val_mae: 2.3130\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9833 - mae: 1.2866 - val_loss: 13.0106 - val_mae: 2.4298\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.1950 - mae: 1.3159 - val_loss: 12.6762 - val_mae: 2.2384\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9585 - mae: 1.2848 - val_loss: 11.9887 - val_mae: 2.1485\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9092 - mae: 1.2507 - val_loss: 12.3962 - val_mae: 2.5064\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2637 - mae: 1.3541 - val_loss: 11.5362 - val_mae: 2.2663\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7616 - mae: 1.2337 - val_loss: 12.6461 - val_mae: 2.2857\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7974 - mae: 1.2332 - val_loss: 12.2587 - val_mae: 2.2923\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8656 - mae: 1.2431 - val_loss: 12.6578 - val_mae: 2.2114\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8740 - mae: 1.2621 - val_loss: 12.3709 - val_mae: 2.2574\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6831 - mae: 1.2068 - val_loss: 13.3232 - val_mae: 2.2029\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7771 - mae: 1.2205 - val_loss: 12.9845 - val_mae: 2.1819\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8265 - mae: 1.2458 - val_loss: 11.8053 - val_mae: 2.1504\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9493 - mae: 1.2844 - val_loss: 12.7678 - val_mae: 2.2383\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8974 - mae: 1.2530 - val_loss: 12.2513 - val_mae: 2.2474\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8221 - mae: 1.2374 - val_loss: 13.6927 - val_mae: 2.4047\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6872 - mae: 1.1939 - val_loss: 12.9685 - val_mae: 2.4987\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8501 - mae: 1.2663 - val_loss: 12.6604 - val_mae: 2.1653\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6970 - mae: 1.2217 - val_loss: 13.1703 - val_mae: 2.5693\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8449 - mae: 1.2815 - val_loss: 12.2516 - val_mae: 2.1655\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8341 - mae: 1.2469 - val_loss: 12.2534 - val_mae: 2.3062\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8828 - mae: 1.2511 - val_loss: 11.6512 - val_mae: 2.2475\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6608 - mae: 1.2109 - val_loss: 12.8556 - val_mae: 2.2099\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7726 - mae: 1.2196 - val_loss: 11.2484 - val_mae: 2.1474\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7670 - mae: 1.2298 - val_loss: 11.9282 - val_mae: 2.2816\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7708 - mae: 1.2497 - val_loss: 12.5012 - val_mae: 2.3366\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6775 - mae: 1.1980 - val_loss: 12.5780 - val_mae: 2.2561\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6347 - mae: 1.2001 - val_loss: 12.1669 - val_mae: 2.2879\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6513 - mae: 1.1815 - val_loss: 14.0611 - val_mae: 2.5662\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7745 - mae: 1.2550 - val_loss: 12.6034 - val_mae: 2.4093\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6403 - mae: 1.1868 - val_loss: 11.9783 - val_mae: 2.2103\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6617 - mae: 1.2055 - val_loss: 12.0892 - val_mae: 2.2758\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5997 - mae: 1.1902 - val_loss: 11.9006 - val_mae: 2.1606\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6888 - mae: 1.2325 - val_loss: 13.6050 - val_mae: 2.6022\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7401 - mae: 1.2488 - val_loss: 13.0987 - val_mae: 2.5930\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6583 - mae: 1.2105 - val_loss: 13.8025 - val_mae: 2.3109\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7598 - mae: 1.2154 - val_loss: 11.8691 - val_mae: 2.2209\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5950 - mae: 1.2163 - val_loss: 12.4251 - val_mae: 2.1628\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5839 - mae: 1.1995 - val_loss: 12.8386 - val_mae: 2.4985\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5647 - mae: 1.1887 - val_loss: 12.7023 - val_mae: 2.2907\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4777 - mae: 1.1737 - val_loss: 11.7566 - val_mae: 2.2218\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5998 - mae: 1.1979 - val_loss: 12.6332 - val_mae: 2.4233\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5208 - mae: 1.1516 - val_loss: 12.8367 - val_mae: 2.1719\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.5407 - mae: 1.1671 - val_loss: 12.8377 - val_mae: 2.4384\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6312 - mae: 1.2088 - val_loss: 11.9629 - val_mae: 2.2659\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4383 - mae: 1.1542 - val_loss: 13.0143 - val_mae: 2.2840\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4860 - mae: 1.1525 - val_loss: 13.6781 - val_mae: 2.3081\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6221 - mae: 1.1866 - val_loss: 12.0878 - val_mae: 2.1775\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5126 - mae: 1.1969 - val_loss: 14.4652 - val_mae: 2.3909\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5053 - mae: 1.1626 - val_loss: 14.6030 - val_mae: 2.7487\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5757 - mae: 1.1889 - val_loss: 13.3226 - val_mae: 2.2462\n",
      "Epoch 301/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 1.7603 - mae: 1.0801\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4736 - mae: 1.1686 - val_loss: 12.6293 - val_mae: 2.2491\n",
      "Epoch 302/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5655 - mae: 1.2130 - val_loss: 13.6008 - val_mae: 2.2668\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4627 - mae: 1.1726 - val_loss: 13.5357 - val_mae: 2.4861\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4527 - mae: 1.1449 - val_loss: 11.9677 - val_mae: 2.2400\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4520 - mae: 1.1589 - val_loss: 12.3871 - val_mae: 2.2047\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6001 - mae: 1.1860 - val_loss: 13.7840 - val_mae: 2.4248\n",
      "Epoch 307/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3467 - mae: 1.1382 - val_loss: 12.8071 - val_mae: 2.3308\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3109 - mae: 1.1262 - val_loss: 12.3952 - val_mae: 2.1776\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.5010 - mae: 1.1622 - val_loss: 13.6393 - val_mae: 2.4316\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6125 - mae: 1.1814 - val_loss: 13.0411 - val_mae: 2.5046\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3431 - mae: 1.1445 - val_loss: 12.3998 - val_mae: 2.2784\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4884 - mae: 1.1900 - val_loss: 13.1845 - val_mae: 2.5410\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7265 - mae: 1.2083 - val_loss: 12.9388 - val_mae: 2.2606\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3427 - mae: 1.1271 - val_loss: 11.8444 - val_mae: 2.2528\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3458 - mae: 1.1259 - val_loss: 13.6258 - val_mae: 2.2716\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3450 - mae: 1.1332 - val_loss: 12.2348 - val_mae: 2.2386\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4168 - mae: 1.1637 - val_loss: 13.1086 - val_mae: 2.3162\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4140 - mae: 1.1483 - val_loss: 11.7192 - val_mae: 2.2312\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3438 - mae: 1.1334 - val_loss: 14.5770 - val_mae: 2.6309\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3803 - mae: 1.1442 - val_loss: 12.7906 - val_mae: 2.2903\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2044 - mae: 1.1029 - val_loss: 12.5565 - val_mae: 2.3128\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3579 - mae: 1.1042 - val_loss: 15.1191 - val_mae: 2.7751\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4141 - mae: 1.1398 - val_loss: 14.1941 - val_mae: 2.7254\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5089 - mae: 1.1813 - val_loss: 13.1913 - val_mae: 2.4817\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3242 - mae: 1.1295 - val_loss: 14.2788 - val_mae: 2.3579\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4508 - mae: 1.1543 - val_loss: 12.5337 - val_mae: 2.2116\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3924 - mae: 1.1500 - val_loss: 14.4373 - val_mae: 2.3824\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3895 - mae: 1.1275 - val_loss: 12.3368 - val_mae: 2.2639\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2365 - mae: 1.1059 - val_loss: 12.7026 - val_mae: 2.4269\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4302 - mae: 1.1156 - val_loss: 13.0628 - val_mae: 2.2904\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1638 - mae: 1.0952 - val_loss: 12.7211 - val_mae: 2.4119\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3542 - mae: 1.1425 - val_loss: 12.1328 - val_mae: 2.3074\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3184 - mae: 1.1126 - val_loss: 13.3020 - val_mae: 2.4226\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1527 - mae: 1.0764 - val_loss: 12.3222 - val_mae: 2.2868\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2242 - mae: 1.1290 - val_loss: 12.8957 - val_mae: 2.2285\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2552 - mae: 1.0934 - val_loss: 13.1873 - val_mae: 2.5695\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2584 - mae: 1.1194 - val_loss: 14.6495 - val_mae: 2.5091\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4102 - mae: 1.1608 - val_loss: 13.8882 - val_mae: 2.5585\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1206 - mae: 1.0697 - val_loss: 14.5394 - val_mae: 2.3902\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3687 - mae: 1.1504 - val_loss: 12.2033 - val_mae: 2.2825\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1258 - mae: 1.0859 - val_loss: 12.6730 - val_mae: 2.3189\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2814 - mae: 1.1033 - val_loss: 12.3986 - val_mae: 2.2335\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2086 - mae: 1.0921 - val_loss: 13.3436 - val_mae: 2.4187\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5868 - mae: 1.1720 - val_loss: 13.2832 - val_mae: 2.4852\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0614 - mae: 1.0421 - val_loss: 11.6992 - val_mae: 2.2444\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0899 - mae: 1.0698 - val_loss: 12.5763 - val_mae: 2.3066\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0597 - mae: 1.0457 - val_loss: 15.3209 - val_mae: 2.4926\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4236 - mae: 1.1448 - val_loss: 13.9006 - val_mae: 2.2939\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2590 - mae: 1.0991 - val_loss: 12.5313 - val_mae: 2.2334\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1379 - mae: 1.0746 - val_loss: 13.7274 - val_mae: 2.4343\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1154 - mae: 1.0948 - val_loss: 12.3778 - val_mae: 2.3036\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1744 - mae: 1.0828 - val_loss: 12.7038 - val_mae: 2.3713\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1726 - mae: 1.0746 - val_loss: 13.3701 - val_mae: 2.2599\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1335 - mae: 1.0835 - val_loss: 12.6239 - val_mae: 2.3035\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0324 - mae: 1.0537 - val_loss: 12.7846 - val_mae: 2.3375\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1163 - mae: 1.0464 - val_loss: 13.8725 - val_mae: 2.3446\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9928 - mae: 1.0246 - val_loss: 13.3822 - val_mae: 2.2623\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0829 - mae: 1.0535 - val_loss: 12.6596 - val_mae: 2.2459\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2314 - mae: 1.1176 - val_loss: 13.7488 - val_mae: 2.3381\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0371 - mae: 1.0354 - val_loss: 13.7579 - val_mae: 2.5383\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2088 - mae: 1.0999 - val_loss: 12.2620 - val_mae: 2.2226\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9923 - mae: 1.0319 - val_loss: 12.2997 - val_mae: 2.3077\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0965 - mae: 1.0795 - val_loss: 12.5353 - val_mae: 2.2922\n",
      "Epoch 364/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1789 - mae: 1.0701 - val_loss: 14.0381 - val_mae: 2.5843\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1283 - mae: 1.0687 - val_loss: 12.6550 - val_mae: 2.3585\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9374 - mae: 1.0254 - val_loss: 13.1669 - val_mae: 2.3494\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0433 - mae: 1.0644 - val_loss: 12.9163 - val_mae: 2.2708\n",
      "Epoch 368/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1836 - mae: 1.0629 - val_loss: 12.5594 - val_mae: 2.2637\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9408 - mae: 0.9990 - val_loss: 12.8464 - val_mae: 2.3238\n",
      "Epoch 370/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0112 - mae: 1.0314 - val_loss: 13.0047 - val_mae: 2.3022\n",
      "Epoch 371/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9727 - mae: 1.0384 - val_loss: 12.4840 - val_mae: 2.3075\n",
      "Epoch 372/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2615 - mae: 1.1095 - val_loss: 12.4133 - val_mae: 2.2890\n",
      "Epoch 373/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9350 - mae: 1.0056 - val_loss: 14.9248 - val_mae: 2.4577\n",
      "Epoch 374/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1375 - mae: 1.0618 - val_loss: 14.3766 - val_mae: 2.3562\n",
      "Epoch 375/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1383 - mae: 1.0793 - val_loss: 12.1267 - val_mae: 2.2084\n",
      "Epoch 376/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8592 - mae: 1.0107 - val_loss: 13.6057 - val_mae: 2.5396\n",
      "Epoch 377/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0331 - mae: 1.0620 - val_loss: 12.6861 - val_mae: 2.2767\n",
      "Epoch 378/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0212 - mae: 1.0390 - val_loss: 13.6833 - val_mae: 2.4093\n",
      "Epoch 379/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9172 - mae: 0.9854 - val_loss: 13.3745 - val_mae: 2.4098\n",
      "Epoch 380/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8609 - mae: 1.0025 - val_loss: 12.8374 - val_mae: 2.2751\n",
      "Epoch 381/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9687 - mae: 1.0199 - val_loss: 13.1474 - val_mae: 2.2800\n",
      "Epoch 382/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9099 - mae: 1.0133 - val_loss: 13.4113 - val_mae: 2.4025\n",
      "Epoch 383/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0664 - mae: 1.0587 - val_loss: 12.5486 - val_mae: 2.3538\n",
      "Epoch 384/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9641 - mae: 1.0300 - val_loss: 12.4174 - val_mae: 2.2924\n",
      "Epoch 385/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9524 - mae: 1.0067 - val_loss: 12.4244 - val_mae: 2.2312\n",
      "Epoch 386/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9185 - mae: 1.0045 - val_loss: 13.1133 - val_mae: 2.3622\n",
      "Epoch 387/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1463 - mae: 1.0541 - val_loss: 12.0643 - val_mae: 2.2869\n",
      "Epoch 388/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9412 - mae: 1.0129 - val_loss: 13.0016 - val_mae: 2.3278\n",
      "Epoch 389/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7486 - mae: 0.9753 - val_loss: 12.1143 - val_mae: 2.2513\n",
      "Epoch 390/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9034 - mae: 1.0027 - val_loss: 12.4532 - val_mae: 2.3631\n",
      "Epoch 391/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0210 - mae: 1.0337 - val_loss: 12.8263 - val_mae: 2.2841\n",
      "Epoch 392/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7759 - mae: 0.9562 - val_loss: 13.9785 - val_mae: 2.2832\n",
      "Epoch 393/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8619 - mae: 0.9948 - val_loss: 15.3793 - val_mae: 2.4655\n",
      "Epoch 394/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9797 - mae: 1.0161 - val_loss: 13.3125 - val_mae: 2.3011\n",
      "Epoch 395/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8117 - mae: 0.9702 - val_loss: 14.0506 - val_mae: 2.5699\n",
      "Epoch 396/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9538 - mae: 1.0187 - val_loss: 12.5626 - val_mae: 2.2558\n",
      "Epoch 397/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8208 - mae: 0.9914 - val_loss: 15.4566 - val_mae: 2.4549\n",
      "Epoch 398/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0321 - mae: 1.0112 - val_loss: 13.2933 - val_mae: 2.2792\n",
      "Epoch 399/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6893 - mae: 0.9595 - val_loss: 13.1200 - val_mae: 2.3154\n",
      "Epoch 400/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7177 - mae: 0.9492 - val_loss: 16.3770 - val_mae: 2.9714\n",
      "Epoch 401/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 3.1870 - mae: 1.5523\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0026 - mae: 1.0684 - val_loss: 12.6618 - val_mae: 2.2706\n",
      "Epoch 402/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8342 - mae: 1.0020 - val_loss: 12.5442 - val_mae: 2.2832\n",
      "Epoch 403/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8495 - mae: 1.0004 - val_loss: 12.6718 - val_mae: 2.2378\n",
      "Epoch 404/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6233 - mae: 0.9229 - val_loss: 12.7280 - val_mae: 2.4962\n",
      "Epoch 405/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8247 - mae: 0.9965 - val_loss: 12.4105 - val_mae: 2.3808\n",
      "Epoch 406/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0207 - mae: 1.0427 - val_loss: 12.6404 - val_mae: 2.2571\n",
      "Epoch 407/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7853 - mae: 0.9912 - val_loss: 12.8577 - val_mae: 2.3584\n",
      "Epoch 408/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7405 - mae: 0.9225 - val_loss: 13.3010 - val_mae: 2.3052\n",
      "Epoch 409/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8373 - mae: 0.9933 - val_loss: 12.9311 - val_mae: 2.3179\n",
      "Epoch 410/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8707 - mae: 0.9759 - val_loss: 12.9291 - val_mae: 2.3876\n",
      "Epoch 411/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6881 - mae: 0.9659 - val_loss: 12.7107 - val_mae: 2.3895\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8454 - mae: 0.9873 - val_loss: 12.5759 - val_mae: 2.2960\n",
      "Epoch 413/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6751 - mae: 0.9452 - val_loss: 12.9939 - val_mae: 2.3243\n",
      "Epoch 414/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0037 - mae: 1.0215 - val_loss: 12.3075 - val_mae: 2.2448\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8299 - mae: 0.9813 - val_loss: 13.2148 - val_mae: 2.3821\n",
      "Epoch 416/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6835 - mae: 0.9463 - val_loss: 12.5951 - val_mae: 2.2815\n",
      "Epoch 417/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0029 - mae: 1.0411 - val_loss: 12.4028 - val_mae: 2.2796\n",
      "Epoch 418/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7742 - mae: 0.9747 - val_loss: 12.9190 - val_mae: 2.2270\n",
      "Epoch 419/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6872 - mae: 0.9639 - val_loss: 12.7393 - val_mae: 2.4070\n",
      "Epoch 420/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8245 - mae: 0.9665 - val_loss: 13.0931 - val_mae: 2.3909\n",
      "Epoch 421/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7800 - mae: 0.9908 - val_loss: 14.6591 - val_mae: 2.4063\n",
      "Epoch 422/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9842 - mae: 1.0372 - val_loss: 12.8763 - val_mae: 2.3521\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6220 - mae: 0.9155 - val_loss: 12.3396 - val_mae: 2.2325\n",
      "Epoch 424/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6363 - mae: 0.9355 - val_loss: 12.9996 - val_mae: 2.4472\n",
      "Epoch 425/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6750 - mae: 0.9417 - val_loss: 15.5537 - val_mae: 2.8234\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9275 - mae: 0.9828 - val_loss: 13.7108 - val_mae: 2.5478\n",
      "Epoch 427/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6626 - mae: 0.9325 - val_loss: 13.2642 - val_mae: 2.2606\n",
      "Epoch 428/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8786 - mae: 0.9944 - val_loss: 12.6611 - val_mae: 2.3899\n",
      "Epoch 429/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7726 - mae: 0.9436 - val_loss: 14.8418 - val_mae: 2.7594\n",
      "Epoch 430/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9308 - mae: 1.0191 - val_loss: 12.7234 - val_mae: 2.3627\n",
      "Epoch 431/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5800 - mae: 0.9178 - val_loss: 12.9037 - val_mae: 2.3704\n",
      "Epoch 432/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7902 - mae: 0.9816 - val_loss: 11.8087 - val_mae: 2.3480\n",
      "Epoch 433/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7870 - mae: 0.9704 - val_loss: 13.7066 - val_mae: 2.3712\n",
      "Epoch 434/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8191 - mae: 0.9911 - val_loss: 12.4553 - val_mae: 2.3186\n",
      "Epoch 435/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.6306 - mae: 0.9368 - val_loss: 12.4266 - val_mae: 2.3872\n",
      "Epoch 436/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8172 - mae: 1.0164 - val_loss: 13.7035 - val_mae: 2.3797\n",
      "Epoch 437/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7529 - mae: 0.9533 - val_loss: 13.0544 - val_mae: 2.3122\n",
      "Epoch 438/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5831 - mae: 0.9065 - val_loss: 12.4170 - val_mae: 2.3344\n",
      "Epoch 439/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7514 - mae: 0.9607 - val_loss: 13.9646 - val_mae: 2.3026\n",
      "Epoch 440/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6512 - mae: 0.9192 - val_loss: 12.5036 - val_mae: 2.2737\n",
      "Epoch 441/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7434 - mae: 0.9558 - val_loss: 13.0209 - val_mae: 2.3368\n",
      "Epoch 442/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6140 - mae: 0.9295 - val_loss: 12.5054 - val_mae: 2.3349\n",
      "Epoch 443/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7037 - mae: 0.9472 - val_loss: 13.1607 - val_mae: 2.3835\n",
      "Epoch 444/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6740 - mae: 0.9117 - val_loss: 12.6619 - val_mae: 2.3083\n",
      "Epoch 445/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5416 - mae: 0.9092 - val_loss: 12.1401 - val_mae: 2.3488\n",
      "Epoch 446/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5680 - mae: 0.9135 - val_loss: 12.7888 - val_mae: 2.3089\n",
      "Epoch 447/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6086 - mae: 0.9604 - val_loss: 12.8810 - val_mae: 2.3921\n",
      "Epoch 448/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6572 - mae: 0.9365 - val_loss: 13.6556 - val_mae: 2.3174\n",
      "Epoch 449/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6573 - mae: 0.9521 - val_loss: 13.1676 - val_mae: 2.4350\n",
      "Epoch 450/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5114 - mae: 0.8935 - val_loss: 11.5285 - val_mae: 2.2346\n",
      "Epoch 451/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5921 - mae: 0.8954 - val_loss: 14.7877 - val_mae: 2.4822\n",
      "Epoch 452/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6527 - mae: 0.9404 - val_loss: 13.4620 - val_mae: 2.3612\n",
      "Epoch 453/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5483 - mae: 0.9093 - val_loss: 12.6649 - val_mae: 2.3808\n",
      "Epoch 454/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4719 - mae: 0.8885 - val_loss: 16.4385 - val_mae: 2.8766\n",
      "Epoch 455/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7301 - mae: 0.9683 - val_loss: 14.7904 - val_mae: 2.4239\n",
      "Epoch 456/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6991 - mae: 0.9406 - val_loss: 13.1875 - val_mae: 2.5798\n",
      "Epoch 457/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8342 - mae: 0.9852 - val_loss: 12.6472 - val_mae: 2.4119\n",
      "Epoch 458/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6969 - mae: 0.9306 - val_loss: 12.0968 - val_mae: 2.2445\n",
      "Epoch 459/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5924 - mae: 0.9134 - val_loss: 12.3743 - val_mae: 2.2225\n",
      "Epoch 460/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4641 - mae: 0.8587 - val_loss: 14.5839 - val_mae: 2.7802\n",
      "Epoch 461/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6031 - mae: 0.9049 - val_loss: 12.3475 - val_mae: 2.3542\n",
      "Epoch 462/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6111 - mae: 0.9228 - val_loss: 12.6879 - val_mae: 2.2813\n",
      "Epoch 463/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4889 - mae: 0.8934 - val_loss: 13.9672 - val_mae: 2.5864\n",
      "Epoch 464/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8739 - mae: 0.9657 - val_loss: 12.7685 - val_mae: 2.3067\n",
      "Epoch 465/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5326 - mae: 0.9015 - val_loss: 13.7455 - val_mae: 2.5019\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5197 - mae: 0.8908 - val_loss: 14.6333 - val_mae: 2.7737\n",
      "Epoch 467/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9590 - mae: 0.9860 - val_loss: 12.4140 - val_mae: 2.2854\n",
      "Epoch 468/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4544 - mae: 0.8423 - val_loss: 12.2841 - val_mae: 2.2409\n",
      "Epoch 469/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5398 - mae: 0.8961 - val_loss: 13.3526 - val_mae: 2.5196\n",
      "Epoch 470/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6888 - mae: 0.9281 - val_loss: 14.6101 - val_mae: 2.6705\n",
      "Epoch 471/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5227 - mae: 0.8774 - val_loss: 12.1743 - val_mae: 2.3252\n",
      "Epoch 472/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6682 - mae: 0.9246 - val_loss: 13.0471 - val_mae: 2.5457\n",
      "Epoch 473/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5270 - mae: 0.9208 - val_loss: 13.0529 - val_mae: 2.4281\n",
      "Epoch 474/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5907 - mae: 0.9036 - val_loss: 12.9137 - val_mae: 2.2807\n",
      "Epoch 475/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5236 - mae: 0.8900 - val_loss: 12.7801 - val_mae: 2.3939\n",
      "Epoch 476/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4140 - mae: 0.8463 - val_loss: 12.7854 - val_mae: 2.2671\n",
      "Epoch 477/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5868 - mae: 0.8909 - val_loss: 12.7102 - val_mae: 2.3525\n",
      "Epoch 478/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5045 - mae: 0.8977 - val_loss: 12.6889 - val_mae: 2.4334\n",
      "Epoch 479/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6093 - mae: 0.9272 - val_loss: 12.3825 - val_mae: 2.2615\n",
      "Epoch 480/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4946 - mae: 0.9165 - val_loss: 13.0495 - val_mae: 2.4343\n",
      "Epoch 481/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3991 - mae: 0.8701 - val_loss: 12.6425 - val_mae: 2.3497\n",
      "Epoch 482/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5936 - mae: 0.8887 - val_loss: 12.5036 - val_mae: 2.2764\n",
      "Epoch 483/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3094 - mae: 0.8222 - val_loss: 13.8061 - val_mae: 2.6000\n",
      "Epoch 484/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4414 - mae: 0.8668 - val_loss: 12.8950 - val_mae: 2.4416\n",
      "Epoch 485/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step - loss: 1.4071 - mae: 0.8660 - val_loss: 13.8331 - val_mae: 2.4217\n",
      "Epoch 486/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5712 - mae: 0.9230 - val_loss: 14.2784 - val_mae: 2.3457\n",
      "Epoch 487/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6814 - mae: 0.9484 - val_loss: 13.3570 - val_mae: 2.4901\n",
      "Epoch 488/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5760 - mae: 0.9069 - val_loss: 13.3545 - val_mae: 2.3366\n",
      "Epoch 489/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3802 - mae: 0.8525 - val_loss: 13.3571 - val_mae: 2.4788\n",
      "Epoch 490/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5370 - mae: 0.9062 - val_loss: 12.7308 - val_mae: 2.3944\n",
      "Epoch 491/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4020 - mae: 0.8545 - val_loss: 13.0195 - val_mae: 2.3054\n",
      "Epoch 492/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5318 - mae: 0.8950 - val_loss: 14.1847 - val_mae: 2.5782\n",
      "Epoch 493/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4186 - mae: 0.8579 - val_loss: 12.7753 - val_mae: 2.4775\n",
      "Epoch 494/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5117 - mae: 0.8809 - val_loss: 12.8936 - val_mae: 2.3197\n",
      "Epoch 495/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3330 - mae: 0.8324 - val_loss: 12.6498 - val_mae: 2.2847\n",
      "Epoch 496/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7851 - mae: 0.9730 - val_loss: 13.0685 - val_mae: 2.4299\n",
      "Epoch 497/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3779 - mae: 0.8361 - val_loss: 12.4944 - val_mae: 2.3496\n",
      "Epoch 498/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3693 - mae: 0.8391 - val_loss: 12.8225 - val_mae: 2.2698\n",
      "Epoch 499/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5319 - mae: 0.8842 - val_loss: 13.4100 - val_mae: 2.4753\n",
      "Epoch 500/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3655 - mae: 0.8393 - val_loss: 12.6599 - val_mae: 2.4366\n"
     ]
    }
   ],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch.\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "# Store training stats\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model's training progress using the stats stored in the history object. We want to use this data to determine how long to train before the model stops making progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1000$]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mae']), \n",
    "           label='Train mae')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mae']),\n",
    "           label = 'Val mae')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,5])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows little improvement in the model after about 200 epochs. Let's update the model.fit method to automatically stop training when the validation score doesn't improve. We'll use a callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 475.6269 - mae: 19.2931\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 544.5189 - mae: 21.3138 - val_loss: 472.4225 - val_mae: 20.0362\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 465.2781 - mae: 19.3331 - val_loss: 403.8629 - val_mae: 18.2781\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 388.6270 - mae: 17.3476 - val_loss: 324.8369 - val_mae: 16.1021\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 303.0110 - mae: 14.9650 - val_loss: 240.3381 - val_mae: 13.5192\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 216.4394 - mae: 12.2485 - val_loss: 160.6627 - val_mae: 10.5851\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 144.0685 - mae: 9.5840 - val_loss: 110.8360 - val_mae: 8.3050\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 99.5053 - mae: 7.7469 - val_loss: 73.3105 - val_mae: 6.4289\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 70.5391 - mae: 6.3575 - val_loss: 57.1726 - val_mae: 5.5578\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 54.6175 - mae: 5.5399 - val_loss: 47.1657 - val_mae: 4.8969\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 44.9226 - mae: 4.9164 - val_loss: 39.8791 - val_mae: 4.4380\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 37.6504 - mae: 4.4672 - val_loss: 36.4469 - val_mae: 4.1809\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 33.0100 - mae: 4.1601 - val_loss: 34.2266 - val_mae: 3.9375\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 29.6575 - mae: 3.9560 - val_loss: 33.6351 - val_mae: 3.7477\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 26.9107 - mae: 3.7008 - val_loss: 30.6760 - val_mae: 3.6728\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 24.6513 - mae: 3.5537 - val_loss: 29.0025 - val_mae: 3.6281\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 22.8175 - mae: 3.4199 - val_loss: 26.8220 - val_mae: 3.3822\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 20.9816 - mae: 3.2681 - val_loss: 25.2542 - val_mae: 3.1520\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 19.5897 - mae: 3.1368 - val_loss: 24.9629 - val_mae: 3.1140\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.8673 - mae: 3.1072 - val_loss: 25.1663 - val_mae: 3.1084\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 17.5602 - mae: 2.9662 - val_loss: 22.4479 - val_mae: 3.0893\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 16.4322 - mae: 2.8929 - val_loss: 20.6251 - val_mae: 2.9065\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.8556 - mae: 2.8428 - val_loss: 21.6902 - val_mae: 2.8889\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.3454 - mae: 2.7704 - val_loss: 20.1476 - val_mae: 2.8651\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.3096 - mae: 2.6984 - val_loss: 19.5576 - val_mae: 2.8841\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.7712 - mae: 2.6257 - val_loss: 19.6685 - val_mae: 2.7063\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.1759 - mae: 2.5570 - val_loss: 19.2796 - val_mae: 2.6836\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.7079 - mae: 2.5074 - val_loss: 17.6333 - val_mae: 2.5706\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.3446 - mae: 2.4961 - val_loss: 18.1649 - val_mae: 2.5870\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.1147 - mae: 2.4359 - val_loss: 17.4436 - val_mae: 2.5405\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.5093 - mae: 2.4044 - val_loss: 17.2977 - val_mae: 2.7207\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.5152 - mae: 2.3961 - val_loss: 16.1753 - val_mae: 2.4917\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.8986 - mae: 2.3317 - val_loss: 17.5535 - val_mae: 2.5208\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.8011 - mae: 2.2788 - val_loss: 14.9088 - val_mae: 2.4319\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.9453 - mae: 2.3325 - val_loss: 16.4815 - val_mae: 2.6385\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.3094 - mae: 2.2909 - val_loss: 15.3322 - val_mae: 2.4357\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.2120 - mae: 2.2565 - val_loss: 16.4038 - val_mae: 2.5134\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.1395 - mae: 2.2331 - val_loss: 16.2501 - val_mae: 2.6098\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.0229 - mae: 2.2478 - val_loss: 16.3079 - val_mae: 2.4759\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.0531 - mae: 2.2211 - val_loss: 14.7377 - val_mae: 2.3411\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8245 - mae: 2.2293 - val_loss: 15.0274 - val_mae: 2.3444\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.5863 - mae: 2.1686 - val_loss: 14.7385 - val_mae: 2.3394\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.5096 - mae: 2.1681 - val_loss: 15.6187 - val_mae: 2.3724\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.3759 - mae: 2.1376 - val_loss: 14.2038 - val_mae: 2.3187\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.2994 - mae: 2.1749 - val_loss: 15.2205 - val_mae: 2.5651\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.4438 - mae: 2.2098 - val_loss: 14.2095 - val_mae: 2.3745\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.1305 - mae: 2.1317 - val_loss: 13.7808 - val_mae: 2.2771\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0202 - mae: 2.1408 - val_loss: 15.4258 - val_mae: 2.4195\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.9767 - mae: 2.1050 - val_loss: 13.8891 - val_mae: 2.3371\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.1648 - mae: 2.1588 - val_loss: 14.4745 - val_mae: 2.3040\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.8334 - mae: 2.0925 - val_loss: 12.8617 - val_mae: 2.3277\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.8865 - mae: 2.1683 - val_loss: 13.9775 - val_mae: 2.3081\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6653 - mae: 2.1202 - val_loss: 12.7548 - val_mae: 2.2353\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.8015 - mae: 2.1010 - val_loss: 13.7790 - val_mae: 2.3107\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4323 - mae: 2.0752 - val_loss: 15.3814 - val_mae: 2.3619\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6754 - mae: 2.0969 - val_loss: 12.7543 - val_mae: 2.2472\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3439 - mae: 2.0887 - val_loss: 13.6472 - val_mae: 2.3158\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5086 - mae: 2.1041 - val_loss: 13.8824 - val_mae: 2.2424\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3361 - mae: 2.0522 - val_loss: 12.4340 - val_mae: 2.3375\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4445 - mae: 2.1159 - val_loss: 14.4201 - val_mae: 2.2832\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1167 - mae: 2.0056 - val_loss: 13.3613 - val_mae: 2.5572\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4621 - mae: 2.1440 - val_loss: 12.7502 - val_mae: 2.2147\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9582 - mae: 2.0082 - val_loss: 13.8988 - val_mae: 2.5040\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1379 - mae: 2.0829 - val_loss: 13.4621 - val_mae: 2.2170\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0452 - mae: 1.9979 - val_loss: 12.4515 - val_mae: 2.2255\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1188 - mae: 2.0633 - val_loss: 13.0598 - val_mae: 2.2102\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8753 - mae: 1.9956 - val_loss: 13.6956 - val_mae: 2.2389\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9105 - mae: 1.9942 - val_loss: 12.9677 - val_mae: 2.2381\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7308 - mae: 1.9955 - val_loss: 13.5160 - val_mae: 2.3805\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0764 - mae: 2.0521 - val_loss: 12.6437 - val_mae: 2.2801\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7285 - mae: 1.9738 - val_loss: 13.1207 - val_mae: 2.3252\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9496 - mae: 2.0336 - val_loss: 12.9057 - val_mae: 2.2261\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7437 - mae: 1.9865 - val_loss: 12.6394 - val_mae: 2.4085\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5886 - mae: 2.0085 - val_loss: 13.0571 - val_mae: 2.2303\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6954 - mae: 1.9942 - val_loss: 14.0194 - val_mae: 2.2683\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6248 - mae: 1.9757 - val_loss: 12.4815 - val_mae: 2.2194\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4216 - mae: 1.9830 - val_loss: 13.4162 - val_mae: 2.3508\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6569 - mae: 1.9669 - val_loss: 13.4230 - val_mae: 2.3804\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6506 - mae: 1.9756 - val_loss: 12.7210 - val_mae: 2.4571\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement.\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how did the model performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Mean Abs Error: $3117.81\n"
     ]
    }
   ],
   "source": [
    "[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.278067 19.435219 21.748693 31.292349 25.526495 20.472311 26.90453\n",
      " 21.74238  20.963842 24.469542 17.181751 17.229193 17.491232 42.63613\n",
      " 20.286432 19.514547 25.679842 20.829485 21.122982 33.148117 13.392269\n",
      " 16.064754 21.150322 17.022537 21.265265 26.55911  31.195677 29.620646\n",
      " 13.146992 20.640814 20.573057 15.968612 33.82805  24.026712 19.238176\n",
      " 10.37743  17.004377 19.446775 21.700758 25.5073   28.793806 28.418842\n",
      " 15.928351 40.634552 30.674162 24.725939 27.433249 17.826216 24.522776\n",
      " 22.39473  33.805508 21.682905 14.973867 17.53305  35.50396  28.048298\n",
      " 13.596837 48.462994 35.15036  23.124159 26.732267 18.685652 16.181181\n",
      " 19.548674 22.95994  22.523249 15.154825 23.042494 17.28887   8.803266\n",
      " 33.17032  28.533155 27.186827 17.288658 25.552412 18.928617 20.084763\n",
      " 23.542555 35.75457  14.128403 20.430431 39.031918 17.883705 15.135948\n",
      " 18.343435 19.04466  21.422472 21.89456  22.563715 33.354225 20.914417\n",
      " 20.398748 24.322433 40.58147  36.370995 21.643543 37.07644  55.630123\n",
      " 25.895027 46.88312  32.60286  22.200268]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(test_data).flatten()\n",
    "\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduced a few techniques to handle a regression problem.\n",
    "\n",
    "Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems). <br>\n",
    "Similarly, evaluation metrics used for regression differ from classification. <br> \n",
    "A common regression metric is Mean Absolute Error (MAE).<br>\n",
    "When input data features have values with different ranges, each feature should be scaled independently. <br>\n",
    "If there is not much training data, prefer a small network with few hidden layers to avoid overfitting. <br>\n",
    "Early stopping is a useful technique to prevent overfitting.<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
