{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the basic implementation. Using tensorflow's keras class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has shuffling of the data, normalaisation of the features (but on whole df not column wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Has custom callback. The printing of dot with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding data directly from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing = keras.datasets.boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 13 different features:\n",
    "\n",
    "Per capita crime rate. <br>\n",
    "The proportion of residential land zoned for lots over 25,000 square feet.<br>\n",
    "The proportion of non-retail business acres per town.<br>\n",
    "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).<br>\n",
    "Nitric oxides concentration (parts per 10 million).<br>\n",
    "The average number of rooms per dwelling.<br>\n",
    "The proportion of owner-occupied units built before 1940.<br>\n",
    "Weighted distances to five Boston employment centers.<br>\n",
    "Index of accessibility to radial highways.<br>\n",
    "Full-value property-tax rate per $10,000. <br>\n",
    "Pupil-teacher ratio by town.<br>\n",
    "1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.<br>\n",
    "Percentage lower status of the population. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02177</td>\n",
       "      <td>82.5</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>7.610</td>\n",
       "      <td>15.7</td>\n",
       "      <td>6.2700</td>\n",
       "      <td>2.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>395.38</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.89822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>4.970</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>375.52</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>6.037</td>\n",
       "      <td>34.5</td>\n",
       "      <td>5.9853</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.69311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.376</td>\n",
       "      <td>88.4</td>\n",
       "      <td>2.5671</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>391.43</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.21977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>5.602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>6.0877</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.16211</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.240</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.4290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.03466</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>6.031</td>\n",
       "      <td>23.3</td>\n",
       "      <td>6.6407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>362.25</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>2.14918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.709</td>\n",
       "      <td>98.5</td>\n",
       "      <td>1.6232</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>261.95</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.01439</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>6.604</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.2196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>376.70</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2    3       4      5      6       7     8      9   \\\n",
       "0    1.23247   0.0   8.14  0.0  0.5380  6.142   91.7  3.9769   4.0  307.0   \n",
       "1    0.02177  82.5   2.03  0.0  0.4150  7.610   15.7  6.2700   2.0  348.0   \n",
       "2    4.89822   0.0  18.10  0.0  0.6310  4.970  100.0  1.3325  24.0  666.0   \n",
       "3    0.03961   0.0   5.19  0.0  0.5150  6.037   34.5  5.9853   5.0  224.0   \n",
       "4    3.69311   0.0  18.10  0.0  0.7130  6.376   88.4  2.5671  24.0  666.0   \n",
       "..       ...   ...    ...  ...     ...    ...    ...     ...   ...    ...   \n",
       "399  0.21977   0.0   6.91  0.0  0.4480  5.602   62.0  6.0877   3.0  233.0   \n",
       "400  0.16211  20.0   6.96  0.0  0.4640  6.240   16.3  4.4290   3.0  223.0   \n",
       "401  0.03466  35.0   6.06  0.0  0.4379  6.031   23.3  6.6407   1.0  304.0   \n",
       "402  2.14918   0.0  19.58  0.0  0.8710  5.709   98.5  1.6232   5.0  403.0   \n",
       "403  0.01439  60.0   2.93  0.0  0.4010  6.604   18.8  6.2196   1.0  265.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    21.0  396.90  18.72  \n",
       "1    14.7  395.38   3.11  \n",
       "2    20.2  375.52   3.26  \n",
       "3    20.2  396.90   8.01  \n",
       "4    20.2  391.43  14.65  \n",
       "..    ...     ...    ...  \n",
       "399  17.9  396.90  16.20  \n",
       "400  18.6  396.90   6.59  \n",
       "401  16.9  362.25   7.83  \n",
       "402  14.7  261.95  15.79  \n",
       "403  15.6  376.70   4.38  \n",
       "\n",
       "[404 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 12 features in the data\n",
    "# 404 training rows\n",
    "pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.08460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>6.434</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.8347</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>27.25</td>\n",
       "      <td>29.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547</td>\n",
       "      <td>5.913</td>\n",
       "      <td>92.9</td>\n",
       "      <td>2.3534</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>394.95</td>\n",
       "      <td>16.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>5.985</td>\n",
       "      <td>45.4</td>\n",
       "      <td>4.8122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.27346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>6.250</td>\n",
       "      <td>92.6</td>\n",
       "      <td>1.7984</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>338.92</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449</td>\n",
       "      <td>6.121</td>\n",
       "      <td>56.8</td>\n",
       "      <td>3.7476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>395.15</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.47428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718</td>\n",
       "      <td>8.780</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.9047</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.55</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>6.273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2515</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.92</td>\n",
       "      <td>6.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.83377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>7.802</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>389.61</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.92400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>6.101</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.2834</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>240.16</td>\n",
       "      <td>9.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1      2    3      4      5      6       7     8      9   \\\n",
       "0    18.08460  0.0  18.10  0.0  0.679  6.434  100.0  1.8347  24.0  666.0   \n",
       "1     0.12329  0.0  10.01  0.0  0.547  5.913   92.9  2.3534   6.0  432.0   \n",
       "2     0.05497  0.0   5.19  0.0  0.515  5.985   45.4  4.8122   5.0  224.0   \n",
       "3     1.27346  0.0  19.58  1.0  0.605  6.250   92.6  1.7984   5.0  403.0   \n",
       "4     0.07151  0.0   4.49  0.0  0.449  6.121   56.8  3.7476   3.0  247.0   \n",
       "..        ...  ...    ...  ...    ...    ...    ...     ...   ...    ...   \n",
       "97    3.47428  0.0  18.10  1.0  0.718  8.780   82.9  1.9047  24.0  666.0   \n",
       "98    0.07896  0.0  12.83  0.0  0.437  6.273    6.0  4.2515   5.0  398.0   \n",
       "99    1.83377  0.0  19.58  1.0  0.605  7.802   98.2  2.0407   5.0  403.0   \n",
       "100   0.35809  0.0   6.20  1.0  0.507  6.951   88.5  2.8617   8.0  307.0   \n",
       "101   2.92400  0.0  19.58  0.0  0.605  6.101   93.0  2.2834   5.0  403.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    20.2   27.25  29.05  \n",
       "1    17.8  394.95  16.21  \n",
       "2    20.2  396.90   9.74  \n",
       "3    14.7  338.92   5.50  \n",
       "4    18.5  395.15   8.44  \n",
       "..    ...     ...    ...  \n",
       "97   20.2  354.55   5.29  \n",
       "98   18.7  394.92   6.78  \n",
       "99   14.7  389.61   1.92  \n",
       "100  17.4  391.70   9.71  \n",
       "101  14.7  240.16   9.81  \n",
       "\n",
       "[102 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 102 test rows\n",
    "pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (404, 13)\n",
      "Testing set:  (102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: {}\".format(train_data.shape))  # 404 examples, 13 features\n",
    "print(\"Testing set:  {}\".format(test_data.shape))   # 102 examples, 13 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort - Returns the indices that would sort an array.\n",
    "# np.random.random - generate samples from the uniform distribution on [0, 1).\n",
    "np.random.seed(0)\n",
    "order = np.argsort(np.random.random(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[order]\n",
    "train_labels = train_labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01432</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>6.816</td>\n",
       "      <td>40.5</td>\n",
       "      <td>8.3248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>392.90</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>6.059</td>\n",
       "      <td>37.3</td>\n",
       "      <td>4.8122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.14</td>\n",
       "      <td>8.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02543</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>6.696</td>\n",
       "      <td>56.4</td>\n",
       "      <td>5.7321</td>\n",
       "      <td>5.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.49820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>4.138</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.1370</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM     ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0   0.02985    0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "1   0.01432  100.0   1.32   0.0  0.411  6.816   40.5  8.3248   5.0  256.0   \n",
       "2   0.03306    0.0   5.19   0.0  0.515  6.059   37.3  4.8122   5.0  224.0   \n",
       "3   0.02543   55.0   3.78   0.0  0.484  6.696   56.4  5.7321   5.0  370.0   \n",
       "4  18.49820    0.0  18.10   0.0  0.668  4.138  100.0  1.1370  24.0  666.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     18.7  394.12   5.21  \n",
       "1     15.1  392.90   3.95  \n",
       "2     20.2  396.14   8.51  \n",
       "3     17.6  396.90   7.18  \n",
       "4     20.2  396.90  37.97  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "                'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "df = pd.DataFrame(train_data, columns=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.7 31.6 20.6 23.9 13.8 23.2 18.2 34.9 21.2 22.5]\n"
     ]
    }
   ],
   "source": [
    "# The labels are the house prices in thousands of dollars\n",
    "\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to normalize features that use different scales and ranges. For each feature, subtract the mean of the feature and divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40255104 -0.48361547 -1.31186211 -0.25683275 -0.84812434  0.22981568\n",
      " -0.36947682  1.14510462 -0.74135579 -1.10669897  0.10193124  0.41850097\n",
      " -1.03936976]\n"
     ]
    }
   ],
   "source": [
    "# Test data is *not* used when calculating the mean and std.\n",
    "\n",
    "# Calculating column wise mean and std\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "print(train_data[0])  # First training sample, normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, build_model, since we'll create a second model, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, \n",
    "                       input_shape=(train_data.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "    optimizer = tf.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 500 epochs, and record the training and validation accuracy in the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 558.4364 - mae: 21.7508\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 538.7789 - mae: 21.4345 - val_loss: 529.3931 - val_mae: 21.0218\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 470.5694 - mae: 19.8738 - val_loss: 453.2344 - val_mae: 19.2460\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 398.4885 - mae: 18.1052 - val_loss: 361.6307 - val_mae: 17.0098\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 317.1424 - mae: 15.9487 - val_loss: 276.5762 - val_mae: 14.5929\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 239.7539 - mae: 13.5043 - val_loss: 192.8786 - val_mae: 11.8232\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 166.7931 - mae: 10.8336 - val_loss: 126.0488 - val_mae: 9.2369\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 111.2285 - mae: 8.4244 - val_loss: 87.7259 - val_mae: 7.7296\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 78.6112 - mae: 6.8023 - val_loss: 66.7092 - val_mae: 6.7614\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 59.9506 - mae: 5.7462 - val_loss: 51.9817 - val_mae: 5.8979\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 46.5643 - mae: 4.9252 - val_loss: 40.5602 - val_mae: 5.0754\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 37.6891 - mae: 4.3068 - val_loss: 32.4224 - val_mae: 4.4294\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 32.0804 - mae: 3.8632 - val_loss: 26.5459 - val_mae: 4.0082\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 28.4868 - mae: 3.6999 - val_loss: 22.3442 - val_mae: 3.7458\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 26.6074 - mae: 3.5437 - val_loss: 21.0492 - val_mae: 3.6801\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 24.9199 - mae: 3.3842 - val_loss: 19.7437 - val_mae: 3.5897\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 23.5461 - mae: 3.3089 - val_loss: 17.2040 - val_mae: 3.3260\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 22.1658 - mae: 3.1732 - val_loss: 19.7263 - val_mae: 3.5699\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 21.0950 - mae: 3.1397 - val_loss: 17.8184 - val_mae: 3.4013\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 20.1157 - mae: 3.0599 - val_loss: 15.7062 - val_mae: 3.1638\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.8608 - mae: 2.9455 - val_loss: 13.7258 - val_mae: 2.8893\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 17.9512 - mae: 2.8859 - val_loss: 13.2382 - val_mae: 2.8484\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 17.0944 - mae: 2.7843 - val_loss: 13.6684 - val_mae: 2.8805\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 16.3967 - mae: 2.7577 - val_loss: 12.0520 - val_mae: 2.6914\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.9676 - mae: 2.6892 - val_loss: 11.9774 - val_mae: 2.6602\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.2348 - mae: 2.6277 - val_loss: 12.9290 - val_mae: 2.8162\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.8158 - mae: 2.6971 - val_loss: 11.1816 - val_mae: 2.5724\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.2372 - mae: 2.6209 - val_loss: 11.6074 - val_mae: 2.6286\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.8474 - mae: 2.5042 - val_loss: 10.6413 - val_mae: 2.5030\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.6539 - mae: 2.5260 - val_loss: 12.5820 - val_mae: 2.7456\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.4335 - mae: 2.4984 - val_loss: 10.6992 - val_mae: 2.5554\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.7759 - mae: 2.5308 - val_loss: 10.0827 - val_mae: 2.4813\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.6053 - mae: 2.4127 - val_loss: 10.2970 - val_mae: 2.5310\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.4805 - mae: 2.4239 - val_loss: 10.0822 - val_mae: 2.5637\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.2376 - mae: 2.3258 - val_loss: 9.5591 - val_mae: 2.4485\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.8514 - mae: 2.3508 - val_loss: 10.1700 - val_mae: 2.5458\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.6078 - mae: 2.3355 - val_loss: 10.2667 - val_mae: 2.5559\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.3167 - mae: 2.3252 - val_loss: 9.8249 - val_mae: 2.5133\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.2886 - mae: 2.2523 - val_loss: 8.9944 - val_mae: 2.4583\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.9459 - mae: 2.2580 - val_loss: 11.5440 - val_mae: 2.6419\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.9643 - mae: 2.3290 - val_loss: 8.7279 - val_mae: 2.3956\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.7884 - mae: 2.2625 - val_loss: 8.8406 - val_mae: 2.4516\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.5281 - mae: 2.2090 - val_loss: 8.4750 - val_mae: 2.4288\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.3741 - mae: 2.2337 - val_loss: 9.2392 - val_mae: 2.4975\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.3592 - mae: 2.1882 - val_loss: 8.9736 - val_mae: 2.4448\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.1571 - mae: 2.2001 - val_loss: 11.0354 - val_mae: 2.6158\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.0684 - mae: 2.1972 - val_loss: 10.1570 - val_mae: 2.5207\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8811 - mae: 2.1941 - val_loss: 10.3386 - val_mae: 2.5255\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8852 - mae: 2.1352 - val_loss: 8.4977 - val_mae: 2.3417\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.6381 - mae: 2.1788 - val_loss: 8.3542 - val_mae: 2.3272\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.5434 - mae: 2.1521 - val_loss: 8.7241 - val_mae: 2.4570\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.6219 - mae: 2.1121 - val_loss: 7.9658 - val_mae: 2.2990\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.4069 - mae: 2.0979 - val_loss: 9.2026 - val_mae: 2.3854\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.4578 - mae: 2.1247 - val_loss: 8.6264 - val_mae: 2.3797\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.3340 - mae: 2.1073 - val_loss: 8.0889 - val_mae: 2.3798\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.1429 - mae: 2.0285 - val_loss: 7.5530 - val_mae: 2.2270\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.2497 - mae: 2.0765 - val_loss: 7.4448 - val_mae: 2.2676\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0020 - mae: 2.0381 - val_loss: 7.3029 - val_mae: 2.2128\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.8123 - mae: 2.0389 - val_loss: 8.4966 - val_mae: 2.3933\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0799 - mae: 2.0503 - val_loss: 9.2608 - val_mae: 2.3876\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.8625 - mae: 2.0501 - val_loss: 8.0284 - val_mae: 2.2670\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.8173 - mae: 2.0804 - val_loss: 7.5312 - val_mae: 2.2251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6001 - mae: 2.0473 - val_loss: 7.3241 - val_mae: 2.2239\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5382 - mae: 1.9615 - val_loss: 7.7634 - val_mae: 2.2262\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4879 - mae: 2.0589 - val_loss: 7.7028 - val_mae: 2.2407\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5706 - mae: 2.0231 - val_loss: 9.2503 - val_mae: 2.3938\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6383 - mae: 2.0081 - val_loss: 7.2585 - val_mae: 2.1449\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5260 - mae: 2.0096 - val_loss: 7.2727 - val_mae: 2.2183\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1614 - mae: 1.9132 - val_loss: 7.3156 - val_mae: 2.2015\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3605 - mae: 1.9420 - val_loss: 8.4015 - val_mae: 2.2648\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.2269 - mae: 2.0605 - val_loss: 8.8173 - val_mae: 2.3640\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3666 - mae: 1.9980 - val_loss: 7.3687 - val_mae: 2.1688\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1854 - mae: 2.0141 - val_loss: 7.2983 - val_mae: 2.2406\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0369 - mae: 1.8999 - val_loss: 7.7537 - val_mae: 2.2381\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0192 - mae: 1.9495 - val_loss: 7.2845 - val_mae: 2.2005\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1051 - mae: 1.9430 - val_loss: 6.8417 - val_mae: 2.1669\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1028 - mae: 1.9175 - val_loss: 7.2916 - val_mae: 2.1514\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0386 - mae: 1.9534 - val_loss: 6.7753 - val_mae: 2.1048\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8589 - mae: 1.9229 - val_loss: 7.9762 - val_mae: 2.2124\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8794 - mae: 1.9414 - val_loss: 7.1857 - val_mae: 2.1969\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7448 - mae: 1.8811 - val_loss: 7.0722 - val_mae: 2.1671\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7417 - mae: 1.9231 - val_loss: 7.0913 - val_mae: 2.1883\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6596 - mae: 1.8673 - val_loss: 6.7645 - val_mae: 2.1171\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.4829 - mae: 1.8666 - val_loss: 6.6431 - val_mae: 2.1220\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4736 - mae: 1.8813 - val_loss: 7.4621 - val_mae: 2.2449\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4917 - mae: 1.8636 - val_loss: 6.8418 - val_mae: 2.1661\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6846 - mae: 1.8866 - val_loss: 7.1336 - val_mae: 2.1857\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3066 - mae: 1.8586 - val_loss: 8.1759 - val_mae: 2.3161\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4849 - mae: 1.8764 - val_loss: 9.2821 - val_mae: 2.4263\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4330 - mae: 1.8842 - val_loss: 6.6467 - val_mae: 2.1243\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1647 - mae: 1.8218 - val_loss: 6.8053 - val_mae: 2.1243\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1888 - mae: 1.8255 - val_loss: 6.6812 - val_mae: 2.0693\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1372 - mae: 1.8362 - val_loss: 7.1608 - val_mae: 2.1457\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3531 - mae: 1.8605 - val_loss: 7.0014 - val_mae: 2.1329\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0965 - mae: 1.8426 - val_loss: 7.0110 - val_mae: 2.1672\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0702 - mae: 1.8166 - val_loss: 6.8464 - val_mae: 2.0636\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0933 - mae: 1.8309 - val_loss: 8.3672 - val_mae: 2.2997\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9580 - mae: 1.8438 - val_loss: 6.2884 - val_mae: 2.0440\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9687 - mae: 1.7938 - val_loss: 8.6628 - val_mae: 2.2413\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0602 - mae: 1.8458 - val_loss: 7.0303 - val_mae: 2.0971\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2195 - mae: 1.8490 - val_loss: 7.5594 - val_mae: 2.2092\n",
      "Epoch 101/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 6.7644 - mae: 1.9286\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8946 - mae: 1.8001 - val_loss: 6.6907 - val_mae: 2.0729\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8725 - mae: 1.7922 - val_loss: 7.2418 - val_mae: 2.0646\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9186 - mae: 1.7856 - val_loss: 6.7325 - val_mae: 2.0365\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9384 - mae: 1.7909 - val_loss: 6.7454 - val_mae: 2.1122\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6538 - mae: 1.7689 - val_loss: 6.6439 - val_mae: 2.1062\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9271 - mae: 1.8066 - val_loss: 6.8108 - val_mae: 2.0822\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6908 - mae: 1.7700 - val_loss: 6.5382 - val_mae: 2.0552\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5786 - mae: 1.7735 - val_loss: 6.9789 - val_mae: 2.0985\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7028 - mae: 1.8090 - val_loss: 6.5749 - val_mae: 2.0864\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5682 - mae: 1.7697 - val_loss: 6.6414 - val_mae: 2.0810\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6804 - mae: 1.7697 - val_loss: 6.4961 - val_mae: 1.9891\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5885 - mae: 1.7734 - val_loss: 6.2454 - val_mae: 2.0437\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4490 - mae: 1.7225 - val_loss: 8.1526 - val_mae: 2.1778\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8948 - mae: 1.8691 - val_loss: 6.3804 - val_mae: 2.0921\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3006 - mae: 1.7195 - val_loss: 6.3117 - val_mae: 2.0930\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3506 - mae: 1.7350 - val_loss: 6.2878 - val_mae: 2.0228\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3422 - mae: 1.7132 - val_loss: 6.7798 - val_mae: 2.0190\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4337 - mae: 1.7829 - val_loss: 6.2746 - val_mae: 2.0505\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2019 - mae: 1.6944 - val_loss: 10.3343 - val_mae: 2.5729\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4670 - mae: 1.8283 - val_loss: 7.1010 - val_mae: 2.1677\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1330 - mae: 1.7134 - val_loss: 6.5144 - val_mae: 1.9849\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3580 - mae: 1.7751 - val_loss: 6.0477 - val_mae: 2.0338\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0917 - mae: 1.6711 - val_loss: 6.6887 - val_mae: 2.0963\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2034 - mae: 1.7033 - val_loss: 7.0160 - val_mae: 2.0950\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1931 - mae: 1.7310 - val_loss: 7.2247 - val_mae: 2.0992\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9079 - mae: 1.6843 - val_loss: 6.5171 - val_mae: 1.9723\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0840 - mae: 1.7023 - val_loss: 6.1228 - val_mae: 2.0291\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0630 - mae: 1.6866 - val_loss: 6.6832 - val_mae: 1.9745\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9815 - mae: 1.7037 - val_loss: 5.9742 - val_mae: 1.9839\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.0016 - mae: 1.6898 - val_loss: 6.0675 - val_mae: 1.9176\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8837 - mae: 1.6960 - val_loss: 7.2890 - val_mae: 2.2062\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9039 - mae: 1.6548 - val_loss: 6.0903 - val_mae: 1.9923\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7798 - mae: 1.6558 - val_loss: 7.1897 - val_mae: 2.1818\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9045 - mae: 1.6941 - val_loss: 6.1182 - val_mae: 2.0161\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6361 - mae: 1.6332 - val_loss: 6.1106 - val_mae: 2.0091\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9040 - mae: 1.6480 - val_loss: 6.8410 - val_mae: 2.0774\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8144 - mae: 1.6937 - val_loss: 6.5290 - val_mae: 2.0516\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6715 - mae: 1.6329 - val_loss: 6.0938 - val_mae: 1.9892\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8206 - mae: 1.6647 - val_loss: 6.3343 - val_mae: 2.0856\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5926 - mae: 1.6199 - val_loss: 6.2958 - val_mae: 2.0113\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4630 - mae: 1.6297 - val_loss: 6.1256 - val_mae: 2.0023\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6559 - mae: 1.6231 - val_loss: 5.9804 - val_mae: 2.0089\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4251 - mae: 1.6084 - val_loss: 6.4034 - val_mae: 2.0515\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5390 - mae: 1.6107 - val_loss: 6.0390 - val_mae: 2.0107\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4835 - mae: 1.5912 - val_loss: 6.7658 - val_mae: 1.9850\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3927 - mae: 1.6337 - val_loss: 6.7881 - val_mae: 2.0842\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3879 - mae: 1.6012 - val_loss: 7.6235 - val_mae: 2.2005\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4356 - mae: 1.6286 - val_loss: 7.2257 - val_mae: 2.2136\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3421 - mae: 1.5873 - val_loss: 6.8550 - val_mae: 2.0637\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4723 - mae: 1.6110 - val_loss: 6.3220 - val_mae: 1.9437\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1845 - mae: 1.5952 - val_loss: 6.7453 - val_mae: 2.1256\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5146 - mae: 1.5963 - val_loss: 6.6047 - val_mae: 2.0900\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2806 - mae: 1.5522 - val_loss: 6.0641 - val_mae: 2.0044\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0893 - mae: 1.5573 - val_loss: 6.1383 - val_mae: 1.9878\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2040 - mae: 1.5696 - val_loss: 6.3170 - val_mae: 2.0128\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1625 - mae: 1.5538 - val_loss: 6.1088 - val_mae: 1.9980\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2651 - mae: 1.5854 - val_loss: 5.9986 - val_mae: 1.9773\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0664 - mae: 1.5516 - val_loss: 6.1117 - val_mae: 1.9706\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0545 - mae: 1.5305 - val_loss: 6.1241 - val_mae: 1.9556\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1074 - mae: 1.5534 - val_loss: 8.2522 - val_mae: 2.3073\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1167 - mae: 1.5720 - val_loss: 6.8254 - val_mae: 2.1006\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0777 - mae: 1.5148 - val_loss: 6.3086 - val_mae: 1.9859\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8951 - mae: 1.5291 - val_loss: 6.3826 - val_mae: 1.9673\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9086 - mae: 1.5193 - val_loss: 6.2924 - val_mae: 1.8979\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9515 - mae: 1.5366 - val_loss: 6.8364 - val_mae: 2.1107\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0354 - mae: 1.5325 - val_loss: 5.8373 - val_mae: 1.9452\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8009 - mae: 1.4770 - val_loss: 9.9382 - val_mae: 2.5645\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1660 - mae: 1.5653 - val_loss: 5.9234 - val_mae: 1.9606\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6960 - mae: 1.4804 - val_loss: 6.8218 - val_mae: 2.1053\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0111 - mae: 1.5173 - val_loss: 6.5694 - val_mae: 2.0587\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6553 - mae: 1.4707 - val_loss: 6.6539 - val_mae: 2.0111\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7845 - mae: 1.5351 - val_loss: 5.9501 - val_mae: 1.9668\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8357 - mae: 1.4873 - val_loss: 5.9665 - val_mae: 1.9805\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6618 - mae: 1.4538 - val_loss: 6.2975 - val_mae: 1.9861\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5875 - mae: 1.4729 - val_loss: 5.9912 - val_mae: 1.9521\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6050 - mae: 1.4748 - val_loss: 6.6431 - val_mae: 1.9738\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.8895 - mae: 1.5216 - val_loss: 6.4910 - val_mae: 2.0242\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6858 - mae: 1.4742 - val_loss: 6.4119 - val_mae: 2.0388\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.6534 - mae: 1.4752 - val_loss: 5.9321 - val_mae: 1.9741\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5316 - mae: 1.4593 - val_loss: 6.3597 - val_mae: 2.0507\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.7497 - mae: 1.4885 - val_loss: 5.7778 - val_mae: 1.9255\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4612 - mae: 1.4436 - val_loss: 7.0919 - val_mae: 2.1195\n",
      "Epoch 183/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5956 - mae: 1.4674 - val_loss: 5.9519 - val_mae: 1.9060\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4733 - mae: 1.4452 - val_loss: 9.1242 - val_mae: 2.3321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5038 - mae: 1.4855 - val_loss: 6.8155 - val_mae: 2.0867\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5640 - mae: 1.4667 - val_loss: 5.9858 - val_mae: 1.9328\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4268 - mae: 1.4395 - val_loss: 8.5780 - val_mae: 2.3136\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5234 - mae: 1.5078 - val_loss: 5.8981 - val_mae: 1.9570\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3690 - mae: 1.4099 - val_loss: 6.3358 - val_mae: 2.0352\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2618 - mae: 1.4062 - val_loss: 7.3069 - val_mae: 1.9764\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.4367 - mae: 1.4939 - val_loss: 6.3431 - val_mae: 1.9492\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2909 - mae: 1.4413 - val_loss: 6.0547 - val_mae: 1.9750\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2036 - mae: 1.3997 - val_loss: 7.5464 - val_mae: 2.0583\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.5227 - mae: 1.4565 - val_loss: 5.8630 - val_mae: 1.9053\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2788 - mae: 1.4322 - val_loss: 6.1435 - val_mae: 1.9813\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.3164 - mae: 1.4381 - val_loss: 5.8854 - val_mae: 1.9243\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 4.2756 - mae: 1.4421 - val_loss: 6.0120 - val_mae: 1.8980\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1245 - mae: 1.4290 - val_loss: 6.5226 - val_mae: 2.0617\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2222 - mae: 1.4339 - val_loss: 6.4333 - val_mae: 2.0454\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1266 - mae: 1.4080 - val_loss: 6.3064 - val_mae: 1.9289\n",
      "Epoch 201/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 3.7614 - mae: 1.4707\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1562 - mae: 1.4082 - val_loss: 5.9083 - val_mae: 1.8737\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 4.1001 - mae: 1.3850 - val_loss: 7.4158 - val_mae: 2.0633\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2451 - mae: 1.4380 - val_loss: 6.0570 - val_mae: 1.9518\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0845 - mae: 1.3915 - val_loss: 7.7984 - val_mae: 2.2153\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0491 - mae: 1.3946 - val_loss: 6.1868 - val_mae: 1.9710\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0993 - mae: 1.3991 - val_loss: 6.5772 - val_mae: 1.9857\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0361 - mae: 1.3839 - val_loss: 6.2589 - val_mae: 2.0011\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8500 - mae: 1.3518 - val_loss: 6.2909 - val_mae: 2.0250\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9248 - mae: 1.3374 - val_loss: 10.4182 - val_mae: 2.4984\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.2309 - mae: 1.4537 - val_loss: 6.8588 - val_mae: 2.0866\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.0025 - mae: 1.3762 - val_loss: 7.0613 - val_mae: 2.0854\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8491 - mae: 1.3630 - val_loss: 7.0441 - val_mae: 2.1343\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.9052 - mae: 1.3607 - val_loss: 8.2560 - val_mae: 2.2515\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8827 - mae: 1.3696 - val_loss: 8.7746 - val_mae: 2.3687\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7817 - mae: 1.3534 - val_loss: 6.0987 - val_mae: 1.9331\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.9457 - mae: 1.3778 - val_loss: 6.3003 - val_mae: 2.0143\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.7753 - mae: 1.3513 - val_loss: 7.1622 - val_mae: 2.1199\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 3.7969 - mae: 1.3118 - val_loss: 7.0584 - val_mae: 2.0689\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6959 - mae: 1.3399 - val_loss: 6.8753 - val_mae: 2.0446\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6863 - mae: 1.3731 - val_loss: 7.5653 - val_mae: 2.1524\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.1373 - mae: 1.4064 - val_loss: 7.1709 - val_mae: 1.9831\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7491 - mae: 1.3657 - val_loss: 6.1455 - val_mae: 1.9752\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6558 - mae: 1.3165 - val_loss: 6.0743 - val_mae: 1.8992\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.7791 - mae: 1.3457 - val_loss: 6.2688 - val_mae: 1.9881\n",
      "Epoch 225/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8248 - mae: 1.3566 - val_loss: 6.6055 - val_mae: 2.0527\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5558 - mae: 1.2918 - val_loss: 8.4670 - val_mae: 2.2654\n",
      "Epoch 227/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.8350 - mae: 1.3827 - val_loss: 6.3571 - val_mae: 2.0031\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5442 - mae: 1.3195 - val_loss: 6.7740 - val_mae: 1.9898\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5321 - mae: 1.3257 - val_loss: 6.7075 - val_mae: 2.0765\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6298 - mae: 1.3217 - val_loss: 7.2372 - val_mae: 1.9428\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6833 - mae: 1.3798 - val_loss: 6.7924 - val_mae: 1.9739\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4644 - mae: 1.2976 - val_loss: 6.0637 - val_mae: 1.9181\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6529 - mae: 1.3480 - val_loss: 5.9615 - val_mae: 1.8824\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5690 - mae: 1.2821 - val_loss: 6.9138 - val_mae: 2.0991\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5327 - mae: 1.2761 - val_loss: 6.5604 - val_mae: 2.0458\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5698 - mae: 1.3077 - val_loss: 6.2831 - val_mae: 1.9135\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4910 - mae: 1.2945 - val_loss: 6.8076 - val_mae: 2.0229\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4324 - mae: 1.3092 - val_loss: 6.9544 - val_mae: 2.1075\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.6665 - mae: 1.3224 - val_loss: 6.5366 - val_mae: 2.0017\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4176 - mae: 1.2961 - val_loss: 6.2143 - val_mae: 1.8943\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.5134 - mae: 1.3177 - val_loss: 6.0871 - val_mae: 1.8697\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3259 - mae: 1.2637 - val_loss: 6.2689 - val_mae: 1.9317\n",
      "Epoch 243/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3433 - mae: 1.2562 - val_loss: 7.0196 - val_mae: 2.0831\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4850 - mae: 1.2787 - val_loss: 6.2670 - val_mae: 1.9649\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3675 - mae: 1.2625 - val_loss: 6.1438 - val_mae: 1.8547\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3411 - mae: 1.2593 - val_loss: 7.3234 - val_mae: 2.0565\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4729 - mae: 1.2866 - val_loss: 6.0357 - val_mae: 1.8711\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2505 - mae: 1.2636 - val_loss: 6.9633 - val_mae: 2.0367\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.4973 - mae: 1.2836 - val_loss: 7.3932 - val_mae: 2.0446\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3702 - mae: 1.2862 - val_loss: 6.5035 - val_mae: 1.9187\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3075 - mae: 1.2594 - val_loss: 6.3916 - val_mae: 1.8596\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3312 - mae: 1.2825 - val_loss: 7.0994 - val_mae: 2.0908\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2020 - mae: 1.2463 - val_loss: 7.1421 - val_mae: 2.0338\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2782 - mae: 1.2370 - val_loss: 6.3947 - val_mae: 1.9703\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2551 - mae: 1.2480 - val_loss: 7.0161 - val_mae: 1.9955\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1951 - mae: 1.2310 - val_loss: 6.4629 - val_mae: 1.9587\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2988 - mae: 1.2844 - val_loss: 7.1400 - val_mae: 1.9893\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1668 - mae: 1.2761 - val_loss: 9.6448 - val_mae: 2.4917\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.3596 - mae: 1.2593 - val_loss: 7.1378 - val_mae: 2.0975\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0921 - mae: 1.1958 - val_loss: 7.2410 - val_mae: 2.0716\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2646 - mae: 1.2510 - val_loss: 6.6477 - val_mae: 1.9105\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1153 - mae: 1.2318 - val_loss: 7.0939 - val_mae: 1.9524\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1587 - mae: 1.2506 - val_loss: 6.3407 - val_mae: 1.8998\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0353 - mae: 1.2140 - val_loss: 6.6080 - val_mae: 1.9968\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.1358 - mae: 1.2196 - val_loss: 6.6191 - val_mae: 1.9747\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0536 - mae: 1.2030 - val_loss: 7.8071 - val_mae: 2.1522\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.2537 - mae: 1.2472 - val_loss: 8.0117 - val_mae: 2.2075\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9956 - mae: 1.1889 - val_loss: 9.5131 - val_mae: 2.4151\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0023 - mae: 1.2521 - val_loss: 6.4338 - val_mae: 1.8982\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0345 - mae: 1.2030 - val_loss: 6.4835 - val_mae: 1.8838\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9255 - mae: 1.2067 - val_loss: 6.4218 - val_mae: 1.8909\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9438 - mae: 1.2000 - val_loss: 6.1377 - val_mae: 1.8688\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9685 - mae: 1.1740 - val_loss: 6.4711 - val_mae: 1.9255\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9161 - mae: 1.1668 - val_loss: 7.0413 - val_mae: 2.0245\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9805 - mae: 1.1756 - val_loss: 6.4225 - val_mae: 1.8585\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9577 - mae: 1.1859 - val_loss: 6.2757 - val_mae: 1.8910\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9852 - mae: 1.1903 - val_loss: 6.6439 - val_mae: 1.9708\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8415 - mae: 1.1917 - val_loss: 8.2828 - val_mae: 2.1694\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9454 - mae: 1.1747 - val_loss: 11.2555 - val_mae: 2.6271\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 3.0326 - mae: 1.2452 - val_loss: 6.6283 - val_mae: 1.9200\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8774 - mae: 1.1723 - val_loss: 7.5002 - val_mae: 1.9148\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8936 - mae: 1.1846 - val_loss: 6.7077 - val_mae: 1.9552\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8120 - mae: 1.1564 - val_loss: 9.6420 - val_mae: 2.3733\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9727 - mae: 1.2049 - val_loss: 8.4718 - val_mae: 2.2648\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9022 - mae: 1.1966 - val_loss: 6.2202 - val_mae: 1.8708\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7632 - mae: 1.1522 - val_loss: 8.3049 - val_mae: 2.1304\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.9607 - mae: 1.2013 - val_loss: 8.6140 - val_mae: 2.1266\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8631 - mae: 1.1918 - val_loss: 6.9161 - val_mae: 1.9298\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6846 - mae: 1.1735 - val_loss: 6.5838 - val_mae: 1.9558\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6289 - mae: 1.1268 - val_loss: 7.6503 - val_mae: 2.0587\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8251 - mae: 1.1972 - val_loss: 8.4010 - val_mae: 2.1460\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8714 - mae: 1.1801 - val_loss: 6.6922 - val_mae: 1.9453\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8379 - mae: 1.1538 - val_loss: 6.4824 - val_mae: 1.9349\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.7389 - mae: 1.1444 - val_loss: 7.3891 - val_mae: 2.0474\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6430 - mae: 1.1460 - val_loss: 7.3879 - val_mae: 2.0932\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.7953 - mae: 1.1904 - val_loss: 7.4167 - val_mae: 1.9954\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8288 - mae: 1.1612 - val_loss: 8.1717 - val_mae: 2.1261\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6368 - mae: 1.1434 - val_loss: 8.0389 - val_mae: 2.1314\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7450 - mae: 1.1451 - val_loss: 6.9466 - val_mae: 1.9086\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.6592 - mae: 1.1544 - val_loss: 7.0562 - val_mae: 1.9985\n",
      "Epoch 301/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 1.4153 - mae: 0.9374\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7163 - mae: 1.1632 - val_loss: 8.0708 - val_mae: 2.1532\n",
      "Epoch 302/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6179 - mae: 1.1275 - val_loss: 6.3388 - val_mae: 1.8915\n",
      "Epoch 303/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6040 - mae: 1.1014 - val_loss: 6.6184 - val_mae: 1.9433\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6942 - mae: 1.1629 - val_loss: 10.2591 - val_mae: 2.4893\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6923 - mae: 1.1491 - val_loss: 9.4677 - val_mae: 2.3881\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6973 - mae: 1.1805 - val_loss: 6.3286 - val_mae: 1.8926\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6726 - mae: 1.1390 - val_loss: 8.3378 - val_mae: 2.1679\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5423 - mae: 1.1292 - val_loss: 8.8853 - val_mae: 2.2812\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.7228 - mae: 1.1912 - val_loss: 7.4756 - val_mae: 2.1104\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5384 - mae: 1.0956 - val_loss: 6.6622 - val_mae: 1.8768\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5392 - mae: 1.1174 - val_loss: 7.5803 - val_mae: 2.1303\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5473 - mae: 1.1311 - val_loss: 6.9367 - val_mae: 1.9563\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.6116 - mae: 1.1686 - val_loss: 7.3626 - val_mae: 1.9672\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5155 - mae: 1.0894 - val_loss: 7.0493 - val_mae: 1.9288\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5889 - mae: 1.1584 - val_loss: 6.6558 - val_mae: 1.9144\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4941 - mae: 1.0786 - val_loss: 6.9421 - val_mae: 1.8984\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4308 - mae: 1.1233 - val_loss: 8.5752 - val_mae: 2.2958\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5433 - mae: 1.1214 - val_loss: 7.7062 - val_mae: 2.2174\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4524 - mae: 1.0610 - val_loss: 7.0127 - val_mae: 1.9577\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.8009 - mae: 1.1926 - val_loss: 6.5721 - val_mae: 1.9817\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5117 - mae: 1.1098 - val_loss: 6.7409 - val_mae: 2.0007\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3510 - mae: 1.0581 - val_loss: 7.4906 - val_mae: 2.0946\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3733 - mae: 1.0720 - val_loss: 12.0986 - val_mae: 2.6948\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5929 - mae: 1.1367 - val_loss: 8.6874 - val_mae: 2.1216\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5008 - mae: 1.1411 - val_loss: 6.9942 - val_mae: 1.9919\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5314 - mae: 1.1332 - val_loss: 7.5181 - val_mae: 2.1053\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5507 - mae: 1.1237 - val_loss: 6.9668 - val_mae: 1.9740\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4543 - mae: 1.0833 - val_loss: 7.1765 - val_mae: 2.0633\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5105 - mae: 1.1077 - val_loss: 6.8587 - val_mae: 1.9757\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2773 - mae: 1.0427 - val_loss: 7.4239 - val_mae: 2.1156\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2783 - mae: 1.0482 - val_loss: 7.2300 - val_mae: 1.9314\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.4754 - mae: 1.0977 - val_loss: 10.0904 - val_mae: 2.3426\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3644 - mae: 1.1012 - val_loss: 7.5451 - val_mae: 2.0105\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4577 - mae: 1.1090 - val_loss: 8.4343 - val_mae: 2.2476\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2996 - mae: 1.0522 - val_loss: 6.7677 - val_mae: 1.9149\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3262 - mae: 1.0654 - val_loss: 7.0514 - val_mae: 1.9715\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2600 - mae: 1.0625 - val_loss: 6.9478 - val_mae: 1.9990\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5094 - mae: 1.1435 - val_loss: 7.0083 - val_mae: 1.9528\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.4030 - mae: 1.1124 - val_loss: 6.7924 - val_mae: 1.9298\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2347 - mae: 1.0514 - val_loss: 7.1250 - val_mae: 2.0588\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2952 - mae: 1.0529 - val_loss: 9.3144 - val_mae: 2.2367\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.3314 - mae: 1.1065 - val_loss: 7.9827 - val_mae: 2.1381\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2013 - mae: 1.0189 - val_loss: 7.5588 - val_mae: 1.9826\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3442 - mae: 1.0981 - val_loss: 7.0759 - val_mae: 2.0022\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2974 - mae: 1.0592 - val_loss: 7.0734 - val_mae: 1.9187\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2470 - mae: 1.0470 - val_loss: 9.1861 - val_mae: 2.2636\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3377 - mae: 1.0770 - val_loss: 8.0402 - val_mae: 2.2182\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1695 - mae: 1.0364 - val_loss: 10.6500 - val_mae: 2.4786\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.5611 - mae: 1.1427 - val_loss: 8.6720 - val_mae: 2.2294\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3835 - mae: 1.1080 - val_loss: 7.7903 - val_mae: 2.0769\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2412 - mae: 1.0620 - val_loss: 6.8251 - val_mae: 1.9250\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1272 - mae: 1.0284 - val_loss: 7.7757 - val_mae: 1.9857\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2739 - mae: 1.0653 - val_loss: 8.4281 - val_mae: 2.1461\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2475 - mae: 1.0663 - val_loss: 7.1766 - val_mae: 2.0462\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.2971 - mae: 1.0833 - val_loss: 7.1439 - val_mae: 1.9537\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1169 - mae: 1.0168 - val_loss: 9.3594 - val_mae: 2.2211\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2136 - mae: 1.0932 - val_loss: 7.9788 - val_mae: 2.1582\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1023 - mae: 1.0376 - val_loss: 7.8434 - val_mae: 2.1861\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2875 - mae: 1.0282 - val_loss: 11.4563 - val_mae: 2.6325\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3514 - mae: 1.0736 - val_loss: 7.3520 - val_mae: 1.9556\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.3325 - mae: 1.0775 - val_loss: 7.1028 - val_mae: 1.9835\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0628 - mae: 1.0163 - val_loss: 9.8918 - val_mae: 2.4150\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0755 - mae: 1.0092 - val_loss: 10.3551 - val_mae: 2.4080\n",
      "Epoch 364/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2490 - mae: 1.0660 - val_loss: 8.2901 - val_mae: 2.1769\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1054 - mae: 1.0095 - val_loss: 8.8219 - val_mae: 2.2749\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1571 - mae: 1.0592 - val_loss: 7.1497 - val_mae: 1.9438\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0542 - mae: 1.0191 - val_loss: 7.3927 - val_mae: 2.0745\n",
      "Epoch 368/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1035 - mae: 1.0601 - val_loss: 8.1753 - val_mae: 2.0559\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0893 - mae: 1.0186 - val_loss: 8.8107 - val_mae: 2.2084\n",
      "Epoch 370/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2016 - mae: 1.0606 - val_loss: 6.8808 - val_mae: 1.9786\n",
      "Epoch 371/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0805 - mae: 1.0168 - val_loss: 7.4276 - val_mae: 2.0738\n",
      "Epoch 372/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1316 - mae: 1.0201 - val_loss: 7.8003 - val_mae: 2.0287\n",
      "Epoch 373/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.2095 - mae: 1.0541 - val_loss: 8.0570 - val_mae: 2.1865\n",
      "Epoch 374/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0927 - mae: 1.0317 - val_loss: 8.0794 - val_mae: 2.1628\n",
      "Epoch 375/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1141 - mae: 1.0125 - val_loss: 7.5845 - val_mae: 2.0692\n",
      "Epoch 376/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9136 - mae: 0.9630 - val_loss: 9.0721 - val_mae: 2.3363\n",
      "Epoch 377/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1354 - mae: 1.0424 - val_loss: 7.9925 - val_mae: 2.0310\n",
      "Epoch 378/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0994 - mae: 1.0352 - val_loss: 11.4368 - val_mae: 2.6489\n",
      "Epoch 379/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0882 - mae: 1.0173 - val_loss: 7.1346 - val_mae: 2.0042\n",
      "Epoch 380/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9672 - mae: 0.9811 - val_loss: 7.4791 - val_mae: 2.0068\n",
      "Epoch 381/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9668 - mae: 1.0016 - val_loss: 9.0316 - val_mae: 2.3074\n",
      "Epoch 382/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9455 - mae: 0.9898 - val_loss: 7.4677 - val_mae: 2.0638\n",
      "Epoch 383/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1153 - mae: 1.0273 - val_loss: 7.4912 - val_mae: 1.9599\n",
      "Epoch 384/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9473 - mae: 0.9888 - val_loss: 10.5353 - val_mae: 2.4418\n",
      "Epoch 385/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0576 - mae: 1.0403 - val_loss: 8.1773 - val_mae: 2.1117\n",
      "Epoch 386/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9701 - mae: 1.0063 - val_loss: 7.4228 - val_mae: 2.0296\n",
      "Epoch 387/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8694 - mae: 0.9797 - val_loss: 7.6300 - val_mae: 2.1196\n",
      "Epoch 388/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1778 - mae: 1.0475 - val_loss: 8.0932 - val_mae: 1.9878\n",
      "Epoch 389/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0504 - mae: 1.0104 - val_loss: 7.3751 - val_mae: 2.0505\n",
      "Epoch 390/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9700 - mae: 0.9717 - val_loss: 9.6457 - val_mae: 2.3903\n",
      "Epoch 391/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0231 - mae: 0.9722 - val_loss: 7.9273 - val_mae: 2.0678\n",
      "Epoch 392/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8874 - mae: 0.9801 - val_loss: 8.2972 - val_mae: 2.2649\n",
      "Epoch 393/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0771 - mae: 1.0080 - val_loss: 7.4996 - val_mae: 2.0633\n",
      "Epoch 394/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8523 - mae: 0.9738 - val_loss: 7.2698 - val_mae: 2.0096\n",
      "Epoch 395/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0042 - mae: 1.0148 - val_loss: 8.8445 - val_mae: 2.1831\n",
      "Epoch 396/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.8289 - mae: 0.9679 - val_loss: 7.4112 - val_mae: 2.0570\n",
      "Epoch 397/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8946 - mae: 0.9666 - val_loss: 10.2810 - val_mae: 2.4212\n",
      "Epoch 398/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9887 - mae: 1.0270 - val_loss: 8.8995 - val_mae: 2.2432\n",
      "Epoch 399/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7821 - mae: 0.9544 - val_loss: 7.5176 - val_mae: 2.0880\n",
      "Epoch 400/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.8682 - mae: 0.9714 - val_loss: 7.4183 - val_mae: 1.9940\n",
      "Epoch 401/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 2.3002 - mae: 1.2469\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8287 - mae: 0.9750 - val_loss: 10.0803 - val_mae: 2.1777\n",
      "Epoch 402/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0330 - mae: 1.0056 - val_loss: 7.8471 - val_mae: 2.1211\n",
      "Epoch 403/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9335 - mae: 0.9789 - val_loss: 8.0252 - val_mae: 2.0900\n",
      "Epoch 404/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.1829 - mae: 1.0318 - val_loss: 8.6572 - val_mae: 2.2953\n",
      "Epoch 405/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8295 - mae: 0.9630 - val_loss: 9.8705 - val_mae: 2.2939\n",
      "Epoch 406/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0361 - mae: 0.9968 - val_loss: 8.2289 - val_mae: 2.1361\n",
      "Epoch 407/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8750 - mae: 0.9806 - val_loss: 9.6055 - val_mae: 2.4003\n",
      "Epoch 408/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9982 - mae: 1.0033 - val_loss: 8.2636 - val_mae: 2.0959\n",
      "Epoch 409/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8265 - mae: 0.9518 - val_loss: 7.9690 - val_mae: 2.1698\n",
      "Epoch 410/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9369 - mae: 0.9790 - val_loss: 7.6802 - val_mae: 2.0774\n",
      "Epoch 411/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.8505 - mae: 0.9507 - val_loss: 10.6654 - val_mae: 2.5535\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9342 - mae: 0.9879 - val_loss: 8.0714 - val_mae: 2.1868\n",
      "Epoch 413/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6810 - mae: 0.9361 - val_loss: 8.1956 - val_mae: 2.1445\n",
      "Epoch 414/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9214 - mae: 0.9740 - val_loss: 8.5317 - val_mae: 2.2544\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.9467 - mae: 0.9977 - val_loss: 8.2464 - val_mae: 2.1719\n",
      "Epoch 416/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8350 - mae: 0.9690 - val_loss: 7.6608 - val_mae: 2.0345\n",
      "Epoch 417/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8022 - mae: 0.9295 - val_loss: 8.1488 - val_mae: 2.0572\n",
      "Epoch 418/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7847 - mae: 0.9720 - val_loss: 8.8306 - val_mae: 2.2872\n",
      "Epoch 419/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2.0475 - mae: 0.9777 - val_loss: 7.6833 - val_mae: 2.0394\n",
      "Epoch 420/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8023 - mae: 0.9447 - val_loss: 7.9928 - val_mae: 2.1241\n",
      "Epoch 421/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.8578 - mae: 0.9623 - val_loss: 7.7784 - val_mae: 2.0643\n",
      "Epoch 422/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7974 - mae: 0.9657 - val_loss: 9.6053 - val_mae: 2.2509\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7156 - mae: 0.9328 - val_loss: 9.1054 - val_mae: 2.3168\n",
      "Epoch 424/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.8857 - mae: 0.9722 - val_loss: 8.3483 - val_mae: 2.2216\n",
      "Epoch 425/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7531 - mae: 0.9176 - val_loss: 7.7912 - val_mae: 2.0474\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.7272 - mae: 0.9303 - val_loss: 7.9262 - val_mae: 2.0642\n",
      "Epoch 427/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8741 - mae: 0.9697 - val_loss: 7.6419 - val_mae: 2.0758\n",
      "Epoch 428/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6431 - mae: 0.9205 - val_loss: 7.8389 - val_mae: 2.0456\n",
      "Epoch 429/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6782 - mae: 0.9132 - val_loss: 7.7785 - val_mae: 2.0123\n",
      "Epoch 430/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8423 - mae: 0.9509 - val_loss: 7.6474 - val_mae: 2.0722\n",
      "Epoch 431/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7341 - mae: 0.9298 - val_loss: 7.3722 - val_mae: 1.9876\n",
      "Epoch 432/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8598 - mae: 0.9732 - val_loss: 7.7175 - val_mae: 2.0479\n",
      "Epoch 433/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6200 - mae: 0.8942 - val_loss: 8.3484 - val_mae: 2.2010\n",
      "Epoch 434/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6672 - mae: 0.8954 - val_loss: 8.4168 - val_mae: 2.2107\n",
      "Epoch 435/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7493 - mae: 0.9297 - val_loss: 8.3831 - val_mae: 2.2307\n",
      "Epoch 436/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7514 - mae: 0.9511 - val_loss: 8.1941 - val_mae: 2.1077\n",
      "Epoch 437/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7160 - mae: 0.9483 - val_loss: 7.6713 - val_mae: 2.0044\n",
      "Epoch 438/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6691 - mae: 0.9205 - val_loss: 8.1475 - val_mae: 2.1350\n",
      "Epoch 439/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6130 - mae: 0.8895 - val_loss: 9.0709 - val_mae: 2.2438\n",
      "Epoch 440/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8193 - mae: 0.9553 - val_loss: 7.9598 - val_mae: 2.1271\n",
      "Epoch 441/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5361 - mae: 0.8811 - val_loss: 7.9127 - val_mae: 2.0457\n",
      "Epoch 442/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6750 - mae: 0.9278 - val_loss: 7.5855 - val_mae: 2.0270\n",
      "Epoch 443/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5721 - mae: 0.9034 - val_loss: 8.6557 - val_mae: 2.2590\n",
      "Epoch 444/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7251 - mae: 0.9257 - val_loss: 8.1518 - val_mae: 2.1072\n",
      "Epoch 445/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.7578 - mae: 0.9620 - val_loss: 10.8592 - val_mae: 2.4941\n",
      "Epoch 446/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6824 - mae: 0.9077 - val_loss: 10.9811 - val_mae: 2.5516\n",
      "Epoch 447/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7018 - mae: 0.9079 - val_loss: 8.4398 - val_mae: 2.2028\n",
      "Epoch 448/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6508 - mae: 0.8955 - val_loss: 11.0072 - val_mae: 2.4953\n",
      "Epoch 449/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.7306 - mae: 0.9608 - val_loss: 8.5283 - val_mae: 2.1649\n",
      "Epoch 450/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6571 - mae: 0.9125 - val_loss: 7.6538 - val_mae: 2.0126\n",
      "Epoch 451/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5732 - mae: 0.8914 - val_loss: 8.2540 - val_mae: 2.0252\n",
      "Epoch 452/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6095 - mae: 0.8879 - val_loss: 7.9912 - val_mae: 2.1476\n",
      "Epoch 453/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6517 - mae: 0.9179 - val_loss: 9.2853 - val_mae: 2.3163\n",
      "Epoch 454/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8077 - mae: 0.9470 - val_loss: 8.3066 - val_mae: 2.1031\n",
      "Epoch 455/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6138 - mae: 0.9069 - val_loss: 8.3492 - val_mae: 2.1008\n",
      "Epoch 456/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5910 - mae: 0.8951 - val_loss: 9.4801 - val_mae: 2.3388\n",
      "Epoch 457/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5861 - mae: 0.9115 - val_loss: 8.5446 - val_mae: 2.2043\n",
      "Epoch 458/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5207 - mae: 0.8702 - val_loss: 9.5631 - val_mae: 2.3393\n",
      "Epoch 459/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.8590 - mae: 0.9726 - val_loss: 9.5891 - val_mae: 2.3995\n",
      "Epoch 460/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5584 - mae: 0.9057 - val_loss: 8.0454 - val_mae: 2.1920\n",
      "Epoch 461/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6162 - mae: 0.8955 - val_loss: 8.3794 - val_mae: 2.2311\n",
      "Epoch 462/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6647 - mae: 0.9286 - val_loss: 8.5185 - val_mae: 2.1828\n",
      "Epoch 463/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6002 - mae: 0.9089 - val_loss: 9.9086 - val_mae: 2.4364\n",
      "Epoch 464/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5744 - mae: 0.9177 - val_loss: 8.7764 - val_mae: 2.3234\n",
      "Epoch 465/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.5468 - mae: 0.8766 - val_loss: 8.2093 - val_mae: 2.0989\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5769 - mae: 0.8588 - val_loss: 8.6780 - val_mae: 2.1534\n",
      "Epoch 467/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6439 - mae: 0.9138 - val_loss: 11.4690 - val_mae: 2.6298\n",
      "Epoch 468/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6781 - mae: 0.9164 - val_loss: 9.5149 - val_mae: 2.2465\n",
      "Epoch 469/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6729 - mae: 0.9214 - val_loss: 9.0887 - val_mae: 2.2955\n",
      "Epoch 470/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5615 - mae: 0.8816 - val_loss: 7.6501 - val_mae: 2.1300\n",
      "Epoch 471/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5906 - mae: 0.8623 - val_loss: 9.8814 - val_mae: 2.4438\n",
      "Epoch 472/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5130 - mae: 0.8576 - val_loss: 8.1492 - val_mae: 2.0715\n",
      "Epoch 473/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4884 - mae: 0.8686 - val_loss: 8.8564 - val_mae: 2.1777\n",
      "Epoch 474/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6292 - mae: 0.9158 - val_loss: 8.4354 - val_mae: 2.2332\n",
      "Epoch 475/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3800 - mae: 0.8237 - val_loss: 8.5468 - val_mae: 2.1707\n",
      "Epoch 476/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4526 - mae: 0.8635 - val_loss: 9.0693 - val_mae: 2.3119\n",
      "Epoch 477/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.6805 - mae: 0.9282 - val_loss: 9.4599 - val_mae: 2.2554\n",
      "Epoch 478/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4839 - mae: 0.8741 - val_loss: 8.2658 - val_mae: 2.2039\n",
      "Epoch 479/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5897 - mae: 0.8852 - val_loss: 8.7592 - val_mae: 2.1197\n",
      "Epoch 480/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4976 - mae: 0.8830 - val_loss: 8.1733 - val_mae: 2.0741\n",
      "Epoch 481/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5911 - mae: 0.8983 - val_loss: 12.7083 - val_mae: 2.7751\n",
      "Epoch 482/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5802 - mae: 0.9048 - val_loss: 8.7991 - val_mae: 2.2530\n",
      "Epoch 483/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3816 - mae: 0.8193 - val_loss: 8.3117 - val_mae: 2.0910\n",
      "Epoch 484/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4357 - mae: 0.8591 - val_loss: 8.1847 - val_mae: 2.1884\n",
      "Epoch 485/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5533 - mae: 0.8808 - val_loss: 8.2372 - val_mae: 2.0680\n",
      "Epoch 486/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5658 - mae: 0.8990 - val_loss: 11.5757 - val_mae: 2.6655\n",
      "Epoch 487/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5002 - mae: 0.8728 - val_loss: 9.1270 - val_mae: 2.3120\n",
      "Epoch 488/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3626 - mae: 0.8229 - val_loss: 9.9439 - val_mae: 2.5032\n",
      "Epoch 489/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5995 - mae: 0.8752 - val_loss: 8.5182 - val_mae: 2.1302\n",
      "Epoch 490/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4821 - mae: 0.8425 - val_loss: 8.5502 - val_mae: 2.1094\n",
      "Epoch 491/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5367 - mae: 0.8824 - val_loss: 8.8786 - val_mae: 2.3171\n",
      "Epoch 492/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5292 - mae: 0.8849 - val_loss: 9.1953 - val_mae: 2.2986\n",
      "Epoch 493/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.4050 - mae: 0.8606 - val_loss: 9.0484 - val_mae: 2.2522\n",
      "Epoch 494/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3744 - mae: 0.8383 - val_loss: 8.7262 - val_mae: 2.2361\n",
      "Epoch 495/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3177 - mae: 0.7952 - val_loss: 8.4134 - val_mae: 2.1706\n",
      "Epoch 496/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3902 - mae: 0.8454 - val_loss: 11.1400 - val_mae: 2.3612\n",
      "Epoch 497/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5827 - mae: 0.9093 - val_loss: 8.7762 - val_mae: 2.2717\n",
      "Epoch 498/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5114 - mae: 0.8830 - val_loss: 8.7422 - val_mae: 2.2485\n",
      "Epoch 499/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.3020 - mae: 0.7970 - val_loss: 14.0668 - val_mae: 2.9058\n",
      "Epoch 500/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.5144 - mae: 0.8515 - val_loss: 8.3035 - val_mae: 2.0881\n"
     ]
    }
   ],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch.\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "# Store training stats\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model's training progress using the stats stored in the history object. We want to use this data to determine how long to train before the model stops making progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [1000$]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mae']), \n",
    "           label='Train mae')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mae']),\n",
    "           label = 'Val mae')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,5])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows little improvement in the model after about 200 epochs. Let's update the model.fit method to automatically stop training when the validation score doesn't improve. We'll use a callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 501.6305 - mae: 21.4076\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 521.6760 - mae: 21.0250 - val_loss: 498.4574 - val_mae: 20.3074\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 436.2768 - mae: 19.0240 - val_loss: 402.1305 - val_mae: 18.0203\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 351.0425 - mae: 16.8243 - val_loss: 309.5327 - val_mae: 15.4833\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 266.8212 - mae: 14.3627 - val_loss: 216.8212 - val_mae: 12.6322\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 187.1973 - mae: 11.6184 - val_loss: 145.5999 - val_mae: 9.8939\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 126.2525 - mae: 8.9699 - val_loss: 91.3905 - val_mae: 7.6167\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 82.3596 - mae: 6.7792 - val_loss: 65.5257 - val_mae: 6.5996\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 60.0212 - mae: 5.6287 - val_loss: 50.6004 - val_mae: 5.8234\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 47.2576 - mae: 4.9407 - val_loss: 40.8545 - val_mae: 5.2375\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 39.6491 - mae: 4.4621 - val_loss: 33.4716 - val_mae: 4.7370\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 33.9554 - mae: 4.1585 - val_loss: 27.7198 - val_mae: 4.3497\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 30.0753 - mae: 3.7917 - val_loss: 24.3801 - val_mae: 4.0589\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 27.0594 - mae: 3.6478 - val_loss: 20.5809 - val_mae: 3.7371\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 25.4948 - mae: 3.4941 - val_loss: 20.9459 - val_mae: 3.7139\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 24.0288 - mae: 3.3961 - val_loss: 18.6465 - val_mae: 3.4868\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 22.1914 - mae: 3.2757 - val_loss: 16.9417 - val_mae: 3.3051\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 20.7162 - mae: 3.1573 - val_loss: 17.3168 - val_mae: 3.3630\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 19.7282 - mae: 3.0679 - val_loss: 14.6332 - val_mae: 3.0199\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.6432 - mae: 2.9594 - val_loss: 14.0170 - val_mae: 2.9688\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 17.8293 - mae: 2.8973 - val_loss: 13.2201 - val_mae: 2.8086\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 12.7629 - mae: 2.768 - 0s 3ms/step - loss: 17.0803 - mae: 2.8082 - val_loss: 12.7293 - val_mae: 2.7742\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 16.4049 - mae: 2.8046 - val_loss: 13.3860 - val_mae: 2.9026\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.7895 - mae: 2.7621 - val_loss: 12.3626 - val_mae: 2.8165\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.1422 - mae: 2.6164 - val_loss: 11.9966 - val_mae: 2.7313\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.8115 - mae: 2.6462 - val_loss: 12.6472 - val_mae: 2.8260\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.4910 - mae: 2.6393 - val_loss: 12.2509 - val_mae: 2.7822\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.2201 - mae: 2.6129 - val_loss: 11.2581 - val_mae: 2.7006\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 13.6915 - mae: 2.5461 - val_loss: 11.1025 - val_mae: 2.6627\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.4798 - mae: 2.5254 - val_loss: 10.8916 - val_mae: 2.6370\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.2151 - mae: 2.4759 - val_loss: 14.5572 - val_mae: 3.1248\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.0990 - mae: 2.5138 - val_loss: 11.6338 - val_mae: 2.7489\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.6756 - mae: 2.4372 - val_loss: 10.2421 - val_mae: 2.5206\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.2748 - mae: 2.4188 - val_loss: 9.9901 - val_mae: 2.5587\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.1339 - mae: 2.3930 - val_loss: 10.1684 - val_mae: 2.5710\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.9792 - mae: 2.3346 - val_loss: 9.8832 - val_mae: 2.5304\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.8392 - mae: 2.3153 - val_loss: 10.6633 - val_mae: 2.6344\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.5715 - mae: 2.3376 - val_loss: 9.8863 - val_mae: 2.5425\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.2859 - mae: 2.3079 - val_loss: 9.5452 - val_mae: 2.5426\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.3646 - mae: 2.3200 - val_loss: 10.2274 - val_mae: 2.5108\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.1631 - mae: 2.2926 - val_loss: 9.2420 - val_mae: 2.4754\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 11.0199 - mae: 2.2666 - val_loss: 10.2632 - val_mae: 2.5985\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.9132 - mae: 2.2707 - val_loss: 9.7257 - val_mae: 2.5161\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.9795 - mae: 2.2461 - val_loss: 9.1069 - val_mae: 2.4697\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.3846 - mae: 2.2224 - val_loss: 9.7865 - val_mae: 2.5355\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.4113 - mae: 2.2080 - val_loss: 9.6330 - val_mae: 2.4931\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.5967 - mae: 2.2303 - val_loss: 8.8629 - val_mae: 2.4374\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.1699 - mae: 2.1986 - val_loss: 8.7046 - val_mae: 2.4145\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.2100 - mae: 2.1622 - val_loss: 8.9386 - val_mae: 2.4744\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 10.0486 - mae: 2.1517 - val_loss: 9.2309 - val_mae: 2.4875\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.9286 - mae: 2.1553 - val_loss: 9.1872 - val_mae: 2.4985\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.8126 - mae: 2.1231 - val_loss: 8.6750 - val_mae: 2.4210\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.6560 - mae: 2.1384 - val_loss: 10.6369 - val_mae: 2.6453\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.6379 - mae: 2.1492 - val_loss: 8.4473 - val_mae: 2.3739\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.4723 - mae: 2.1086 - val_loss: 9.4815 - val_mae: 2.5361\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.4179 - mae: 2.1123 - val_loss: 8.5204 - val_mae: 2.3703\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.3819 - mae: 2.0674 - val_loss: 9.8237 - val_mae: 2.5190\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.4955 - mae: 2.1011 - val_loss: 8.7377 - val_mae: 2.3875\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.2019 - mae: 2.0917 - val_loss: 8.9819 - val_mae: 2.4299\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.1561 - mae: 2.0523 - val_loss: 8.5307 - val_mae: 2.4252\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.0383 - mae: 2.0455 - val_loss: 9.1958 - val_mae: 2.5172\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 8.9610 - mae: 2.0445 - val_loss: 8.4018 - val_mae: 2.3523\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.9099 - mae: 2.0320 - val_loss: 8.8676 - val_mae: 2.3880\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.9046 - mae: 2.0258 - val_loss: 9.0068 - val_mae: 2.3852\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.7480 - mae: 2.0562 - val_loss: 8.2365 - val_mae: 2.3394\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.6962 - mae: 2.0085 - val_loss: 9.3030 - val_mae: 2.4349\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6197 - mae: 2.0435 - val_loss: 8.1331 - val_mae: 2.3390\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5995 - mae: 2.0070 - val_loss: 8.1515 - val_mae: 2.3506\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3991 - mae: 1.9917 - val_loss: 9.4402 - val_mae: 2.5006\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.0134 - mae: 2.0271 - val_loss: 8.3552 - val_mae: 2.3730\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.4894 - mae: 1.9543 - val_loss: 8.5747 - val_mae: 2.3763\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.2903 - mae: 1.9749 - val_loss: 8.2583 - val_mae: 2.3745\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.2062 - mae: 1.9500 - val_loss: 8.4106 - val_mae: 2.3442\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9816 - mae: 1.9721 - val_loss: 11.5058 - val_mae: 2.6767\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3826 - mae: 2.0299 - val_loss: 8.2648 - val_mae: 2.3707\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1393 - mae: 1.9456 - val_loss: 8.1656 - val_mae: 2.3505\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0221 - mae: 1.9352 - val_loss: 7.9508 - val_mae: 2.3190\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1205 - mae: 1.9677 - val_loss: 9.5450 - val_mae: 2.4190\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1867 - mae: 2.0274 - val_loss: 7.7607 - val_mae: 2.2529\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8594 - mae: 1.9236 - val_loss: 8.4024 - val_mae: 2.4032\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9584 - mae: 1.9301 - val_loss: 8.1562 - val_mae: 2.3584\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8351 - mae: 1.8963 - val_loss: 8.6298 - val_mae: 2.3294\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7484 - mae: 1.9255 - val_loss: 10.1404 - val_mae: 2.6258\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7065 - mae: 1.8759 - val_loss: 7.9164 - val_mae: 2.2453\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7375 - mae: 1.9337 - val_loss: 9.8148 - val_mae: 2.4713\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8400 - mae: 1.9787 - val_loss: 7.7527 - val_mae: 2.2511\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5858 - mae: 1.8874 - val_loss: 8.1220 - val_mae: 2.3567\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5596 - mae: 1.8843 - val_loss: 8.3462 - val_mae: 2.3226\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3535 - mae: 1.8580 - val_loss: 7.8522 - val_mae: 2.2901\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6203 - mae: 1.8790 - val_loss: 7.9809 - val_mae: 2.3311\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5852 - mae: 1.8573 - val_loss: 8.4004 - val_mae: 2.3619\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4952 - mae: 1.8515 - val_loss: 8.5776 - val_mae: 2.3186\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5468 - mae: 1.8559 - val_loss: 7.7050 - val_mae: 2.2184\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2861 - mae: 1.8606 - val_loss: 7.7250 - val_mae: 2.2217\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3312 - mae: 1.8443 - val_loss: 8.2356 - val_mae: 2.2993\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4801 - mae: 1.8819 - val_loss: 8.2205 - val_mae: 2.3626\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1514 - mae: 1.8291 - val_loss: 8.2776 - val_mae: 2.3807\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1647 - mae: 1.8407 - val_loss: 8.0466 - val_mae: 2.2964\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1859 - mae: 1.8116 - val_loss: 8.8558 - val_mae: 2.3481\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1790 - mae: 1.8194 - val_loss: 7.6644 - val_mae: 2.2085\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1873 - mae: 1.8221 - val_loss: 11.4765 - val_mae: 2.6325\n",
      "Epoch 101/500\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 5.8255 - mae: 1.8964\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2384 - mae: 1.8632 - val_loss: 7.8616 - val_mae: 2.2185\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0147 - mae: 1.8223 - val_loss: 8.0384 - val_mae: 2.3034\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0740 - mae: 1.8195 - val_loss: 7.9235 - val_mae: 2.2913\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0165 - mae: 1.7966 - val_loss: 7.9457 - val_mae: 2.2550\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9344 - mae: 1.7934 - val_loss: 10.0029 - val_mae: 2.5468\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.2258 - mae: 1.8430 - val_loss: 7.5241 - val_mae: 2.1875\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8640 - mae: 1.7717 - val_loss: 7.9728 - val_mae: 2.2752\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9633 - mae: 1.8161 - val_loss: 7.6051 - val_mae: 2.2314\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7654 - mae: 1.7608 - val_loss: 9.8815 - val_mae: 2.4669\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0113 - mae: 1.8330 - val_loss: 7.6802 - val_mae: 2.2257\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7545 - mae: 1.7708 - val_loss: 11.4823 - val_mae: 2.7395\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1384 - mae: 1.8448 - val_loss: 7.9867 - val_mae: 2.2900\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6748 - mae: 1.7770 - val_loss: 8.0204 - val_mae: 2.1974\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6729 - mae: 1.7911 - val_loss: 8.4688 - val_mae: 2.2296\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5512 - mae: 1.7553 - val_loss: 8.6328 - val_mae: 2.4037\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.6893 - mae: 1.7269 - val_loss: 7.7414 - val_mae: 2.1706\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.5310 - mae: 1.7507 - val_loss: 7.9291 - val_mae: 2.1948\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6100 - mae: 1.7534 - val_loss: 7.7724 - val_mae: 2.2677\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7503 - mae: 1.7421 - val_loss: 8.7809 - val_mae: 2.2545\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7508 - mae: 1.8120 - val_loss: 8.1088 - val_mae: 2.3780\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6816 - mae: 1.7468 - val_loss: 7.5677 - val_mae: 2.2099\n",
      "Epoch 122/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5925 - mae: 1.7687 - val_loss: 8.5435 - val_mae: 2.2638\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4355 - mae: 1.7539 - val_loss: 7.6080 - val_mae: 2.1596\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.3302 - mae: 1.7106 - val_loss: 8.7410 - val_mae: 2.3927\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.6532 - mae: 1.7363 - val_loss: 7.7257 - val_mae: 2.1538\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2793 - mae: 1.6964 - val_loss: 9.2024 - val_mae: 2.4242\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement.\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how did the model performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Mean Abs Error: $3184.51\n"
     ]
    }
   ],
   "source": [
    "[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.467021 18.478525 20.241821 32.487667 23.267378 19.935753 23.925894\n",
      " 19.362019 19.345188 21.042511 16.611588 15.947913 15.05253  40.40284\n",
      " 18.984264 18.738138 25.322021 17.562374 18.775425 27.202175 11.254208\n",
      " 13.873248 19.455494 16.289324 17.113075 24.213533 28.675505 26.74896\n",
      " 11.46329  18.34393  18.96711  14.396249 30.872004 23.172245 17.044868\n",
      "  9.381846 14.86984  17.692762 19.69315  23.576035 28.90406  25.537983\n",
      " 15.01252  42.11083  26.796223 24.275566 25.626177 17.487076 23.134007\n",
      " 21.293932 32.572754 18.424171 12.552039 14.691627 32.185623 25.967611\n",
      " 13.206102 46.86676  32.23148  21.386864 24.089268 17.274136 14.315688\n",
      " 18.495705 22.064404 18.383223 13.716554 19.93609  13.533074  7.882605\n",
      " 26.352188 28.02758  24.641842 13.86303  23.431252 15.527324 17.93497\n",
      " 22.993013 32.633636 12.289364 19.943043 36.70379  14.48315  14.182065\n",
      " 16.093273 16.147297 21.204535 19.551825 21.066742 32.07097  19.678492\n",
      " 17.124332 23.610239 41.566776 33.813065 18.931118 34.110176 51.312954\n",
      " 24.985178 48.380775 30.155107 21.370558]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(test_data).flatten()\n",
    "\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduced a few techniques to handle a regression problem.\n",
    "\n",
    "Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems). <br>\n",
    "Similarly, evaluation metrics used for regression differ from classification. <br> \n",
    "A common regression metric is Mean Absolute Error (MAE).<br>\n",
    "When input data features have values with different ranges, each feature should be scaled independently. <br>\n",
    "If there is not much training data, prefer a small network with few hidden layers to avoid overfitting. <br>\n",
    "Early stopping is a useful technique to prevent overfitting.<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
