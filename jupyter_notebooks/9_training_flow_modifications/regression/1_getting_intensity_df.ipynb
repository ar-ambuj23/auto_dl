{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/izenda/env/lib/python3.5/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, History \n",
    "\n",
    "from psqlConnector import deleteQuery,insertQuery,getRowCount\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fetch\n",
    "def data_fetch(csv_name):\n",
    "    try:\n",
    "        import subprocess\n",
    "        sub_output = subprocess.check_output('file -b --mime-encoding datasets/{}.csv'.format(csv_name), shell=True)\n",
    "        encoding_csv = sub_output.decode('utf-8')\n",
    "        df = pd.read_csv('datasets/{}.csv'.format(csv_name),index_col=0, encoding = encoding_csv)\n",
    "        \n",
    "        if(len(df.index)!=df.shape[0]):\n",
    "            print('There are duplicate values in the index column. \\nExiting the script...')\n",
    "            flush_memory()\n",
    "        \n",
    "        global train_columns\n",
    "        train_columns = df.columns\n",
    "        return df\n",
    "    except UnicodeError as e: \n",
    "        print(\"The {} csv has wrong encoding. Please use a csv with utf-8 encoding. \\nExiting the script...\".format(csv_name))\n",
    "        flush_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"The {} csv is not present in the datasets folder. \\nExiting the script...\".format(csv_name))\n",
    "        flush_memory()\n",
    "\n",
    "def data_fetch_test_array(csv_name):\n",
    "    try:\n",
    "        import subprocess\n",
    "        sub_output = subprocess.check_output('file -b --mime-encoding datasets/{}.csv'.format(csv_name), shell=True)\n",
    "        encoding_csv = sub_output.decode('utf-8')\n",
    "        df = pd.read_csv('datasets/{}.csv'.format(csv_name),index_col=0,encoding = encoding_csv)\n",
    "                  \n",
    "        if(len(df.index)!=df.shape[0]):\n",
    "            print('There are duplicate values in the index column of the test csv. \\nExiting the script...')\n",
    "            flush_memory()\n",
    "                  \n",
    "        return df\n",
    "    except UnicodeError as e: \n",
    "        print(\"The {} csv has wrong encoding. Please use a csv with utf-8 encoding. \\nExiting the script...\".format(csv_name))\n",
    "        flush_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"The {} csv is not present in the datasets folder. \\nExiting the script...\".format(csv_name))\n",
    "        flush_memory()\n",
    "\n",
    "        \n",
    "# Pre Processing\n",
    "def get_string_cols(df):    \n",
    "    string_cols = list(df.select_dtypes(include=['object','category']).columns)\n",
    "    return string_cols\n",
    "\n",
    "def get_num_cols(df):    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    num_cols = list(df.select_dtypes(include=numerics).columns)    \n",
    "    return num_cols\n",
    "\n",
    "def pre_process(df,label_col):\n",
    "    \n",
    "    try:\n",
    "        y = df[label_col]\n",
    "    except Exception as e:\n",
    "        print(\"The {} column in not present in the given csv. \\nExiting the script...\".format(label_col))\n",
    "        flush_memory()\n",
    "        \n",
    "    print(\"Pre-Processing the data...\")\n",
    "    \n",
    "    df = df.drop(label_col,axis=1)\n",
    "    string_cols = get_string_cols(df)\n",
    "    num_cols = get_num_cols(df)\n",
    "    substring = ':string'\n",
    "    num_cat_cols = []\n",
    "    for string in num_cols:\n",
    "        if(substring in string):\n",
    "            num_cat_cols.append(string)\n",
    "    for col in num_cat_cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "    categorical_cols = string_cols + num_cat_cols\n",
    "    print('Size of df before making dummies: ', df.memory_usage(deep=True).sum()/1024/1024, 'mb') \n",
    "    print('Converting categorical columns to dummies...') \n",
    "    df = pd.get_dummies(df,columns=categorical_cols,sparse=True)\n",
    "    print('Size of df after making dummies: ', df.memory_usage(deep=True).sum()/1024/1024, 'mb')\n",
    "    global training_dummy_columns\n",
    "    training_dummy_columns = df.columns\n",
    "    df.columns = df.columns.str.replace(':string','')\n",
    "    df[label_col] = y  \n",
    "    return df\n",
    "\n",
    "def pre_process_test_csv(df,label_col,training_dummy_columns):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    try:\n",
    "        y = df_copy[label_col]\n",
    "    except Exception as e:\n",
    "        print(\"The {} column in not present in the given test csv. \\nExiting the script...\".format(label_col))\n",
    "        flush_memory()\n",
    "\n",
    "    y = df_copy[label_col]\n",
    "    df_copy = df_copy.drop(label_col,axis=1)\n",
    "    string_cols = get_string_cols(df_copy)\n",
    "    num_cols = get_num_cols(df_copy)\n",
    "    substring = ':string'\n",
    "    num_cat_cols = []\n",
    "    for string in num_cols:\n",
    "        if(substring in string):\n",
    "            num_cat_cols.append(string)\n",
    "    for col in num_cat_cols:\n",
    "        df_copy[col] = df_copy[col].astype('object')\n",
    "    categorical_cols = string_cols + num_cat_cols\n",
    "    print('Size of df before making dummies: ', df_copy.memory_usage(deep=True).sum()/1024/1024, 'mb') \n",
    "    print('Converting categorical columns to dummies...')\n",
    "    df_copy = pd.get_dummies(df_copy,columns=categorical_cols,sparse=True)\n",
    "    print('Size of df after making dummies: ', df_copy.memory_usage(deep=True).sum()/1024/1024, 'mb')\n",
    "    missing_cols = set( training_dummy_columns ) - set( df_copy.columns )\n",
    "    for col in missing_cols:\n",
    "        df_copy.loc[:,col] = 0\n",
    "    df_copy = df_copy[training_dummy_columns]\n",
    "    df_copy.columns = df_copy.columns.str.replace(':string','')\n",
    "    df_copy[label_col] = y\n",
    "    return df_copy\n",
    "    \n",
    "# Train Test Split\n",
    "def split_train_test(df,label_col,test_size=0.2):    \n",
    "    X = df.loc[:,df.columns != label_col]\n",
    "    y = df[label_col]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "# Model Training\n",
    "def get_search_space():    \n",
    "    space = {'num_layers': hp.choice('num_layers',['one_hidden', 'two_hidden']),\n",
    "                'units1': hp.choice('units1', [32, 64, 128, 256,512]),\n",
    "                'units2': hp.choice('units2', [32, 64, 128, 256,512]),\n",
    "                'dropout1': hp.uniform('dropout1', .25,.75),\n",
    "                'dropout2': hp.uniform('dropout2',  .25,.75),\n",
    "                'batch_size' : hp.choice('batch_size', [16,32,64,128]),\n",
    "                'nb_epochs' :  500,\n",
    "                'optimizer': hp.choice('optimizer',['rmsprop', 'adam', 'nadam','sgd']),\n",
    "                'activation': hp.choice('activation',['relu','sigmoid']),\n",
    "                'early_stop_rounds': hp.choice('early_stop_rounds',[10,20,30,40,50]),\n",
    "            }\n",
    "    return space\n",
    "\n",
    "def data(csv_name,label_col,test_arg,use_lime,features_subset):    \n",
    "    print(\"Reading the data...\")\n",
    "    data = data_fetch(csv_name)\n",
    "    pre_processed_data = pre_process(df=data,label_col=label_col)\n",
    "    x_train, x_test, y_train, y_test = split_train_test(df=pre_processed_data,label_col=label_col)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    test_arg_processed = []\n",
    "    if(test_arg!=None):\n",
    "        for test_csv in test_arg:\n",
    "            test_dummy = data_fetch_test_array(test_csv)\n",
    "            test_dummy_processed = pre_process_test_csv(df=test_dummy,label_col=label_col,training_dummy_columns=training_dummy_columns)\n",
    "            test_arg_processed.append(test_dummy_processed)\n",
    "    if(use_lime!=None):\n",
    "        x_train_lime = x_train[features_subset]\n",
    "        x_test_lime = x_test[features_subset]\n",
    "        x_valid_lime = x_valid[features_subset]\n",
    "        y_train_lime = y_train\n",
    "        y_test_lime = y_test\n",
    "        y_valid_lime = y_valid\n",
    "        test_arg_processed_lime = []\n",
    "        for test_dummy in test_arg_processed:\n",
    "            y_test_dummy = test_dummy[label_col]\n",
    "            test_dummy = test_dummy[features_subset] \n",
    "            test_dummy_copy = test_dummy.copy()\n",
    "            test_dummy_copy[label_col] = y_test_dummy\n",
    "            test_arg_processed_lime.append(test_dummy_copy)\n",
    "        return data, x_train_lime, x_test_lime, x_valid_lime, y_train_lime, y_test_lime, y_valid_lime, test_arg_processed_lime\n",
    "    return data, x_train, x_test, x_valid, y_train, y_test, y_valid, test_arg_processed\n",
    "\n",
    "def create_model(params):    \n",
    "    x_train_temp = x_train.copy() \n",
    "    y_train_temp = y_train.copy()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    if(params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(params['units2']))\n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mse', metrics=['mse'],\n",
    "                  optimizer=params['optimizer'])\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['early_stop_rounds'])\n",
    "    terminate_nan = keras.callbacks.TerminateOnNaN()\n",
    "    history = History()\n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=500,\n",
    "              callbacks=[early_stop, terminate_nan, history],\n",
    "              verbose=0,\n",
    "              validation_data=(x_valid,y_valid)) \n",
    "    [loss, mse] = model.evaluate(x_valid,y_valid, verbose=0)\n",
    "    global num\n",
    "    mem = psutil.virtual_memory()\n",
    "    if(np.isnan(mse)):\n",
    "        print(\"{}) Validation set root mean sq. error: NaN\".format(num),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "        num = num + 1\n",
    "        return {'loss': np.inf, 'status': STATUS_OK, 'params': params}\n",
    "    print(\"{}) Validation set root mean sq. error: {:7.2f}\".format(num,mse**0.5),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "    num = num + 1\n",
    "    return {'loss': loss**0.5, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "def train_best_model(best_params):   \n",
    "    print('Training the best selected model...') \n",
    "    x_train_temp = x_train.copy() \n",
    "    y_train_temp = y_train.copy()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(best_params['activation']))\n",
    "    model.add(Dropout(best_params['dropout1']))\n",
    "    if(best_params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(best_params['units2']))\n",
    "        model.add(Activation(best_params['activation']))\n",
    "        model.add(Dropout(best_params['dropout2']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mse', metrics=['mse'],\n",
    "                  optimizer=best_params['optimizer'])\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=best_params['early_stop_rounds'])\n",
    "    history = History()\n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=best_params['batch_size'],\n",
    "              epochs=500,\n",
    "              callbacks=[early_stop, history],\n",
    "              verbose=0,\n",
    "              validation_data=(x_valid,y_valid)) \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_best_model(csv_name,label_col,test_arg,use_lime=None,features_subset=None):\n",
    "    global x_train, x_test, x_valid, y_train, y_test, y_valid\n",
    "    input_df, x_train, x_test, x_valid, y_train, y_test, y_valid, test_arg_processed = data(csv_name=csv_name,label_col=label_col,test_arg=test_arg,use_lime=use_lime,features_subset=features_subset)\n",
    "    trials=Trials()\n",
    "    space = get_search_space()\n",
    "    print(\"Selecting the best network architecture specifically for your data...\")\n",
    "    best = fmin(create_model, space, algo=tpe.suggest, max_evals=2, trials=trials)\n",
    "    best_trials_temp = trials.best_trial['result'] \n",
    "    best_model_temp = train_best_model(best_trials_temp['params']) \n",
    "    scaled_feature_df = pd.concat([x_train,x_valid,x_test])\n",
    "    label_df = pd.concat([y_train,y_valid,y_test])\n",
    "    pred_df = make_predictions(model=best_model_temp,df=scaled_feature_df)\n",
    "    output_df = pd.merge(input_df,pred_df['predictions'].to_frame(),left_index=True,right_index=True)\n",
    "    return best_model_temp, output_df, test_arg_processed\n",
    "\n",
    "# Make Predictions\n",
    "def make_predictions(model,df):    \n",
    "    predictions = model.predict(df).flatten()\n",
    "    df['predictions'] = predictions    \n",
    "    return df\n",
    "\n",
    "#Check Authenticity\n",
    "def check_authenticity(df,train_columns,label_col,test_csv_name):\n",
    "    test_columns = list(df.columns)\n",
    "    train_columns = list(train_columns)\n",
    "    try:\n",
    "        test_columns.remove(label_col)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        train_columns.remove(label_col)\n",
    "    except:\n",
    "        pass\n",
    "    if(list(test_columns) != list(train_columns)):\n",
    "        print(\"The columns in the {} test csv do not match with the given model configuration. \\nExiting the script...\".format(test_csv_name))\n",
    "        flush_memory()\n",
    "\n",
    "# Displaying Result\n",
    "def display_results(model,output_df,label_col,test_arg_processed):    \n",
    "    print(\"#####################################################\")\n",
    "    print(\"Results on the training data:\")\n",
    "    print(\"Training Size: {} rows\".format(x_train.shape[0]))\n",
    "    print(\"Testing Size: {} rows\".format(x_test.shape[0]))\n",
    "    loss,mse = model.evaluate(x_test,y_test,verbose=0)\n",
    "    rmse = loss**0.5\n",
    "    pct_error = (rmse / y_test.mean())*100\n",
    "    print(\"RMSE on the test data: \",rmse)\n",
    "    print(\"Percent error on the test data: \", pct_error, \"%\")\n",
    "    print(\"#####################################################\")\n",
    "    for test_df in test_arg_processed:\n",
    "        loss_temp,mse_temp = model.evaluate(test_df.loc[:, test_df.columns != label_col],test_df[label_col],verbose=0)\n",
    "        rmse_temp = loss**0.5\n",
    "        pct_error_temp = (rmse_temp / test_df[label_col].mean())*100\n",
    "        print(\"RMSE on the given test csv: \",rmse_temp)\n",
    "        print(\"Percent error on the given test csv: \", pct_error_temp, \"%\")\n",
    "        print(\"#####################################################\")\n",
    "    return rmse, pct_error\n",
    "\n",
    "# Save the model and update the config db\n",
    "def update_config_db(csv_name,label_col,selected_cols,rmse,pct_error,model_type):\n",
    "    try:\n",
    "        deleteQuery(csv_name)\n",
    "    except:\n",
    "        pass\n",
    "    values = (csv_name,model_type,str(selected_cols).replace(\"'\",'\"'),label_col,rmse,pct_error)\n",
    "    insertQuery(values)\n",
    "    print(\"Record inserted for\", csv_name, \"in the config database.\")\n",
    "\n",
    "def check_duplicacy(csv_name):\n",
    "    rowcount = getRowCount(csv_name)\n",
    "    if(rowcount != 0):\n",
    "        print(\"Model for that csv already exists.\")\n",
    "        while(True):\n",
    "            continue_or_not = input(\"Do you still want to continue? y/n - \")\n",
    "            continue_or_not = continue_or_not.strip(\" \")\n",
    "            try:\n",
    "                if(continue_or_not not in ['Y','y','N','n']):\n",
    "                    raise ValueError(\"Please enter a valid input.\")\n",
    "                else:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        if(continue_or_not in ['N','n']):\n",
    "            print(\"Exiting the script...\")\n",
    "            flush_memory()\n",
    "    \n",
    "def save(csv_name,label_col,model,output_df,selected_cols,rmse, pct_error,model_type):\n",
    "    print(\"Saving the model...\")    \n",
    "    model.save(\"models/{}.h5\".format(csv_name))\n",
    "    print(\"Saving the output predictions...\")\n",
    "    output_df.to_csv(\"training_predictions/{}_predictions.csv\".format(csv_name))\n",
    "    print(\"Pickling necessary data...\")\n",
    "    import pickle\n",
    "    with open('models/{}_columns.pkl'.format(csv_name), 'wb') as f:\n",
    "        pickle.dump(train_columns, f)\n",
    "    with open('models/{}_dummy_columns.pkl'.format(csv_name), 'wb') as f:\n",
    "        pickle.dump(training_dummy_columns, f)\n",
    "    with open('models/{}_selected_columns.pkl'.format(csv_name), 'wb') as f:\n",
    "        pickle.dump(selected_cols, f)\n",
    "    print(\"Updating the config database...\")\n",
    "    update_config_db(csv_name=csv_name,label_col=label_col,selected_cols=selected_cols,rmse=rmse,pct_error=pct_error,model_type=model_type)\n",
    "    print(\"Model and the config information has been saved.\")\n",
    "    \n",
    "def monitor_ram(threshold):\n",
    "    print(\"Monitoring RAM...\")\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(\"Initial RAM available\",(mem.available/1024)/1024,\"mb\")\n",
    "    if(mem.available > threshold):\n",
    "        while(mem.available > threshold):\n",
    "            mem = psutil.virtual_memory()\n",
    "            if(mem.available <= threshold):\n",
    "                print(\"Overflow...\")\n",
    "                print(\"RAM is full. Please upgrade your machine.\")\n",
    "                print(\"Exiting the script...\")\n",
    "                flush_memory()\n",
    "    else:\n",
    "        flush_memory()        \n",
    "        \n",
    "def flush_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    os.system(\"kill -KILL {}\".format(process.pid))\n",
    "\n",
    "def start_thread(threshold):\n",
    "    from threading import Thread\n",
    "    thread = Thread(target = monitor_ram, args=(threshold,))\n",
    "    thread.daemon = True\n",
    "    thread.start()    \n",
    "    \n",
    "# Lime functions\n",
    "def imp_features_lime():\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    selected_features = training_dummy_columns\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(np.array(x_train), feature_names=selected_features, class_names=[], verbose=True, mode='regression')\n",
    "    df1 = get_intensity_dfs(explainer,x_valid)\n",
    "    features_subset = get_best_cols_lime(df1)\n",
    "    return features_subset\n",
    " \n",
    "def predict(qc):\n",
    "    qc = best_model.predict(qc)\n",
    "    return qc.reshape(qc.shape[0])\n",
    "    \n",
    "def get_intensity_dfs(explainer,x_valid):\n",
    "    print('Generating intensity values for training data...')\n",
    "    x_valid_copy = x_valid.head(4).copy()\n",
    "    x_valid_copy.reset_index(drop=True,inplace=True)\n",
    "    print('The LIME iterations will run {} times...'.format(x_valid_copy.shape[0]))\n",
    "    for im in range(x_valid_copy.shape[0]):\n",
    "        print('-'*25,im+1,'-'*25)\n",
    "        exp = explainer.explain_instance(x_valid_copy.loc[im], predict, num_features=x_valid_copy.shape[1])\n",
    "        name_pos = list(x_valid_copy.columns)\n",
    "        intansity = [0]*len(name_pos)\n",
    "        grt = [0]*len(name_pos)\n",
    "        grt_and_eql = [0]*len(name_pos)\n",
    "        less = [0]*len(name_pos)\n",
    "        less_and_eql = [0]*len(name_pos)\n",
    "        try:\n",
    "            for i in exp.as_list():\n",
    "                if i[0].find(' < ') != -1 and i[0].find(' <= ') != -1:\n",
    "                    grt[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = float(i[0][0:i[0].find(' < ')])\n",
    "                    less_and_eql[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = float(i[0][i[0].find(' <= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = i[1]\n",
    "                elif i[0].find(' <= ') != -1 and i[0].find(' < ') != -1:\n",
    "                    grt_and_eql[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = float(i[0][0:i[0].find(' <= ')])\n",
    "                    less[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = float(i[0][i[0].find(' < ')+3:])\n",
    "                    intansity[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = i[1]\n",
    "                elif i[0].find(' < ') != -1:\n",
    "                    less[name_pos.index(i[0][0:i[0].find(' < ')])] = float(i[0][i[0].find(' < ')+3:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' < ')])] = i[1]\n",
    "                elif i[0].find(' <= ') != -1:\n",
    "                    less_and_eql[name_pos.index(i[0][0:i[0].find(' <= ')])] = float(i[0][i[0].find(' <= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' <= ')])] = i[1]\n",
    "                elif i[0].find(' > ') != -1:\n",
    "                    grt[name_pos.index(i[0][0:i[0].find(' > ')])] = float(i[0][i[0].find(' > ')+3:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' > ')])] = i[1]\n",
    "                elif i[0].find(' >= ') != -1:\n",
    "                    grt_and_eql[name_pos.index(i[0][0:i[0].find(' >= ')])] = float(i[0][i[0].find(' >= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' >= ')])] = i[1]\n",
    "        except:\n",
    "            pass\n",
    "        if im == 0:\n",
    "            intensity_dic = {'feature_name':name_pos, 'intensity0':intansity}\n",
    "            df_int = pd.DataFrame(intensity_dic)\n",
    "           \n",
    "        else:\n",
    "            df_int['intensity'+str(im)] = intansity\n",
    "            \n",
    "    return df_int.T\n",
    "\n",
    "def get_best_cols_lime(intensity_df):\n",
    "    header = intensity_df.iloc[0]\n",
    "    intensity_df = intensity_df[1:]\n",
    "    intensity_df.columns = header\n",
    "    intensity_df_trans = intensity_df.T\n",
    "    intensity_df_trans['sum_of_intensities'] = intensity_df_trans.abs().sum(axis=1)\n",
    "    intensity_df_trans.sort_values(by=['sum_of_intensities'],ascending=False,inplace=True)\n",
    "    intensity_df_trans.to_csv('house_pred_intensity_df.csv') #############\n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    intensity_df_trans['sum_of_intensities'] = min_max_scaler.fit_transform(intensity_df_trans['sum_of_intensities'].values.reshape(-1,1)) * 100\n",
    "    features_subset = []\n",
    "    valid_inputs = []\n",
    "    for x in range(1,101):\n",
    "        valid_inputs.append(x)\n",
    "    valid_inputs.append(-1)\n",
    "    while(True):\n",
    "            thresh_lime = input(\"Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - \")\n",
    "            thresh_lime = int(thresh_lime)\n",
    "            try:\n",
    "                if(thresh_lime not in valid_inputs):\n",
    "                    raise ValueError(\"Please enter a valid input.\")\n",
    "                else:\n",
    "                    if(thresh_lime == -1):\n",
    "                        features_subset = []\n",
    "                        break\n",
    "                    else:\n",
    "                        features_subset = list(intensity_df_trans[intensity_df_trans['sum_of_intensities'] >= thresh_lime].index)\n",
    "                        no_of_features = len(features_subset)\n",
    "                        print('The {} features selected are:\\n'.format(no_of_features))\n",
    "                        print(features_subset)\n",
    "                        while(True):\n",
    "                            what_to_do = input('Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - ')\n",
    "                            what_to_do = what_to_do.strip(\" \")\n",
    "                            try:\n",
    "                                if(what_to_do not in ['Y','y','N','n','-1']):\n",
    "                                    raise ValueError(\"Please enter a valid input.\")\n",
    "                                else:\n",
    "                                    break\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                continue\n",
    "                        if(what_to_do == '-1'):\n",
    "                            features_subset = []\n",
    "                            break\n",
    "                        elif(what_to_do in ['n','N']):\n",
    "                            continue\n",
    "                        else:\n",
    "                            return features_subset\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    return features_subset\n",
    "    \n",
    "def driver(csv_name,label_col,test_array,model_type):\n",
    "    threshold = 150 * 1024 * 1024\n",
    "    start_thread(threshold)\n",
    "    global num \n",
    "    num = 1\n",
    "    check_duplicacy(csv_name)\n",
    "    data = data_fetch(csv_name)\n",
    "    if(test_array != [None]):\n",
    "        test_arg = test_array\n",
    "        for test_csv in test_array:\n",
    "            test_dummy = data_fetch_test_array(test_csv)\n",
    "            check_authenticity(df=test_dummy,train_columns=train_columns,label_col=label_col,test_csv_name=test_csv)\n",
    "    else:\n",
    "        test_arg = None\n",
    "    global best_model\n",
    "    best_model, output_df, test_arg_processed = get_best_model(csv_name=csv_name,label_col=label_col,test_arg=test_arg)\n",
    "    rmse, pct_error = display_results(model=best_model, output_df=output_df, label_col=label_col, test_arg_processed=test_arg_processed)\n",
    "    pct_error_lime = -1\n",
    "    while(True):\n",
    "        feature_selection = input(\"Do you want to select features using LIME? y/n - \")\n",
    "        feature_selection = feature_selection.strip(\" \")\n",
    "        try:\n",
    "            if(feature_selection not in ['Y','y','N','n']):\n",
    "                raise ValueError(\"Please enter a valid input.\")\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    if(feature_selection in ['Y','y']):\n",
    "        num = 1\n",
    "        features_subset = imp_features_lime()\n",
    "        if(len(features_subset)!=0):\n",
    "            best_model_lime, output_df_lime, test_arg_processed_lime = get_best_model(csv_name=csv_name,label_col=label_col,test_arg=test_arg,use_lime=1,features_subset=features_subset)\n",
    "            rmse_lime, pct_error_lime = display_results(model=best_model_lime, output_df=output_df_lime,label_col=label_col,test_arg_processed=test_arg_processed_lime)\n",
    "        else:\n",
    "            pct_error_lime = -1\n",
    "    elif(feature_selection in ['N','n']):\n",
    "        pct_error_lime = -1\n",
    "    while(True):\n",
    "        save_model = input(\"Do you want to save the best model? y/n - \")\n",
    "        save_model = save_model.strip(\" \")\n",
    "        try:\n",
    "            if(save_model not in ['Y','y','N','n']):\n",
    "                raise ValueError(\"Please enter a valid input.\")\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    if(save_model in ['Y','y']):\n",
    "        if(pct_error_lime != -1):\n",
    "            if(pct_error<pct_error_lime):\n",
    "                save(csv_name=csv_name,label_col=label_col,model=best_model, output_df=output_df, selected_cols = list(training_dummy_columns), rmse=rmse, pct_error=pct_error,model_type=model_type)\n",
    "                flush_memory()\n",
    "            else:\n",
    "                save(csv_name=csv_name,label_col=label_col,model=best_model_lime, output_df=output_df_lime, selected_cols = features_subset, rmse=rmse_lime, pct_error=pct_error_lime,model_type=model_type)\n",
    "                flush_memory()\n",
    "        else:\n",
    "            save(csv_name=csv_name,label_col=label_col,model=best_model, output_df=output_df, selected_cols = list(training_dummy_columns), rmse=rmse, pct_error=pct_error,model_type=model_type)\n",
    "            flush_memory()\n",
    "    else:\n",
    "        print(\"Exiting the script...\")\n",
    "        flush_memory()\n",
    "\n",
    "# def process(lst):\n",
    "#     try:\n",
    "#         if(len(lst) == 4):\n",
    "#             raise Exception(\"Test csv not provided. Using default value. \\n Test csv : None\")\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         test_array = None\n",
    "#     if(len(lst) == 5):\n",
    "#         test_array = lst[4]\n",
    "#     csv_name = lst[1]\n",
    "#     label_col = lst[2]\n",
    "#     model_type = lst[3]\n",
    "#     test_array = [test_array]\n",
    "#     try:\n",
    "#         driver(csv_name=csv_name,label_col=label_col,test_array=test_array,model_type=model_type)\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring RAM...\n",
      "Initial RAM available 58807.7421875 mb\n",
      "Model for that csv already exists.\n",
      "Do you still want to continue? y/n - y\n",
      "Reading the data...\n",
      "Pre-Processing the data...\n",
      "Size of df before making dummies:  3.773350715637207 mb\n",
      "Converting categorical columns to dummies...\n",
      "Size of df after making dummies:  0.44281005859375 mb\n",
      "Selecting the best network architecture specifically for your data...\n",
      "1) Validation set root mean sq. error: 196071.24 \tAvailable Mem: 58743.91796875 mb\n",
      "2) Validation set root mean sq. error: 41392.92 \tAvailable Mem: 58724.75 mb\n",
      "Training the best selected model...\n",
      "#####################################################\n",
      "Results on the training data:\n",
      "Training Size: 870 rows\n",
      "Testing Size: 291 rows\n",
      "RMSE on the test data:  36312.9363220588\n",
      "Percent error on the test data:  20.175042907682514 %\n",
      "#####################################################\n",
      "Do you want to select features using LIME? y/n - y\n",
      "Generating intensity values for training data...\n",
      "The LIME iterations will run 4 times...\n",
      "------------------------- 1 -------------------------\n",
      "Intercept 309010.6301817965\n",
      "Prediction_local [194008.70225614]\n",
      "Right: 200153.17\n",
      "------------------------- 2 -------------------------\n",
      "Intercept 79251.230315162\n",
      "Prediction_local [199177.6916469]\n",
      "Right: 183979.9\n",
      "------------------------- 3 -------------------------\n",
      "Intercept 129215.51415736991\n",
      "Prediction_local [207083.27509953]\n",
      "Right: 185476.9\n",
      "------------------------- 4 -------------------------\n",
      "Intercept 201561.4375425577\n",
      "Prediction_local [175200.54020691]\n",
      "Right: 183619.67\n",
      "Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - -1\n",
      "Do you want to save the best model? y/n - y\n",
      "Saving the model...\n",
      "Saving the output predictions...\n"
     ]
    }
   ],
   "source": [
    "driver(csv_name='train_no_null',label_col='SalePrice',test_array=[None],model_type='predict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
