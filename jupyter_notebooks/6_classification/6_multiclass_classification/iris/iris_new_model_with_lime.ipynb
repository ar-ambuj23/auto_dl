{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting no of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_of_classes = df['class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_classes = list(df['class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['class']\n",
    "dummy_Y = pd.get_dummies(Y,sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df = pd.merge(df,dummy_Y,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_df.drop(['class'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df,label_cols,test_size=0.2):    \n",
    "    X = df[df.columns.difference(label_cols)]\n",
    "    y = df[label_cols]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(df=pre_processed_df,label_cols=names_of_classes)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 4), (30, 4), (30, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 3), (30, 3), (30, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_space():    \n",
    "    space = {'num_layers': hp.choice('num_layers',['one_hidden', 'two_hidden']),\n",
    "                'units1': hp.choice('units1', [32, 64, 128, 256,512]),\n",
    "                'units2': hp.choice('units2', [32, 64, 128, 256,512]),\n",
    "                'dropout1': hp.uniform('dropout1', .25,.75),\n",
    "                'dropout2': hp.uniform('dropout2',  .25,.75),\n",
    "                'batch_size' : hp.choice('batch_size', [16,32,64,128]),\n",
    "                'nb_epochs' :  500,\n",
    "                'optimizer': hp.choice('optimizer',['rmsprop', 'adam', 'nadam','sgd']),\n",
    "                'activation': hp.choice('activation',['relu','sigmoid']),\n",
    "                'early_stop_rounds': hp.choice('early_stop_rounds',[10,20,30,40,50]),\n",
    "            }\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):    \n",
    "    x_train_temp = x_train.copy() \n",
    "    y_train_temp = y_train.copy()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    if(params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(params['units2']))\n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(y_train_temp.shape[1]))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=params['optimizer'])\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['early_stop_rounds'])\n",
    "    terminate_nan = keras.callbacks.TerminateOnNaN()\n",
    "    history = History()\n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=500,\n",
    "              callbacks=[early_stop, terminate_nan, history],\n",
    "              verbose=0,\n",
    "              validation_data=(x_valid,y_valid)) \n",
    "    [loss, acc] = model.evaluate(x_valid,y_valid, verbose=0)\n",
    "    global num\n",
    "    mem = psutil.virtual_memory()\n",
    "    if(np.isnan(acc)):\n",
    "        print(\"{}) Validation set Accuracy: NaN\".format(num),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "        num = num + 1\n",
    "        return {'loss': np.inf, 'status': STATUS_OK, 'params': params}\n",
    "    print(\"{}) Validation set Accuracy: {:7.2f}\".format(num,acc*100),\"\\tAvailable Mem:\",(mem.available/1024)/1024,\"mb\")\n",
    "    num = num + 1\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(best_params):   \n",
    "    print('Training the best selected model...') \n",
    "    x_train_temp = x_train.copy() \n",
    "    y_train_temp = y_train.copy()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(best_params['units1'], input_shape=(x_train_temp.shape[1],)))\n",
    "    model.add(Activation(best_params['activation']))\n",
    "    model.add(Dropout(best_params['dropout1']))\n",
    "    if(best_params['num_layers'] == 'two_hidden'):\n",
    "        model.add(Dense(best_params['units2']))\n",
    "        model.add(Activation(best_params['activation']))\n",
    "        model.add(Dropout(best_params['dropout2']))\n",
    "    model.add(Dense(y_train_temp.shape[1]))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=best_params['optimizer'])\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=best_params['early_stop_rounds'])\n",
    "    history = History()\n",
    "    model.fit(x_train_temp, y_train_temp,\n",
    "              batch_size=best_params['batch_size'],\n",
    "              epochs=500,\n",
    "              callbacks=[early_stop, history],\n",
    "              verbose=0,\n",
    "              validation_data=(x_valid,y_valid)) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime functions\n",
    "def imp_features_lime():\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    selected_features = x_train.columns\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(np.array(x_train), feature_names=selected_features, class_names=[], verbose=True, mode='classification')\n",
    "    df1 = get_intensity_dfs(explainer,x_valid)\n",
    "    features_subset = get_best_cols_lime(df1)\n",
    "    return features_subset\n",
    "\n",
    "def predict1(ip):\n",
    "    p = best_model.predict_proba(ip)\n",
    "    return p\n",
    "\n",
    "def get_intensity_dfs(explainer,x_valid):\n",
    "    print('Generating intensity values for training data...')\n",
    "    x_valid_copy = x_valid.head(3).copy()\n",
    "    x_valid_copy.reset_index(drop=True,inplace=True)\n",
    "    print('The LIME iterations will run {} times...'.format(x_valid_copy.shape[0]))\n",
    "    for im in range(x_valid_copy.shape[0]):\n",
    "        print('-'*25,im+1,'-'*25) \n",
    "        exp = explainer.explain_instance(x_valid_copy.loc[im], predict1, num_features=x_valid_copy.shape[1])\n",
    "        name_pos = list(x_valid_copy.columns)\n",
    "        intansity = [0]*len(name_pos)\n",
    "        grt = [0]*len(name_pos)\n",
    "        grt_and_eql = [0]*len(name_pos)\n",
    "        less = [0]*len(name_pos)\n",
    "        less_and_eql = [0]*len(name_pos)\n",
    "        try:\n",
    "            for i in exp.as_list():\n",
    "                if i[0].find(' < ') != -1 and i[0].find(' <= ') != -1:\n",
    "                    grt[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = float(i[0][0:i[0].find(' < ')])\n",
    "                    less_and_eql[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = float(i[0][i[0].find(' <= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][i[0].find(' < ')+3:i[0].find(' <= ')])] = i[1]\n",
    "                elif i[0].find(' <= ') != -1 and i[0].find(' < ') != -1:\n",
    "                    grt_and_eql[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = float(i[0][0:i[0].find(' <= ')])\n",
    "                    less[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = float(i[0][i[0].find(' < ')+3:])\n",
    "                    intansity[name_pos.index(i[0][i[0].find(' <= ')+4:i[0].find(' < ')])] = i[1]\n",
    "                elif i[0].find(' < ') != -1:\n",
    "                    less[name_pos.index(i[0][0:i[0].find(' < ')])] = float(i[0][i[0].find(' < ')+3:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' < ')])] = i[1]\n",
    "                elif i[0].find(' <= ') != -1:\n",
    "                    less_and_eql[name_pos.index(i[0][0:i[0].find(' <= ')])] = float(i[0][i[0].find(' <= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' <= ')])] = i[1]\n",
    "                elif i[0].find(' > ') != -1:\n",
    "                    grt[name_pos.index(i[0][0:i[0].find(' > ')])] = float(i[0][i[0].find(' > ')+3:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' > ')])] = i[1]\n",
    "                elif i[0].find(' >= ') != -1:\n",
    "                    grt_and_eql[name_pos.index(i[0][0:i[0].find(' >= ')])] = float(i[0][i[0].find(' >= ')+4:])\n",
    "                    intansity[name_pos.index(i[0][0:i[0].find(' >= ')])] = i[1]\n",
    "        except:\n",
    "            pass\n",
    "        if im == 0:\n",
    "            intensity_dic = {'feature_name':name_pos, 'intensity0':intansity}\n",
    "            df_int = pd.DataFrame(intensity_dic)\n",
    "            \n",
    "        else:\n",
    "            df_int['intensity'+str(im)] = intansity\n",
    "            \n",
    "    return df_int.T\n",
    "\n",
    "def get_best_cols_lime(intensity_df):\n",
    "    header = intensity_df.iloc[0]\n",
    "    intensity_df = intensity_df[1:]\n",
    "    intensity_df.columns = header\n",
    "    intensity_df_trans = intensity_df.T\n",
    "    intensity_df_trans['sum_of_intensities'] = intensity_df_trans.abs().sum(axis=1)\n",
    "    intensity_df_trans.sort_values(by=['sum_of_intensities'],ascending=False,inplace=True)\n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    intensity_df_trans['sum_of_intensities'] = min_max_scaler.fit_transform(intensity_df_trans['sum_of_intensities'].values.reshape(-1,1)) * 100\n",
    "    features_subset = []\n",
    "    valid_inputs = []\n",
    "    for x in range(1,101):\n",
    "        valid_inputs.append(x)\n",
    "    valid_inputs.append(-1)\n",
    "    while(True):\n",
    "            thresh_lime = input(\"Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - \")\n",
    "            thresh_lime = int(thresh_lime)\n",
    "            try:\n",
    "                if(thresh_lime not in valid_inputs):\n",
    "                    raise ValueError(\"Please enter a valid input.\")\n",
    "                else:\n",
    "                    if(thresh_lime == -1):\n",
    "                        features_subset = []\n",
    "                        break\n",
    "                    else:\n",
    "                        features_subset = list(intensity_df_trans[intensity_df_trans['sum_of_intensities'] >= thresh_lime].index)\n",
    "                        no_of_features = len(features_subset)\n",
    "                        print('The {} features selected are:\\n'.format(no_of_features))\n",
    "                        print(features_subset)\n",
    "                        while(True):\n",
    "                            what_to_do = input('Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - ')\n",
    "                            what_to_do = what_to_do.strip(\" \")\n",
    "                            try:\n",
    "                                if(what_to_do not in ['Y','y','N','n','-1']):\n",
    "                                    raise ValueError(\"Please enter a valid input.\")\n",
    "                                else:\n",
    "                                    break\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                continue\n",
    "                        if(what_to_do == '-1'):\n",
    "                            features_subset = []\n",
    "                            break\n",
    "                        elif(what_to_do in ['n','N']):\n",
    "                            continue\n",
    "                        else:\n",
    "                            return features_subset\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    return features_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the best network architecture specifically for your data...\n",
      "1) Validation set Accuracy:   93.33 \tAvailable Mem: 2816.0703125 mb\n",
      "2) Validation set Accuracy:   93.33 \tAvailable Mem: 2802.0390625 mb\n",
      "3) Validation set Accuracy:   60.00 \tAvailable Mem: 2792.59375 mb\n",
      "4) Validation set Accuracy:   93.33 \tAvailable Mem: 2785.1015625 mb\n",
      "5) Validation set Accuracy:   93.33 \tAvailable Mem: 2773.9375 mb\n",
      "6) Validation set Accuracy:   93.33 \tAvailable Mem: 2768.6875 mb\n",
      "7) Validation set Accuracy:   90.00 \tAvailable Mem: 2758.60546875 mb\n",
      "8) Validation set Accuracy:   93.33 \tAvailable Mem: 2745.44140625 mb\n",
      "9) Validation set Accuracy:   93.33 \tAvailable Mem: 2727.9453125 mb\n",
      "10) Validation set Accuracy:   93.33 \tAvailable Mem: 2712.953125 mb\n",
      "Training the best selected model...\n"
     ]
    }
   ],
   "source": [
    "num= 1\n",
    "trials=Trials()\n",
    "space = get_search_space()\n",
    "print(\"Selecting the best network architecture specifically for your data...\")\n",
    "best = fmin(create_model, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "best_trials_temp = trials.best_trial['result'] \n",
    "best_model = train_best_model(best_trials_temp['params']) \n",
    "# scaled_feature_df = pd.concat([x_train,x_valid,x_test])\n",
    "# label_df = pd.concat([y_train,y_valid,y_test])\n",
    "# pred_df = make_predictions(model=best_model_temp,df=scaled_feature_df)\n",
    "# output_df = pd.merge(input_df,pred_df['predictions'].to_frame(),left_index=True,right_index=True)\n",
    "# return best_model_temp, output_df, test_arg_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating intensity values for training data...\n",
      "The LIME iterations will run 3 times...\n",
      "------------------------- 1 -------------------------\n",
      "Intercept 0.32704507420634554\n",
      "Prediction_local [0.46663434]\n",
      "Right: 0.8611476\n",
      "------------------------- 2 -------------------------\n",
      "Intercept 0.33924448749397307\n",
      "Prediction_local [0.46960798]\n",
      "Right: 0.74101204\n",
      "------------------------- 3 -------------------------\n",
      "Intercept 0.4765230690246954\n",
      "Prediction_local [0.09706961]\n",
      "Right: 0.01518198\n",
      "Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - 56\n",
      "The 1 features selected are:\n",
      "\n",
      "['petal_length']\n",
      "Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - n\n",
      "Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - 78\n",
      "The 1 features selected are:\n",
      "\n",
      "['petal_length']\n",
      "Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - n\n",
      "Enter the threshold to select features between 1 to 100. Enter -1 to exit feature selection. - 20\n",
      "The 1 features selected are:\n",
      "\n",
      "['petal_length']\n",
      "Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - b\n",
      "Please enter a valid input.\n",
      "Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - b\n",
      "Please enter a valid input.\n",
      "Do you want to train with selected features? - y/n Enter -1 to exit feature selection. - -1\n"
     ]
    }
   ],
   "source": [
    "features_subset = imp_features_lime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "selected_features = x_train.columns\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(np.array(x_train), feature_names=selected_features, class_names=[], verbose=True, mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(ip):\n",
    "    p = best_model.predict_proba(ip)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept 0.4578051763988422\n",
      "Prediction_local [0.1111323]\n",
      "Right: 0.01518198\n"
     ]
    }
   ],
   "source": [
    "x_valid_copy = x_valid.copy()\n",
    "x_valid_copy.reset_index(drop=True,inplace=True)\n",
    "exp = explainer.explain_instance(x_valid_copy.loc[2], predict1, num_features=x_valid_copy.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('petal_length <= 1.50', -0.42463830101396405),\n",
       " ('petal_width <= 0.30', 0.07370078133002056),\n",
       " ('3.00 < sepal_width <= 3.40', 0.01492378485936213),\n",
       " ('sepal_length <= 5.10', -0.010659141989413291)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.get_dummies(predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17454342544078827, 0.9666666388511658]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.columns = y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dummies(df,label_col):\n",
    "    predict_col = pd.DataFrame(df.idxmax(axis=1),columns=[label_col + '_prediction'])\n",
    "    return predict_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reversed = reverse_dummies(y_pred,'class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
